This file is a merged representation of the entire codebase, combined into a single document. The content has been processed where empty lines have been removed, security check has been disabled.
Generated by Repomix on: 2025-02-14T23:24:34.648Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Security check has been disabled - content may contain sensitive information
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  rules/
    etlguidelines.mdc
config/
  __init__.py
  db_config.py
  queries.py
  settings.py
deployment/
  config/
    __init__.py
    db_config.py
    queries.py
    settings.py
  src/
    business/
      __init__.py
      rules.py
      special_cases.py
      transformations.py
    config/
      validation_config.py
    database/
      __init__.py
      connection.py
      operations.py
    etl/
      __init__.py
      extractors.py
      loaders.py
      transformers.py
    models/
      __init__.py
      sales.py
      stock.py
    utils/
      __init__.py
      data_processing.py
      date_utils.py
      logging_utils.py
      s3_utils.py
    validation/
      __init__.py
      data_validator.py
      schema_validator.py
  file_standardization_job.py
src/
  business/
    __init__.py
    rules.py
    special_cases.py
    transformations.py
  config/
    validation_config.py
  database/
    __init__.py
    connection.py
    operations.py
  etl/
    __init__.py
    extractors.py
    loaders.py
    transformers.py
  models/
    __init__.py
    sales.py
    stock.py
  utils/
    __init__.py
    data_processing.py
    date_utils.py
    logging_utils.py
    s3_utils.py
  validation/
    __init__.py
    data_validator.py
    schema_validator.py
brainstorming.md
file_standardization_2025-02-03.py
file_standardization_job.py
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/etlguidelines.mdc">
---
description: ETL Job Refactoring Guidelines
globs: {     "globs": {         "python_files": [             "src/**/*.py",             "config/**/*.py",             "tests/**/*.py",             "file_standardization_job.py"         ],         "config_files": [             "config/settings.py",             "config/queries.py"         ],         "source_files": [             "src/database/*.py",             "src/etl/*.py",             "src/models/*.py",             "src/business/*.py",             "src/validation/*.py",             "src/utils/*.py"         ],         "test_files": [             "tests/**/*_test.py",             "tests/**/test_*.py"         ],         "exclude": [             "**/__pycache__/**",             "**/*.pyc",             "**/.pytest_cache/**",             "**/venv/**",             "**/env/**",             "**/.env/**",             "**/build/**",             "**/dist/**",             "**/*.egg-info/**"         ],         "schema_files": [             "src/models/schemas/*.json",             "config/schemas/*.json"         ],         "sql_files": [             "config/sql/*.sql"         ]     },     "file_associations": {         ".py": "python",         ".sql": "sql",         ".json": "json"     },     "watch_patterns": [         "src/**/*.py",         "config/**/*.py",         "tests/**/*.py"     ],     "formatting": {         "python": {             "indent_size": 4,             "max_line_length": 100,             "quote_type": "single"         }     } }
---

# Your rule content

Core Principles

Functionality Preservation

All existing business logic from file_standardization_2025-02-03.py must be maintained
Processing flow and data validation rules must remain unchanged
Error handling and logging patterns should be preserved
File processing status tracking must work the same way



Naming Conventions

DataFrames

Main processing DataFrame: data_df
Transformed data: transformed_df
Validation results: validation_df
Temporary results: temp_df
Schema-specific frames: [entity_type]_df (e.g., stock_df, sales_df)


Functions

Extractors: extract_[source]_data
Transformers: transform_[entity]_data
Validators: validate_[rule_type]
Loaders: load_[entity]_data
Utilities: util_[purpose]


Variables

Configuration: CONFIG_[PURPOSE]
Constants: CONSTANT_[PURPOSE]
Temporary variables: temp_[purpose]
Status flags: is_[condition]



Module Organization

Config Module (config/)

Database credentials
AWS configurations
Processing rules
SQL queries
Schema definitions


ETL Module (src/etl/)

File extraction logic
Data transformation rules
Loading procedures
Status tracking


Models Module (src/models/)

Data structure definitions
Schema validations
Type conversions


Business Module (src/business/)

Business validation rules
Data transformation logic
Special case handling


Database Module (src/database/)

Connection management
Query execution
Transaction handling
Error recovery



Required Functionality

File Processing

S3 file reading
Schema validation
Data transformation
PostgreSQL loading
Status updates


Validation Rules

Schema compliance
Business rule validation
Data type checking
Mandatory field validation


Error Handling

Exception capture
Error logging
Transaction rollback
Status updates


Logging

Operation tracking
Error recording
Performance metrics
Status updates



Database Interactions

Tables

Maintain existing table structures
Preserve index definitions
Keep foreign key relationships
Maintain temporary table logic


Queries

Preserve existing query logic
Maintain transaction boundaries
Keep batch processing approach


Performance Considerations

Batch Processing

Maintain existing batch sizes
Preserve memory management
Keep processing checkpoints
Maintain cleanup procedures


Resource Usage

Monitor memory consumption
Track processing time
Manage database connections
Handle S3 interactions efficiently



Additional Notes

Code changes should prioritize readability and maintainability
Document any structural improvements
Maintain AWS Glue compatibility
Preserve existing monitoring capabilities
</file>

<file path="config/__init__.py">
"""
Configuration module for ETL job settings and queries.
"""
from .settings import (
    AWS_REGION,
    S3_BUCKET_PREFIX,
    DB_CONFIG,
    DATE_FORMATS,
    SPECIAL_CASES,
    STOCK_SCHEMA,
    SALES_SCHEMA
)
from .queries import (
    DAQ_LOG_INFO_QUERY,
    ENTITY_DETAIL_QUERY_WITH_SHEET,
    ENTITY_DETAIL_QUERY_WITH_FILE,
    ATTRIBUTE_DETAIL_QUERY,
    UPDATE_DAQ_LOG_INFO
)
__all__ = [
    'AWS_REGION',
    'S3_BUCKET_PREFIX',
    'DB_CONFIG',
    'DATE_FORMATS',
    'SPECIAL_CASES',
    'STOCK_SCHEMA',
    'SALES_SCHEMA',
    'DAQ_LOG_INFO_QUERY',
    'ENTITY_DETAIL_QUERY_WITH_SHEET',
    'ENTITY_DETAIL_QUERY_WITH_FILE',
    'ATTRIBUTE_DETAIL_QUERY',
    'UPDATE_DAQ_LOG_INFO'
]
</file>

<file path="config/db_config.py">
from dataclasses import dataclass
@dataclass
class DBConfig:
    host: str
    dbname: str
    user: str
    password: str
    port: int = 5432
    schema: str = "imip"
</file>

<file path="config/queries.py">
"""
SQL query templates for the ETL job.
"""
# Query to get data from DAQ_LOG_INFO table
DAQ_LOG_INFO_QUERY = """
SELECT DISTINCT 
    id, 
    receiver_address, 
    sender_address, 
    file, 
    sheet_name as daq_sheet_name, 
    mail_date,
    CASE 
        WHEN position('.' in reverse(file)) > 0 THEN 
            substring(file from (char_length(file) - position('.' in reverse(file)) + 2))
        ELSE 'EMPTY'
    END as file_extension,
    (date_trunc('month', mail_date) - INTERVAL '1 day')::date AS mail_date_prev_month_last_day
FROM {schema}.daq_log_info a
WHERE EXISTS (
    SELECT 1
    FROM (
        SELECT 
            file,
            sheet_name,
            subject,
            receiver_address,
            sender_address,
            to_char(mail_date, 'yyyymmdd') as str_mail_date,
            max(id) as max_id
        FROM {schema}.daq_log_info
        WHERE is_corrupt = 0 
        AND is_processed = 0
        GROUP BY 
            file,
            sheet_name,
            subject,
            receiver_address,
            sender_address,
            to_char(mail_date, 'yyyymmdd')
    ) b
    WHERE a.id = b.max_id
)
"""
# Query to get data from DD_ENTITY_DETAIL table with sheet name filter
ENTITY_DETAIL_QUERY_WITH_SHEET = """
SELECT DISTINCT 
    data_owner, 
    country, 
    context, 
    period, 
    frequency, 
    data_owner_mail, 
    structure,
    file_table_name as entity_file_table_name, 
    sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE 
        WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
             lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
        THEN 1
        ELSE 0
    END as sheet_name_comparison_result
FROM {schema}.dd_entity_detail
WHERE is_api = 0
AND data_owner_mail = '{sender_address}'
AND lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
"""
# Query to get data from DD_ENTITY_DETAIL table with file extension filter
ENTITY_DETAIL_QUERY_WITH_FILE = """
SELECT DISTINCT 
    data_owner, 
    country, 
    context, 
    period, 
    frequency, 
    data_owner_mail, 
    structure,
    file_table_name as entity_file_table_name, 
    sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE 
        WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
             lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
        THEN 1
        ELSE 0
    END as sheet_name_comparison_result
FROM (
    SELECT ed.*,
        CASE  
            WHEN position('.' in reverse(file_table_name)) > 0
            THEN substring(file_table_name from (char_length(file_table_name) - position('.' in reverse(file_table_name)) + 2))
            ELSE 'EMPTY'
        END as file_extension
    FROM {schema}.dd_entity_detail ed
)
WHERE is_api = 0
AND CASE 
        WHEN upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) = 'XLS'
        THEN 'XLSX'
        ELSE upper(coalesce(replace(file_extension, 'n', ''), 'file_ext'))
    END =
    CASE 
        WHEN replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I') = 'XLS'
        THEN 'XLSX'
        ELSE replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I')
    END
"""
# Query to get data from DD_ATTRIBUTE_DETAIL table
ATTRIBUTE_DETAIL_QUERY = """
SELECT 
    original_column_name, 
    second_column_name, 
    etl_column_name,
    column_position, 
    starting_row, 
    is_mandatory
FROM {schema}.dd_attribute_detail
WHERE data_owner = '{data_owner}'
AND context = '{context}'
AND file_table_name = '{file_table_name}'
AND sheet_name = '{sheet_name}'
ORDER BY column_position, starting_row
"""
# Query to update DAQ_LOG_INFO table
UPDATE_DAQ_LOG_INFO = """
UPDATE {schema}.daq_log_info 
SET is_processed = 1 
WHERE {condition}
"""
# Query without sheet name filter
ENTITY_DETAIL_QUERY_NO_SHEET = """
SELECT DISTINCT 
    data_owner, country, context, period, frequency, data_owner_mail, structure,
    file_table_name as entity_file_table_name, sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
              lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
         THEN 1 ELSE 0 
    END as sheet_name_comparison_result
FROM {schema}.dd_entity_detail
WHERE is_api = 0
AND data_owner_mail = '{sender_address}'
"""
# Query with country and filename matching
ENTITY_DETAIL_QUERY_COUNTRY = """
SELECT DISTINCT 
    data_owner, 
    country, 
    context, 
    period, 
    frequency, 
    data_owner_mail, 
    structure,
    file_table_name as entity_file_table_name, 
    sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE 
        WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
             lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
        THEN 1 
        ELSE 0 
    END as sheet_name_comparison_result
FROM (
    SELECT ed.*,
        CASE  
            WHEN position('.' in reverse(file_table_name)) > 0
            THEN substring(file_table_name from (char_length(file_table_name) - position('.' in reverse(file_table_name)) + 2))
            ELSE 'EMPTY'
        END as file_extension
    FROM {schema}.dd_entity_detail ed
)
WHERE is_api = 0
AND CASE 
        WHEN upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) = 'XLS'
        THEN 'XLSX'
        ELSE upper(coalesce(replace(file_extension, 'n', ''), 'file_ext'))
    END =
    CASE 
        WHEN replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I') = 'XLS'
        THEN 'XLSX'
        ELSE replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I')
    END
AND (
    (
        (replace(replace(Upper(data_owner), 'İ', 'I'), ' ', '') = replace(replace(Upper('{sender_address}'), 'İ', 'I'), ' ', ''))
        OR
        ((SELECT (Regexp_matches(data_owner_mail, '@([^.]+)'))[1]) = '{sender_address}')
    )
    OR
    (
        country = (
            SELECT DISTINCT dc.country_name
            FROM {schema}.email_connection_info eci,
                 {schema}.dim_countries dc
            WHERE eci.email_address = '{receiver_address}'
            AND dc.country_id = eci.country_id
        )
        AND (
            CASE
                WHEN POSITION('-' IN file_table_name) > 0
                THEN SUBSTRING(file_table_name, 1, POSITION('-' IN file_table_name) - 1)
                WHEN POSITION('.' IN file_table_name) > 0
                THEN SUBSTRING(file_table_name, 1, POSITION('.' IN file_table_name) - 1)
                ELSE file_table_name
            END
        ) = (
            SELECT DISTINCT
                CASE
                    WHEN POSITION('-' IN file) > 0 THEN SUBSTRING(file, 1, POSITION('-' IN file) - 1)
                    WHEN POSITION('.' IN file) > 0 THEN SUBSTRING(file, 1, POSITION('.' IN file) - 1)
                    ELSE file
                END AS file
            FROM {schema}.daq_log_info
            WHERE id = {file_id}
        )
    )
)
"""
</file>

<file path="config/settings.py">
"""
Configuration settings for the ETL job.
"""
# AWS Settings
AWS_REGION = 'eu-central-1'
S3_BUCKET_PREFIX = 's3://pt-s3-imip-{env}-imip-all-data/mail/mnt/c/Roche/IMIP-file/RECEIVED_FILE_PATH'
# Database Settings
DB_CONFIG = {
    'schema': 'imip',
    'driver': 'org.postgresql.Driver',
}
# Date Formats
DATE_FORMATS = [
    # 4-digit year formats (YYYY)
    # Hyphen-separated
    "%Y-%m-%d %H%M%S",  # 2024-12-31 235959
    "%Y-%m-%d %H%M",    # 2024-12-31 2359
    "%Y-%m-%d",         # 2024-12-31
    "%d-%m-%Y %H%M%S",  # 31-12-2024 235959
    "%d-%m-%Y %H%M",    # 31-12-2024 2359
    "%d-%m-%Y",         # 31-12-2024
    "%m-%d-%Y %H%M%S",  # 12-31-2024 235959
    "%m-%d-%Y %H%M",    # 12-31-2024 2359
    "%m-%d-%Y",         # 12-31-2024
    # Dot-separated
    "%Y.%m.%d %H%M%S",  # 2024.12.31 235959
    "%Y.%m.%d %H%M",    # 2024.12.31 2359
    "%Y.%m.%d",         # 2024.12.31
    "%d.%m.%Y %H%M%S",  # 31.12.2024 235959
    "%d.%m.%Y %H%M",    # 31.12.2024 2359
    "%d.%m.%Y",         # 31.12.2024
    "%m.%d.%Y %H%M%S",  # 12.31.2024 235959
    "%m.%d.%Y %H%M",    # 12.31.2024 2359
    "%m.%d.%Y",         # 12.31.2024
    # No separator
    "%Y%m%d%H%M%S",     # 20241231235959
    "%Y%m%d%H%M",       # 202412312359
    "%Y%m%d",           # 20241231
    # 2-digit year formats (YY)
    "%d-%m-%y %H%M%S",  # 31-12-24 235959
    "%d-%m-%y %H%M",    # 31-12-24 2359
    "%d-%m-%y",         # 31-12-24
    "%y-%m-%d %H%M%S",  # 24-12-31 235959
    "%y-%m-%d %H%M",    # 24-12-31 2359
    "%y-%m-%d",         # 24-12-31
    "%m-%d-%y %H%M%S",  # 12-31-24 235959
    "%m-%d-%y %H%M",    # 12-31-24 2359
    "%m-%d-%y",         # 12-31-24
    # Dot-separated 2-digit year
    "%d.%m.%y %H%M%S",  # 31.12.24 235959
    "%d.%m.%y %H%M",    # 31.12.24 2359
    "%d.%m.%y",         # 31.12.24
    "%y.%m.%d %H%M%S",  # 24.12.31 235959
    "%y.%m.%d %H%M",    # 24.12.31 2359
    "%y.%m.%d",         # 24.12.31
    "%m.%d.%y %H%M%S",  # 12.31.24 235959
    "%m.%d.%y %H%M",    # 12.31.24 2359
    "%m.%d.%y",         # 12.31.24
    # No separator 2-digit year
    "%y%m%d%H%M%S",     # 241231235959
    "%y%m%d%H%M",       # 2412312359
    "%y%m%d",           # 241231
]
# Special Cases Configuration
SPECIAL_CASES = {
    'ALLIANCE': {
        'country': 'TURKIYE',
        'sheet_name': 'urundepobazinda'
    },
    'RAFED': {
        'country': 'UAE',
        'valid_sheets': ['mafraq-ssmc data', 'tawam data'],
        'SHEET_MAPPING': {
            'TAWAM DATA': 'TAWAM',
            'MAFRAQ-SSMC DATA': 'SSMC'
        }
    },
    'CITYPHARMACY': {
        'STOCK_CATEGORIES': {'I': 'PU', 'N': 'PR'},
        'BRANCH_MAPPING': {
            'ABU DHABI': 'KIZAD',
            'AL AIN': 'KIZAD',
            'DEFAULT': 'SHARJAH'
        }
    }
}
# DataFrame Schemas
STOCK_SCHEMA = {
    'DATA_DATE': 'datetime64[ns]',
    'COUNTRY_NAME': 'object',
    'ORGANIZATION_NAME': 'object',
    'BRANCH_NAME': 'object',
    'PRODUCT_ID': 'object',
    'PRODUCT_NAME': 'object',
    'AVAILABLE_QUANTITY': 'Int64',
    'BLOCKED_QUANTITY': 'Int64',
    'INVENTORY_CATEGORY': 'object',
    'BATCH_NUMBER': 'object',
    'EXPIRY_DATE': 'datetime64[ns]'
}
SALES_SCHEMA = {
    'DATA_DATE': 'datetime64[ns]',
    'COUNTRY_NAME': 'object',
    'ORGANIZATION_NAME': 'object',
    'BRANCH_NAME': 'object',
    'CUSTOMER_ID': 'object',
    'CUSTOMER_NAME': 'object',
    'PRODUCT_ID': 'object',
    'PRODUCT_NAME': 'object',
    'INVOICE_DATE': 'datetime64[ns]',
    'SALES_QUANTITY': 'Int64',
    'RETURN_QUANTITY': 'Int64',
    'SALES_CATEGORY': 'object',
    'SALES_VALUE': 'float64',
    'RETURN_VALUE': 'float64',
    'AUCTION_NUMBER': 'object',
    'TAX_IDENTIFICATION_NUMBER': 'Int64'
}
COLUMN_MAPPINGS = {
    'QUANTITY_AVAILABLE': 'AVAILABLE_QUANTITY',
    'BATCH_NO': 'BATCH_NUMBER',
    'REGION': 'BRANCH_NAME'  # For specific cases
}
SPECIAL_CASE_MAPPINGS = {
    'SURGIPHARM': {
        'POSITION': {
            'column_mappings': {
                'REGION': 'BRANCH_NAME'
            }
        }
    }
}
# Data Owner Constants
DATA_OWNERS = {
    'ALLIANCE': 'ALLIANCE',
    'RAFED': 'RAFED',
    'CITYPHARMACY': 'CITYPHARMACY',
    'SURGIPHARM': 'SURGIPHARM',
    'QUIMICA_SUIZA': 'QUIMICA SUIZA',
    'BEK': 'BEK',
    'ISKOOP': 'ISKOOP',
    'SELCUK': 'SELÇUK',
    'SIMGE_ECZA': 'SİMGE ECZA DEPOSU MERKEZ',
    'YUSUFPASA': 'YUSUFPAŞA',
    'NEVZAT': 'NEVZAT',
    'VEGA': 'VEGA'
}
# Country Constants
COUNTRIES = {
    'PERU': 'PERU',
    'UAE': 'UAE',
    'TURKIYE': 'TURKIYE',
    'KENYA': 'KENYA',
    'SERBIA': 'SERBIA'
}
# Structure Types
STRUCTURE_TYPES = {
    'TABULAR': 'TABULAR',
    'POSITION': 'POSITION',
    'CUSTOM_POSITION': 'CUSTOM POSITION',
    'CUSTOMRAFED': 'CUSTOMRAFED'
}
# Context Types
CONTEXT_TYPES = {
    'SALES': 'SALES',
    'STOCK': 'STOCK',
    'SALESSTOCK': 'SALESSTOCK'
}
# Frequency Types
FREQUENCY_TYPES = {
    'MONTHLY': 'MONTHLY',
    'WEEKLY': 'WEEKLY',
    'DAILY': 'DAILY'
}
# Column Names
COLUMN_NAMES = {
    'DATA_DATE': 'DATA_DATE',
    'COUNTRY_NAME': 'COUNTRY_NAME',
    'ORGANIZATION_NAME': 'ORGANIZATION_NAME',
    'BRANCH_NAME': 'BRANCH_NAME',
    'PRODUCT_ID': 'PRODUCT_ID',
    'PRODUCT_NAME': 'PRODUCT_NAME',
    'AVAILABLE_QUANTITY': 'AVAILABLE_QUANTITY',
    'BLOCKED_QUANTITY': 'BLOCKED_QUANTITY',
    'INVENTORY_CATEGORY': 'INVENTORY_CATEGORY',
    'BATCH_NUMBER': 'BATCH_NUMBER',
    'EXPIRY_DATE': 'EXPIRY_DATE',
    'CUSTOMER_ID': 'CUSTOMER_ID',
    'CUSTOMER_NAME': 'CUSTOMER_NAME',
    'INVOICE_DATE': 'INVOICE_DATE',
    'SALES_QUANTITY': 'SALES_QUANTITY',
    'RETURN_QUANTITY': 'RETURN_QUANTITY',
    'SALES_CATEGORY': 'SALES_CATEGORY',
    'SALES_VALUE': 'SALES_VALUE',
    'RETURN_VALUE': 'RETURN_VALUE',
    'AUCTION_NUMBER': 'AUCTION_NUMBER',
    'TAX_IDENTIFICATION_NUMBER': 'TAX_IDENTIFICATION_NUMBER',
    'REGION': 'REGION',
    'REGION_NAME': 'REGION_NAME',
    'STOCK_CATEGORY': 'STOCK_CATEGORY',
    'QUANTITY_IN_TRANSIT1': 'QUANTITY_IN_TRANSIT1',
    'QUANTITY_IN_TRANSIT2': 'QUANTITY_IN_TRANSIT2',
    'BRANCH_CODE': 'BRANCH_CODE'
}
# Special Values
SPECIAL_VALUES = {
    'NULL': 'NULL',
    'NA': 'NA',
    'GN': 'GN',
    'PR': 'PR',
    'PU': 'PU',
    'I': 'I',
    'N': 'N'
}
# Location Constants
LOCATION_NAMES = {
    'SHARJAH': 'SHARJAH',
    'KIZAD': 'KIZAD',
    'ABU_DHABI': 'ABU DHABI',
    'AL_AIN': 'AL AIN',
    'TAWAM': 'TAWAM',
    'SSMC': 'SSMC',
    'NAIROBI': 'NAIROBI'
}
# Sheet Names
SHEET_NAMES = {
    'TAWAM_DATA': 'Tawam Data',
    'MAFRAQ_SSMC_DATA': 'Mafraq-SSMC Data',
    'URUNDEPOBAZINDA': 'urundepobazinda',
    'SHEET1': 'Sheet1'
}
# Branch Name Templates
BRANCH_TEMPLATES = {
    'SURGIPHARM_NAIROBI': 'SURGIPHARM NAIROBI',
    'QUIMICA_SUIZA_PUBLIC': 'QUIMICA SUIZA PUBLIC',
    'QUIMICA_SUIZA_PRIVATE': 'QUIMICA SUIZA PRIVATE'
}
# Database Constants
DB_CONSTANTS = {
    'JDBC': 'jdbc',
    'OVERWRITE': 'overwrite',
    'APPEND': 'append'
}
# Null Value Indicators
NULL_VALUES = ['', 'nan', 'nat', 'null', 'NULL', 'None']
</file>

<file path="deployment/config/__init__.py">
"""
Configuration module for ETL job settings and queries.
"""
from .settings import (
    AWS_REGION,
    S3_BUCKET_PREFIX,
    DB_CONFIG,
    DATE_FORMATS,
    SPECIAL_CASES,
    STOCK_SCHEMA,
    SALES_SCHEMA
)
from .queries import (
    DAQ_LOG_INFO_QUERY,
    ENTITY_DETAIL_QUERY_WITH_SHEET,
    ENTITY_DETAIL_QUERY_WITH_FILE,
    ATTRIBUTE_DETAIL_QUERY,
    UPDATE_DAQ_LOG_INFO
)
__all__ = [
    'AWS_REGION',
    'S3_BUCKET_PREFIX',
    'DB_CONFIG',
    'DATE_FORMATS',
    'SPECIAL_CASES',
    'STOCK_SCHEMA',
    'SALES_SCHEMA',
    'DAQ_LOG_INFO_QUERY',
    'ENTITY_DETAIL_QUERY_WITH_SHEET',
    'ENTITY_DETAIL_QUERY_WITH_FILE',
    'ATTRIBUTE_DETAIL_QUERY',
    'UPDATE_DAQ_LOG_INFO'
]
</file>

<file path="deployment/config/db_config.py">
from dataclasses import dataclass
@dataclass
class DBConfig:
    host: str
    dbname: str
    user: str
    password: str
    port: int = 5432
    schema: str = "imip"
</file>

<file path="deployment/config/queries.py">
"""
SQL query templates for the ETL job.
"""
# Query to get data from DAQ_LOG_INFO table
DAQ_LOG_INFO_QUERY = """
SELECT DISTINCT 
    id, 
    receiver_address, 
    sender_address, 
    file, 
    sheet_name as daq_sheet_name, 
    mail_date,
    CASE 
        WHEN position('.' in reverse(file)) > 0 THEN 
            substring(file from (char_length(file) - position('.' in reverse(file)) + 2))
        ELSE 'EMPTY'
    END as file_extension,
    (date_trunc('month', mail_date) - INTERVAL '1 day')::date AS mail_date_prev_month_last_day
FROM {schema}.daq_log_info a
WHERE EXISTS (
    SELECT 1
    FROM (
        SELECT 
            file,
            sheet_name,
            subject,
            receiver_address,
            sender_address,
            to_char(mail_date, 'yyyymmdd') as str_mail_date,
            max(id) as max_id
        FROM {schema}.daq_log_info
        WHERE is_corrupt = 0 
        AND is_processed = 0
        GROUP BY 
            file,
            sheet_name,
            subject,
            receiver_address,
            sender_address,
            to_char(mail_date, 'yyyymmdd')
    ) b
    WHERE a.id = b.max_id
)
"""
# Query to get data from DD_ENTITY_DETAIL table with sheet name filter
ENTITY_DETAIL_QUERY_WITH_SHEET = """
SELECT DISTINCT 
    data_owner, 
    country, 
    context, 
    period, 
    frequency, 
    data_owner_mail, 
    structure,
    file_table_name as entity_file_table_name, 
    sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE 
        WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
             lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
        THEN 1
        ELSE 0
    END as sheet_name_comparison_result
FROM {schema}.dd_entity_detail
WHERE is_api = 0
AND data_owner_mail = '{sender_address}'
AND lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
"""
# Query to get data from DD_ENTITY_DETAIL table with file extension filter
ENTITY_DETAIL_QUERY_WITH_FILE = """
SELECT DISTINCT 
    data_owner, 
    country, 
    context, 
    period, 
    frequency, 
    data_owner_mail, 
    structure,
    file_table_name as entity_file_table_name, 
    sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE 
        WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
             lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
        THEN 1
        ELSE 0
    END as sheet_name_comparison_result
FROM (
    SELECT ed.*,
        CASE  
            WHEN position('.' in reverse(file_table_name)) > 0
            THEN substring(file_table_name from (char_length(file_table_name) - position('.' in reverse(file_table_name)) + 2))
            ELSE 'EMPTY'
        END as file_extension
    FROM {schema}.dd_entity_detail ed
)
WHERE is_api = 0
AND CASE 
        WHEN upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) = 'XLS'
        THEN 'XLSX'
        ELSE upper(coalesce(replace(file_extension, 'n', ''), 'file_ext'))
    END =
    CASE 
        WHEN replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I') = 'XLS'
        THEN 'XLSX'
        ELSE replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I')
    END
"""
# Query to get data from DD_ATTRIBUTE_DETAIL table
ATTRIBUTE_DETAIL_QUERY = """
SELECT 
    original_column_name, 
    second_column_name, 
    etl_column_name,
    column_position, 
    starting_row, 
    is_mandatory
FROM {schema}.dd_attribute_detail
WHERE data_owner = '{data_owner}'
AND context = '{context}'
AND file_table_name = '{file_table_name}'
AND sheet_name = '{sheet_name}'
ORDER BY column_position, starting_row
"""
# Query to update DAQ_LOG_INFO table
UPDATE_DAQ_LOG_INFO = """
UPDATE {schema}.daq_log_info 
SET is_processed = 1 
WHERE {condition}
"""
# Query without sheet name filter
ENTITY_DETAIL_QUERY_NO_SHEET = """
SELECT DISTINCT 
    data_owner, country, context, period, frequency, data_owner_mail, structure,
    file_table_name as entity_file_table_name, sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
              lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
         THEN 1 ELSE 0 
    END as sheet_name_comparison_result
FROM {schema}.dd_entity_detail
WHERE is_api = 0
AND data_owner_mail = '{sender_address}'
"""
# Query with country and filename matching
ENTITY_DETAIL_QUERY_COUNTRY = """
SELECT DISTINCT 
    data_owner, 
    country, 
    context, 
    period, 
    frequency, 
    data_owner_mail, 
    structure,
    file_table_name as entity_file_table_name, 
    sheet_name as entity_sheet_name,
    lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1,
    lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g')) as sheet_name2,
    CASE 
        WHEN lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
             lower(regexp_replace('{daq_sheet_name}', '[0-9]', '', 'g'))
        THEN 1 
        ELSE 0 
    END as sheet_name_comparison_result
FROM (
    SELECT ed.*,
        CASE  
            WHEN position('.' in reverse(file_table_name)) > 0
            THEN substring(file_table_name from (char_length(file_table_name) - position('.' in reverse(file_table_name)) + 2))
            ELSE 'EMPTY'
        END as file_extension
    FROM {schema}.dd_entity_detail ed
)
WHERE is_api = 0
AND CASE 
        WHEN upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) = 'XLS'
        THEN 'XLSX'
        ELSE upper(coalesce(replace(file_extension, 'n', ''), 'file_ext'))
    END =
    CASE 
        WHEN replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I') = 'XLS'
        THEN 'XLSX'
        ELSE replace(upper(coalesce(replace('{file_extension}', 'n', ''), 'file_ext')), 'İ', 'I')
    END
AND (
    (
        (replace(replace(Upper(data_owner), 'İ', 'I'), ' ', '') = replace(replace(Upper('{sender_address}'), 'İ', 'I'), ' ', ''))
        OR
        ((SELECT (Regexp_matches(data_owner_mail, '@([^.]+)'))[1]) = '{sender_address}')
    )
    OR
    (
        country = (
            SELECT DISTINCT dc.country_name
            FROM {schema}.email_connection_info eci,
                 {schema}.dim_countries dc
            WHERE eci.email_address = '{receiver_address}'
            AND dc.country_id = eci.country_id
        )
        AND (
            CASE
                WHEN POSITION('-' IN file_table_name) > 0
                THEN SUBSTRING(file_table_name, 1, POSITION('-' IN file_table_name) - 1)
                WHEN POSITION('.' IN file_table_name) > 0
                THEN SUBSTRING(file_table_name, 1, POSITION('.' IN file_table_name) - 1)
                ELSE file_table_name
            END
        ) = (
            SELECT DISTINCT
                CASE
                    WHEN POSITION('-' IN file) > 0 THEN SUBSTRING(file, 1, POSITION('-' IN file) - 1)
                    WHEN POSITION('.' IN file) > 0 THEN SUBSTRING(file, 1, POSITION('.' IN file) - 1)
                    ELSE file
                END AS file
            FROM {schema}.daq_log_info
            WHERE id = {file_id}
        )
    )
)
"""
</file>

<file path="deployment/config/settings.py">
"""
Configuration settings for the ETL job.
"""
# AWS Settings
AWS_REGION = 'eu-central-1'
S3_BUCKET_PREFIX = 's3://pt-s3-imip-{env}-imip-all-data/mail/mnt/c/Roche/IMIP-file/RECEIVED_FILE_PATH'
# Database Settings
DB_CONFIG = {
    'schema': 'imip',
    'driver': 'org.postgresql.Driver',
}
# Date Formats
DATE_FORMATS = [
    # 4-digit year formats (YYYY)
    # Hyphen-separated
    "%Y-%m-%d %H%M%S",  # 2024-12-31 235959
    "%Y-%m-%d %H%M",    # 2024-12-31 2359
    "%Y-%m-%d",         # 2024-12-31
    "%d-%m-%Y %H%M%S",  # 31-12-2024 235959
    "%d-%m-%Y %H%M",    # 31-12-2024 2359
    "%d-%m-%Y",         # 31-12-2024
    "%m-%d-%Y %H%M%S",  # 12-31-2024 235959
    "%m-%d-%Y %H%M",    # 12-31-2024 2359
    "%m-%d-%Y",         # 12-31-2024
    # Dot-separated
    "%Y.%m.%d %H%M%S",  # 2024.12.31 235959
    "%Y.%m.%d %H%M",    # 2024.12.31 2359
    "%Y.%m.%d",         # 2024.12.31
    "%d.%m.%Y %H%M%S",  # 31.12.2024 235959
    "%d.%m.%Y %H%M",    # 31.12.2024 2359
    "%d.%m.%Y",         # 31.12.2024
    "%m.%d.%Y %H%M%S",  # 12.31.2024 235959
    "%m.%d.%Y %H%M",    # 12.31.2024 2359
    "%m.%d.%Y",         # 12.31.2024
    # No separator
    "%Y%m%d%H%M%S",     # 20241231235959
    "%Y%m%d%H%M",       # 202412312359
    "%Y%m%d",           # 20241231
    # 2-digit year formats (YY)
    "%d-%m-%y %H%M%S",  # 31-12-24 235959
    "%d-%m-%y %H%M",    # 31-12-24 2359
    "%d-%m-%y",         # 31-12-24
    "%y-%m-%d %H%M%S",  # 24-12-31 235959
    "%y-%m-%d %H%M",    # 24-12-31 2359
    "%y-%m-%d",         # 24-12-31
    "%m-%d-%y %H%M%S",  # 12-31-24 235959
    "%m-%d-%y %H%M",    # 12-31-24 2359
    "%m-%d-%y",         # 12-31-24
    # Dot-separated 2-digit year
    "%d.%m.%y %H%M%S",  # 31.12.24 235959
    "%d.%m.%y %H%M",    # 31.12.24 2359
    "%d.%m.%y",         # 31.12.24
    "%y.%m.%d %H%M%S",  # 24.12.31 235959
    "%y.%m.%d %H%M",    # 24.12.31 2359
    "%y.%m.%d",         # 24.12.31
    "%m.%d.%y %H%M%S",  # 12.31.24 235959
    "%m.%d.%y %H%M",    # 12.31.24 2359
    "%m.%d.%y",         # 12.31.24
    # No separator 2-digit year
    "%y%m%d%H%M%S",     # 241231235959
    "%y%m%d%H%M",       # 2412312359
    "%y%m%d",           # 241231
]
# Special Cases Configuration
SPECIAL_CASES = {
    'ALLIANCE': {
        'country': 'TURKIYE',
        'sheet_name': 'urundepobazinda'
    },
    'RAFED': {
        'country': 'UAE',
        'valid_sheets': ['mafraq-ssmc data', 'tawam data'],
        'SHEET_MAPPING': {
            'TAWAM DATA': 'TAWAM',
            'MAFRAQ-SSMC DATA': 'SSMC'
        }
    },
    'CITYPHARMACY': {
        'STOCK_CATEGORIES': {'I': 'PU', 'N': 'PR'},
        'BRANCH_MAPPING': {
            'ABU DHABI': 'KIZAD',
            'AL AIN': 'KIZAD',
            'DEFAULT': 'SHARJAH'
        }
    }
}
# DataFrame Schemas
STOCK_SCHEMA = {
    'DATA_DATE': 'datetime64[ns]',
    'COUNTRY_NAME': 'object',
    'ORGANIZATION_NAME': 'object',
    'BRANCH_NAME': 'object',
    'PRODUCT_ID': 'object',
    'PRODUCT_NAME': 'object',
    'AVAILABLE_QUANTITY': 'Int64',
    'BLOCKED_QUANTITY': 'Int64',
    'INVENTORY_CATEGORY': 'object',
    'BATCH_NUMBER': 'object',
    'EXPIRY_DATE': 'datetime64[ns]'
}
SALES_SCHEMA = {
    'DATA_DATE': 'datetime64[ns]',
    'COUNTRY_NAME': 'object',
    'ORGANIZATION_NAME': 'object',
    'BRANCH_NAME': 'object',
    'CUSTOMER_ID': 'object',
    'CUSTOMER_NAME': 'object',
    'PRODUCT_ID': 'object',
    'PRODUCT_NAME': 'object',
    'INVOICE_DATE': 'datetime64[ns]',
    'SALES_QUANTITY': 'Int64',
    'RETURN_QUANTITY': 'Int64',
    'SALES_CATEGORY': 'object',
    'SALES_VALUE': 'float64',
    'RETURN_VALUE': 'float64',
    'AUCTION_NUMBER': 'object',
    'TAX_IDENTIFICATION_NUMBER': 'Int64'
}
COLUMN_MAPPINGS = {
    'QUANTITY_AVAILABLE': 'AVAILABLE_QUANTITY',
    'BATCH_NO': 'BATCH_NUMBER',
    'REGION': 'BRANCH_NAME'  # For specific cases
}
SPECIAL_CASE_MAPPINGS = {
    'SURGIPHARM': {
        'POSITION': {
            'column_mappings': {
                'REGION': 'BRANCH_NAME'
            }
        }
    }
}
# Data Owner Constants
DATA_OWNERS = {
    'ALLIANCE': 'ALLIANCE',
    'RAFED': 'RAFED',
    'CITYPHARMACY': 'CITYPHARMACY',
    'SURGIPHARM': 'SURGIPHARM',
    'QUIMICA_SUIZA': 'QUIMICA SUIZA',
    'BEK': 'BEK',
    'ISKOOP': 'ISKOOP',
    'SELCUK': 'SELÇUK',
    'SIMGE_ECZA': 'SİMGE ECZA DEPOSU MERKEZ',
    'YUSUFPASA': 'YUSUFPAŞA',
    'NEVZAT': 'NEVZAT',
    'VEGA': 'VEGA'
}
# Country Constants
COUNTRIES = {
    'PERU': 'PERU',
    'UAE': 'UAE',
    'TURKIYE': 'TURKIYE',
    'KENYA': 'KENYA',
    'SERBIA': 'SERBIA'
}
# Structure Types
STRUCTURE_TYPES = {
    'TABULAR': 'TABULAR',
    'POSITION': 'POSITION',
    'CUSTOM_POSITION': 'CUSTOM POSITION',
    'CUSTOMRAFED': 'CUSTOMRAFED'
}
# Context Types
CONTEXT_TYPES = {
    'SALES': 'SALES',
    'STOCK': 'STOCK',
    'SALESSTOCK': 'SALESSTOCK'
}
# Frequency Types
FREQUENCY_TYPES = {
    'MONTHLY': 'MONTHLY',
    'WEEKLY': 'WEEKLY',
    'DAILY': 'DAILY'
}
# Column Names
COLUMN_NAMES = {
    'DATA_DATE': 'DATA_DATE',
    'COUNTRY_NAME': 'COUNTRY_NAME',
    'ORGANIZATION_NAME': 'ORGANIZATION_NAME',
    'BRANCH_NAME': 'BRANCH_NAME',
    'PRODUCT_ID': 'PRODUCT_ID',
    'PRODUCT_NAME': 'PRODUCT_NAME',
    'AVAILABLE_QUANTITY': 'AVAILABLE_QUANTITY',
    'BLOCKED_QUANTITY': 'BLOCKED_QUANTITY',
    'INVENTORY_CATEGORY': 'INVENTORY_CATEGORY',
    'BATCH_NUMBER': 'BATCH_NUMBER',
    'EXPIRY_DATE': 'EXPIRY_DATE',
    'CUSTOMER_ID': 'CUSTOMER_ID',
    'CUSTOMER_NAME': 'CUSTOMER_NAME',
    'INVOICE_DATE': 'INVOICE_DATE',
    'SALES_QUANTITY': 'SALES_QUANTITY',
    'RETURN_QUANTITY': 'RETURN_QUANTITY',
    'SALES_CATEGORY': 'SALES_CATEGORY',
    'SALES_VALUE': 'SALES_VALUE',
    'RETURN_VALUE': 'RETURN_VALUE',
    'AUCTION_NUMBER': 'AUCTION_NUMBER',
    'TAX_IDENTIFICATION_NUMBER': 'TAX_IDENTIFICATION_NUMBER',
    'REGION': 'REGION',
    'REGION_NAME': 'REGION_NAME',
    'STOCK_CATEGORY': 'STOCK_CATEGORY',
    'QUANTITY_IN_TRANSIT1': 'QUANTITY_IN_TRANSIT1',
    'QUANTITY_IN_TRANSIT2': 'QUANTITY_IN_TRANSIT2',
    'BRANCH_CODE': 'BRANCH_CODE'
}
# Special Values
SPECIAL_VALUES = {
    'NULL': 'NULL',
    'NA': 'NA',
    'GN': 'GN',
    'PR': 'PR',
    'PU': 'PU',
    'I': 'I',
    'N': 'N'
}
# Location Constants
LOCATION_NAMES = {
    'SHARJAH': 'SHARJAH',
    'KIZAD': 'KIZAD',
    'ABU_DHABI': 'ABU DHABI',
    'AL_AIN': 'AL AIN',
    'TAWAM': 'TAWAM',
    'SSMC': 'SSMC',
    'NAIROBI': 'NAIROBI'
}
# Sheet Names
SHEET_NAMES = {
    'TAWAM_DATA': 'Tawam Data',
    'MAFRAQ_SSMC_DATA': 'Mafraq-SSMC Data',
    'URUNDEPOBAZINDA': 'urundepobazinda',
    'SHEET1': 'Sheet1'
}
# Branch Name Templates
BRANCH_TEMPLATES = {
    'SURGIPHARM_NAIROBI': 'SURGIPHARM NAIROBI',
    'QUIMICA_SUIZA_PUBLIC': 'QUIMICA SUIZA PUBLIC',
    'QUIMICA_SUIZA_PRIVATE': 'QUIMICA SUIZA PRIVATE'
}
# Database Constants
DB_CONSTANTS = {
    'JDBC': 'jdbc',
    'OVERWRITE': 'overwrite',
    'APPEND': 'append'
}
# Null Value Indicators
NULL_VALUES = ['', 'nan', 'nat', 'null', 'NULL', 'None']
</file>

<file path="deployment/src/business/__init__.py">
"""
Business logic module for data processing and validation.
"""
from .rules import DataOwnerRules, TransformationRules, ValidationRules, SpecialCaseRules
from .transformations import StockTransformations, SalesTransformations, DateTransformations
from .special_cases import QuimicaSuizaHandler, RafedHandler, CitypharmacyHandler, KuwaitHandler
__all__ = [
    'DataOwnerRules',
    'TransformationRules',
    'ValidationRules',
    'SpecialCaseRules',
    'StockTransformations',
    'SalesTransformations',
    'DateTransformations',
    'QuimicaSuizaHandler',
    'RafedHandler',
    'CitypharmacyHandler',
    'KuwaitHandler'
]
</file>

<file path="deployment/src/business/rules.py">
"""
Business rules for data validation and transformation.
"""
from typing import Dict, List, Any
import pandas as pd
import numpy as np
class DataOwnerRules:
    """Rules for specific data owners."""
    @staticmethod
    def validate_alliance(data_owner: str, country: str, sheet_name: str) -> bool:
        """
        Validate ALLIANCE data.
        Args:
            data_owner (str): Data owner name
            country (str): Country name
            sheet_name (str): Sheet name
        Returns:
            bool: True if valid, False otherwise
        """
        return (
            data_owner == 'ALLIANCE' 
            and country == 'TURKIYE' 
            and sheet_name.lower() == 'urundepobazinda'
        )
    @staticmethod
    def validate_rafed(data_owner: str, country: str, sheet_name: str) -> bool:
        """
        Validate RAFED data.
        Args:
            data_owner (str): Data owner name
            country (str): Country name
            sheet_name (str): Sheet name
        Returns:
            bool: True if valid, False otherwise
        """
        valid_sheets = ['mafraq-ssmc data', 'tawam data']
        return (
            data_owner == 'RAFED'
            and country == 'UAE'
            and sheet_name.lower() in valid_sheets
        )
class TransformationRules:
    """Rules for data transformations."""
    @staticmethod
    def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
        """
        Clean and standardize column names.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with cleaned column names
        """
        df = df.copy()
        df.columns = (
            df.columns.str.strip()
            .str.replace(r'[^\w\s]', '', regex=True)
            .str.replace(r'\s+', '_', regex=True)
            .str.upper()
        )
        return df
    @staticmethod
    def remove_empty_columns(df: pd.DataFrame) -> pd.DataFrame:
        """
        Remove empty columns from DataFrame.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with empty columns removed
        """
        df = df.copy()
        for col in df.columns:
            if df[col].astype(str).str.strip().eq('').all():
                df = df.drop(columns=[col])
        return df
    @staticmethod
    def remove_empty_rows(df: pd.DataFrame) -> pd.DataFrame:
        """
        Remove empty rows from DataFrame.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with empty rows removed
        """
        df = df.copy()
        return df.dropna(how='all').reset_index(drop=True)
class ValidationRules:
    """Rules for data validation."""
    @staticmethod
    def validate_mandatory_columns(df: pd.DataFrame, required_columns: List[str]) -> bool:
        """
        Validate presence of mandatory columns.
        Args:
            df (pd.DataFrame): Input DataFrame
            required_columns (List[str]): List of required column names
        Returns:
            bool: True if all required columns present, False otherwise
        """
        return all(col in df.columns for col in required_columns)
    @staticmethod
    def validate_numeric_columns(df: pd.DataFrame, numeric_columns: List[str]) -> bool:
        """
        Validate numeric columns.
        Args:
            df (pd.DataFrame): Input DataFrame
            numeric_columns (List[str]): List of numeric column names
        Returns:
            bool: True if all numeric columns are valid, False otherwise
        """
        for col in numeric_columns:
            if col in df.columns:
                if not pd.to_numeric(df[col], errors='coerce').notnull().all():
                    return False
        return True
class SpecialCaseRules:
    """Rules for special cases."""
    @staticmethod
    def process_kuwait_data(df: pd.DataFrame) -> pd.DataFrame:
        """
        Process data for Kuwait.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        new_rows = []
        for _, row in df.iterrows():
            # Find columns containing '~'
            split_cols = [col for col in df.columns if '~' in str(row[col])]
            if not split_cols:
                new_rows.append(row)
                continue
            # Find max splits needed
            max_splits = max(len(str(row[col]).split('~')) for col in split_cols)
            # Process splits
            for i in range(max_splits):
                new_row = row.copy()
                for col in df.columns:
                    if col in split_cols:
                        splits = str(row[col]).split('~')
                        new_row[col] = splits[i] if i < len(splits) else None
                new_rows.append(new_row)
        return pd.DataFrame(new_rows)
    @staticmethod
    def process_citypharmacy_data(df: pd.DataFrame, context: str) -> pd.DataFrame:
        """
        Process data for Citypharmacy.
        Args:
            df (pd.DataFrame): Input DataFrame
            context (str): Processing context (STOCK/SALES)
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if context == 'STOCK':
            # Filter out 'I' category
            if 'STOCK_CATEGORY' in df.columns:
                df = df[df['STOCK_CATEGORY'] != 'I']
            # Update inventory category
            if 'INVENTORY_CATEGORY' in df.columns:
                df['INVENTORY_CATEGORY'] = df.apply(
                    lambda row: 'PU' if row.get('STOCK_CATEGORY') == 'I'
                    else ('PR' if row.get('STOCK_CATEGORY') == 'N' else 'GN'),
                    axis=1
                )
        elif context == 'SALES':
            # Update sales category
            if 'SALES_CATEGORY' in df.columns:
                df['SALES_CATEGORY'] = df.apply(
                    lambda row: 'PU' if row.get('SALES_CATEGORY') == 'I'
                    else ('PR' if row.get('SALES_CATEGORY') == 'N' else 'GN'),
                    axis=1
                )
        return df 
class BusinessRules:
    def apply_organization_rules(self, df, data_owner):
        """Add organization-specific rules"""
        if data_owner == 'ALLIANCE':
            df = self.handle_alliance_data(df)
        elif data_owner == 'RAFED':
            df = self.handle_rafed_data(df)
        return df
</file>

<file path="deployment/src/business/special_cases.py">
"""
Special case handling for specific organizations and countries.
"""
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
from decimal import Decimal
class QuimicaSuizaHandler:
    """Handler for Quimica Suiza specific processing."""
    @staticmethod
    def process_stock_data(df: pd.DataFrame) -> pd.DataFrame:
        """
        Process stock data for Quimica Suiza.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        # Fill NULL values in BATCH_NUMBER with 'NA'
        if 'BATCH_NUMBER' in df.columns:
            df['BATCH_NUMBER'] = df['BATCH_NUMBER'].fillna('NA')
        # Process INVENTORY_CATEGORY based on BRANCH_NAME
        if 'BRANCH_NAME' in df.columns:
            df['INVENTORY_CATEGORY'] = df['BRANCH_NAME'].apply(
                lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
            )
        # Calculate BLOCKED_QUANTITY from transit quantities
        transit_cols = ['QUANTITY_IN_TRANSIT1', 'QUANTITY_IN_TRANSIT2']
        if all(col in df.columns for col in transit_cols):
            df['BLOCKED_QUANTITY'] = (
                pd.to_numeric(df['QUANTITY_IN_TRANSIT1'], errors='coerce').fillna(0) +
                pd.to_numeric(df['QUANTITY_IN_TRANSIT2'], errors='coerce').fillna(0)
            ) * 1000
        # Add missing header row handling
        if len(df.columns) == len(df.iloc[0].unique()):
            empty_row = pd.DataFrame([{col: None for col in df.columns}])
            df = pd.concat([empty_row, df], ignore_index=True)
        return df
    @staticmethod
    def process_sales_data(df: pd.DataFrame) -> pd.DataFrame:
        """
        Process sales data for Quimica Suiza.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        # Process SALES_CATEGORY based on BRANCH_NAME
        if 'BRANCH_NAME' in df.columns:
            df['SALES_CATEGORY'] = df['BRANCH_NAME'].apply(
                lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
            )
        # Scale quantity columns
        quantity_cols = ['SALES_QUANTITY', 'RETURN_QUANTITY']
        for col in quantity_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) * 1000
        # Scale value columns
        value_cols = ['SALES_VALUE', 'RETURN_VALUE']
        for col in value_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) * 1000
        return df
class RafedHandler:
    """Handler for RAFED specific processing."""
    @staticmethod
    def process_organization(df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:
        """
        Process organization name based on sheet name.
        Args:
            df (pd.DataFrame): Input DataFrame
            sheet_name (str): Sheet name
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if sheet_name.lower() == 'tawam data':
            df['ORGANIZATION_NAME'] = 'TAWAM'
            if 'SALES_QUANTITY' in df.columns:
                df['SALES_QUANTITY'] = df['SALES_QUANTITY'] * -1
        elif sheet_name.lower() == 'mafraq-ssmc data':
            df['ORGANIZATION_NAME'] = 'SSMC'
        return df
class CitypharmacyHandler:
    """Handler for Citypharmacy specific processing."""
    @staticmethod
    def process_branch_name(df: pd.DataFrame, country: str) -> pd.DataFrame:
        """
        Process branch name based on region.
        Args:
            df (pd.DataFrame): Input DataFrame
            country (str): Country name
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if 'REGION_NAME' in df.columns and country == 'UAE':
            df['REGION_NAME'] = df['REGION_NAME'].str.upper()
            df['BRANCH_NAME'] = df.apply(
                lambda row: (
                    f"{row['ORGANIZATION_NAME']} KIZAD"
                    if pd.notna(row['REGION_NAME']) and row['REGION_NAME'] in ['ABU DHABI', 'AL AIN']
                    else (
                        f"{row['ORGANIZATION_NAME']} SHARJAH"
                        if pd.notna(row['REGION_NAME'])
                        else f"{row['ORGANIZATION_NAME']} SHARJAH"
                    )
                ),
                axis=1
            )
            df = df.drop(columns=['REGION_NAME'])
        return df
    @staticmethod
    def process_categories(df: pd.DataFrame, context: str) -> pd.DataFrame:
        """
        Process inventory/sales categories.
        Args:
            df (pd.DataFrame): Input DataFrame
            context (str): Processing context (STOCK/SALES)
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if context == 'STOCK':
            if 'STOCK_CATEGORY' in df.columns:
                df['INVENTORY_CATEGORY'] = df['STOCK_CATEGORY'].apply(
                    lambda x: 'PU' if x == 'I' else ('PR' if x == 'N' else 'GN')
                )
                df = df.drop(columns=['STOCK_CATEGORY'])
        elif context == 'SALES':
            if 'SALES_CATEGORY' in df.columns:
                df['SALES_CATEGORY'] = df['SALES_CATEGORY'].apply(
                    lambda x: 'PU' if x == 'I' else ('PR' if x == 'N' else 'GN')
                )
        return df
class KuwaitHandler:
    """Handler for Kuwait specific processing."""
    @staticmethod
    def split_tilde_rows(df: pd.DataFrame) -> pd.DataFrame:
        """
        Split rows containing tilde (~) character.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame with split rows
        """
        df = df.copy()
        new_rows = []
        for _, row in df.iterrows():
            # Find columns containing tilde
            tilde_cols = [col for col in df.columns if '~' in str(row[col])]
            if not tilde_cols:
                new_rows.append(row)
                continue
            # Find maximum number of splits needed
            max_splits = max(len(str(row[col]).split('~')) for col in tilde_cols)
            # Create new rows for each split
            for i in range(max_splits):
                new_row = row.copy()
                for col in df.columns:
                    if col in tilde_cols:
                        splits = str(row[col]).split('~')
                        new_row[col] = splits[i] if i < len(splits) else None
                    else:
                        # Handle numeric values
                        if str(row[col]).isdigit() and str(col) != '0':
                            new_row[col] = row[col] if i == 0 else None
                new_rows.append(new_row)
        return pd.DataFrame(new_rows)
</file>

<file path="deployment/src/business/transformations.py">
"""
Business-specific data transformations.
"""
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from decimal import Decimal
from datetime import datetime, date
class StockTransformations:
    """Transformations for stock data."""
    @staticmethod
    def standardize_quantities(df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize quantity columns in stock data.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with standardized quantities
        """
        df = df.copy()
        quantity_columns = ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']
        for col in quantity_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0).astype('Int64')
        return df
    @staticmethod
    def process_inventory_category(df: pd.DataFrame, data_owner: str) -> pd.DataFrame:
        """
        Process inventory category based on data owner rules.
        Args:
            df (pd.DataFrame): Input DataFrame
            data_owner (str): Data owner name
        Returns:
            pd.DataFrame: DataFrame with processed inventory category
        """
        df = df.copy()
        if 'INVENTORY_CATEGORY' not in df.columns:
            df['INVENTORY_CATEGORY'] = 'GN'
        if data_owner == 'QUIMICA SUIZA':
            if 'BRANCH_NAME' in df.columns:
                df['INVENTORY_CATEGORY'] = df['BRANCH_NAME'].apply(
                    lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                    else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
                )
        return df
class SalesTransformations:
    """Transformations for sales data."""
    @staticmethod
    def standardize_quantities(df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize quantity columns in sales data.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with standardized quantities
        """
        df = df.copy()
        # Process sales quantity
        if 'SALES_QUANTITY' in df.columns:
            df['SALES_QUANTITY'] = pd.to_numeric(df['SALES_QUANTITY'], errors='coerce')
            df['SALES_QUANTITY'] = df['SALES_QUANTITY'].fillna(0).astype('Int64')
        # Process return quantity
        if 'RETURN_QUANTITY' in df.columns:
            df['RETURN_QUANTITY'] = pd.to_numeric(df['RETURN_QUANTITY'], errors='coerce')
            df['RETURN_QUANTITY'] = df['RETURN_QUANTITY'].fillna(0).astype('Int64')
        return df
    @staticmethod
    def standardize_values(df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize monetary value columns in sales data.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with standardized values
        """
        df = df.copy()
        value_columns = ['SALES_VALUE', 'RETURN_VALUE']
        for col in value_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0).astype('float64')
                df[col] = df[col].apply(Decimal)
        return df
    @staticmethod
    def process_sales_category(df: pd.DataFrame, data_owner: str) -> pd.DataFrame:
        """
        Process sales category based on data owner rules.
        Args:
            df (pd.DataFrame): Input DataFrame
            data_owner (str): Data owner name
        Returns:
            pd.DataFrame: DataFrame with processed sales category
        """
        df = df.copy()
        if 'SALES_CATEGORY' not in df.columns:
            df['SALES_CATEGORY'] = 'GN'
        if data_owner == 'QUIMICA SUIZA':
            if 'BRANCH_NAME' in df.columns:
                df['SALES_CATEGORY'] = df['BRANCH_NAME'].apply(
                    lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                    else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
                )
        return df
class DateTransformations:
    """Transformations for date fields."""
    @staticmethod
    def standardize_dates(df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:
        """
        Standardize date columns.
        Args:
            df (pd.DataFrame): Input DataFrame
            date_columns (List[str]): List of date column names
        Returns:
            pd.DataFrame: DataFrame with standardized dates
        """
        df = df.copy()
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                df[col] = df[col].fillna(pd.Timestamp('2199-12-31'))
        return df
    @staticmethod
    def process_data_date(df: pd.DataFrame, frequency: str, mail_date: datetime) -> pd.DataFrame:
        """
        Process DATA_DATE based on frequency.
        Args:
            df (pd.DataFrame): Input DataFrame
            frequency (str): Data frequency (MONTHLY/WEEKLY/DAILY)
            mail_date (datetime): Mail date
        Returns:
            pd.DataFrame: DataFrame with processed DATA_DATE
        """
        df = df.copy()
        if 'DATA_DATE' not in df.columns:
            if frequency == 'MONTHLY':
                # Set to last day of previous month
                first_day = date(mail_date.year, mail_date.month, 1)
                last_day = first_day - pd.Timedelta(days=1)
                df['DATA_DATE'] = last_day
            else:
                df['DATA_DATE'] = mail_date
        return df
</file>

<file path="deployment/src/config/validation_config.py">
"""
Configuration module for data validation rules and constants.
"""
from typing import Dict, List, Any
from datetime import datetime, date
# Stock Data Validation Rules
STOCK_VALIDATION_RULES = {
    'required_columns': [
        'DATA_DATE',
        'COUNTRY_NAME',
        'ORGANIZATION_NAME',
        'PRODUCT_ID',
        'AVAILABLE_QUANTITY'
    ],
    'numeric_columns': [
        'AVAILABLE_QUANTITY',
        'BLOCKED_QUANTITY'
    ],
    'date_columns': [
        'DATA_DATE',
        'EXPIRY_DATE'
    ],
    'categorical_columns': {
        'INVENTORY_CATEGORY': ['GN', 'PR', 'PU']
    },
    'min_date': date(2020, 1, 1),
    'max_date': datetime.now().date()
}
# Sales Data Validation Rules
SALES_VALIDATION_RULES = {
    'required_columns': [
        'DATA_DATE',
        'COUNTRY_NAME',
        'ORGANIZATION_NAME',
        'PRODUCT_ID',
        'SALES_QUANTITY',
        'SALES_VALUE'
    ],
    'numeric_columns': [
        'SALES_QUANTITY',
        'RETURN_QUANTITY',
        'SALES_VALUE',
        'RETURN_VALUE',
        'TAX_IDENTIFICATION_NUMBER'
    ],
    'date_columns': [
        'DATA_DATE',
        'INVOICE_DATE'
    ],
    'categorical_columns': {
        'SALES_CATEGORY': ['GN', 'PR', 'PU']
    },
    'min_date': date(2020, 1, 1),
    'max_date': datetime.now().date()
}
# Organization-specific Rules
ORGANIZATION_RULES = {
    'CITYPHARMACY': {
        'required_branch_name': True,
        'allowed_categories': ['GN', 'PR', 'PU'],
        'country_specific_rules': {
            'KUWAIT': {
                'split_tilde_rows': True
            }
        }
    },
    'RAFED': {
        'sheet_name_mapping': {
            'Stock': 'RAFED',
            'Sales': 'RAFED'
        }
    },
    'ALLIANCE': {
        'country_specific_rules': {
            'KUWAIT': {
                'process_branch_name': True
            }
        }
    }
}
# Data Type Mappings
COLUMN_DATA_TYPES = {
    'DATA_DATE': 'datetime64[ns]',
    'COUNTRY_NAME': 'str',
    'ORGANIZATION_NAME': 'str',
    'PRODUCT_ID': 'str',
    'AVAILABLE_QUANTITY': 'float64',
    'BLOCKED_QUANTITY': 'float64',
    'SALES_QUANTITY': 'float64',
    'RETURN_QUANTITY': 'float64',
    'SALES_VALUE': 'float64',
    'RETURN_VALUE': 'float64',
    'TAX_IDENTIFICATION_NUMBER': 'str'
}
# Error Messages
ERROR_MESSAGES = {
    'missing_columns': "Missing required columns: {columns}",
    'invalid_numeric': "Non-numeric values found in {column}: {values}",
    'invalid_date': "Invalid dates found in {column}: {values}",
    'invalid_category': "Invalid values in {column}: {values}",
    'negative_quantity': "Negative values found in {column}",
    'return_exceeds_sales': "Return {type} exceeds sales {type}",
    'product_mismatch': "Products in sales but not in stock: {products}",
    'org_mismatch': "Organizations not matching between stock and sales: {orgs}"
}
</file>

<file path="deployment/src/database/__init__.py">
"""
Database module for connection and operations.
"""
from .connection import DatabaseConnection
from .operations import DatabaseOperations
__all__ = [
    'DatabaseConnection',
    'DatabaseOperations'
]
</file>

<file path="deployment/src/database/connection.py">
"""
Database connection handling module.
"""
import json
import boto3
from pyspark.sql import SparkSession
from config.settings import DB_CONFIG, AWS_REGION
class DatabaseConnection:
    def __init__(self, secret_name):
        """
        Initialize database connection with AWS Secrets Manager credentials.
        Args:
            secret_name (str): Name of the secret in AWS Secrets Manager
        """
        self.secret_name = secret_name
        self.credentials = self._get_secret()
        self.jdbc_url = f"jdbc:postgresql://{self.credentials['host']}:{self.credentials['port']}/{self.credentials['dbname']}"
        self.jdbc_properties = {
            'user': self.credentials['username'],
            'password': self.credentials['password'],
            'driver': DB_CONFIG['driver']
        }
    def _get_secret(self):
        """
        Retrieve database credentials from AWS Secrets Manager.
        Returns:
            dict: Database credentials
        """
        client = boto3.client('secretsmanager', region_name=AWS_REGION)
        response = client.get_secret_value(SecretId=self.secret_name)
        return json.loads(response['SecretString'])
    def get_spark_session(self):
        """
        Get or create a Spark session.
        Returns:
            SparkSession: Active Spark session
        """
        return SparkSession.builder.appName("PostgreSQL Connection").getOrCreate()
    def execute_query(self, query, params=None):
        """
        Execute a SQL query using Spark JDBC connection.
        Args:
            query (str): SQL query to execute
            params (dict, optional): Parameters to format the query with
        Returns:
            DataFrame: Spark DataFrame with query results
        """
        if params:
            query = query.format(**params)
        spark = self.get_spark_session()
        return (spark.read.format("jdbc")
                .option("url", self.jdbc_url)
                .option("query", query)
                .option("driver", DB_CONFIG['driver'])
                .option("user", self.credentials['username'])
                .option("password", self.credentials['password'])
                .load())
    def execute_update(self, query, params=None):
        """
        Execute an update SQL query using JDBC connection.
        Args:
            query (str): SQL update query to execute
            params (dict, optional): Parameters to format the query with
        """
        if params:
            query = query.format(**params)
        spark = self.get_spark_session()
        conn = spark.sparkContext._jvm.java.sql.DriverManager.getConnection(
            self.jdbc_url,
            self.credentials['username'],
            self.credentials['password']
        )
        try:
            stmt = conn.createStatement()
            stmt.execute(query)
        finally:
            if conn:
                conn.close()
</file>

<file path="deployment/src/database/operations.py">
"""
Database operations module for handling specific database queries and operations.
"""
from config.queries import (
    DAQ_LOG_INFO_QUERY,
    ENTITY_DETAIL_QUERY_WITH_SHEET,
    ENTITY_DETAIL_QUERY_WITH_FILE,
    ATTRIBUTE_DETAIL_QUERY,
    UPDATE_DAQ_LOG_INFO,
    ENTITY_DETAIL_QUERY_NO_SHEET,
    ENTITY_DETAIL_QUERY_COUNTRY
)
from config.settings import DB_CONFIG
class DatabaseOperations:
    def __init__(self, db_connection):
        """
        Initialize database operations with a database connection.
        Args:
            db_connection: DatabaseConnection instance
        """
        self.db_connection = db_connection
        self.schema = DB_CONFIG['schema']
    def get_unprocessed_files(self):
        """
        Get unprocessed files from DAQ_LOG_INFO table.
        Returns:
            DataFrame: Spark DataFrame with unprocessed files information
        """
        params = {'schema': self.schema}
        return self.db_connection.execute_query(DAQ_LOG_INFO_QUERY, params)
    def get_entity_details_by_sheet(self, daq_sheet_name, sender_address):
        """
        Get entity details filtered by sheet name.
        Args:
            daq_sheet_name (str): Sheet name from DAQ log
            sender_address (str): Sender's email address
        Returns:
            DataFrame: Spark DataFrame with entity details
        """
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'sender_address': sender_address
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_WITH_SHEET, params)
    def get_entity_details_by_file(self, daq_sheet_name, file_extension):
        """
        Get entity details filtered by file extension.
        Args:
            daq_sheet_name (str): Sheet name from DAQ log
            file_extension (str): File extension
        Returns:
            DataFrame: Spark DataFrame with entity details
        """
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'file_extension': file_extension
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_WITH_FILE, params)
    def get_attribute_details(self, data_owner, context, file_table_name, sheet_name):
        """
        Get attribute details for a specific entity.
        Args:
            data_owner (str): Data owner name
            context (str): Context
            file_table_name (str): File/table name
            sheet_name (str): Sheet name
        Returns:
            DataFrame: Spark DataFrame with attribute details
        """
        params = {
            'schema': self.schema,
            'data_owner': data_owner,
            'context': context,
            'file_table_name': file_table_name,
            'sheet_name': sheet_name
        }
        return self.db_connection.execute_query(ATTRIBUTE_DETAIL_QUERY, params)
    def mark_file_as_processed(self, file_id=None):
        """
        Mark file(s) as processed in DAQ_LOG_INFO table.
        Args:
            file_id (int, optional): Specific file ID to mark as processed.
                                   If None, marks all unprocessed files.
        """
        condition = f"id = {file_id}" if file_id else "is_corrupt = 0 and is_processed = 0"
        params = {
            'schema': self.schema,
            'condition': condition
        }
        self.db_connection.execute_update(UPDATE_DAQ_LOG_INFO, params)
    def insert_temp_load_info(self, table_name, context, frequency, is_invoice, is_process):
        """
        Insert record into temp_load_info table.
        Args:
            table_name (str): Table name
            context (str): Context
            frequency (str): Frequency
            is_invoice (int): Invoice flag
            is_process (int): Process flag
        """
        query = f"""
        INSERT INTO {self.schema}.temp_load_info 
        (table_name, context, frequency, is_invoice, is_process, load_datetime)
        VALUES 
        ('{table_name.upper()}', '{context}', '{frequency}', {is_invoice}, {is_process}, CURRENT_TIMESTAMP)
        """
        self.db_connection.execute_update(query)
    def get_entity_details(self, daq_sheet_name, sender_address, file_extension, receiver_address, file_id):
        """
        Get entity details using sequential matching strategy.
        Tries different matching criteria in sequence until a match is found.
        Args:
            daq_sheet_name (str): Sheet name from DAQ log
            sender_address (str): Sender's email address
            file_extension (str): File extension
            receiver_address (str): Receiver's email address
            file_id (int): File ID from DAQ_LOG_INFO
        Returns:
            DataFrame: Spark DataFrame with entity details from the first successful match
        """
        # Step 1: Try sheet name match first (strictest match)
        result = self.get_entity_details_by_sheet(daq_sheet_name, sender_address)
        if result.count() > 0:
            return result
        # Step 2: Try file extension match
        result = self.get_entity_details_by_file(daq_sheet_name, file_extension)
        if result.count() > 0:
            return result
        # Step 3: Try without sheet name restriction
        result = self.get_entity_details_no_sheet(daq_sheet_name, sender_address)
        if result.count() > 0:
            return result
        # Step 4: Finally try country and filename matching (most relaxed criteria)
        return self.get_entity_details_by_country(
            daq_sheet_name, file_extension, sender_address, receiver_address, file_id
        )
    def get_entity_details_no_sheet(self, daq_sheet_name, sender_address):
        """Get entity details without sheet name filter"""
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'sender_address': sender_address
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_NO_SHEET, params)
    def get_entity_details_by_country(self, daq_sheet_name, file_extension, sender_address, receiver_address, file_id):
        """Get entity details with country and filename matching"""
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'file_extension': file_extension,
            'sender_address': sender_address,
            'receiver_address': receiver_address,
            'file_id': file_id
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_COUNTRY, params)
</file>

<file path="deployment/src/etl/__init__.py">
"""
ETL module for data extraction, transformation, and loading.
"""
from .extractors import DataExtractor
from .transformers import DataTransformer
from .loaders import DataLoader
__all__ = [
    'DataExtractor',
    'DataTransformer',
    'DataLoader'
]
</file>

<file path="deployment/src/etl/extractors.py">
"""
Data extraction module for the ETL job.
"""
import pandas as pd
import s3fs
from config.settings import S3_BUCKET_PREFIX, SPECIAL_CASES
class DataExtractor:
    def __init__(self, s3_connection, logger):
        """
        Initialize the data extractor.
        Args:
            s3_connection: S3 connection object
            logger: Logger instance
        """
        self.s3 = s3_connection
        self.logger = logger
        self.fs = s3fs.S3FileSystem(anon=False)
    def read_excel_file(self, file_path, sheet_name=None):
        """
        Read data from an Excel file.
        Args:
            file_path (str): Path to the Excel file
            sheet_name (str, optional): Name of the sheet to read
        Returns:
            DataFrame: pandas DataFrame with the file contents
        """
        try:
            with self.fs.open(file_path, 'rb') as f:
                if sheet_name:
                    df = pd.read_excel(f, sheet_name=sheet_name)
                else:
                    df = pd.read_excel(f)
            self.logger.log_info(f"Successfully read Excel file: {file_path}")
            self.logger.log_dataframe_info(df, "Excel Data")
            return df
        except Exception as e:
            self.logger.log_error(f"Error reading Excel file {file_path}: {str(e)}", exc_info=e)
            return None
    def read_csv_file(self, file_path, **kwargs):
        """
        Read data from a CSV file.
        Args:
            file_path (str): Path to the CSV file
            **kwargs: Additional arguments for pd.read_csv
        Returns:
            DataFrame: pandas DataFrame with the file contents
        """
        try:
            with self.fs.open(file_path, 'rb') as f:
                df = pd.read_csv(f, **kwargs)
            self.logger.log_info(f"Successfully read CSV file: {file_path}")
            self.logger.log_dataframe_info(df, "CSV Data")
            return df
        except Exception as e:
            self.logger.log_error(f"Error reading CSV file {file_path}: {str(e)}", exc_info=e)
            return None
    def validate_file_source(self, data_owner, country, sheet_name):
        """
        Validate file source based on special cases.
        Args:
            data_owner (str): Data owner name
            country (str): Country name
            sheet_name (str): Sheet name
        Returns:
            bool: True if source is valid, False otherwise
        """
        if data_owner.upper() in SPECIAL_CASES:
            case = SPECIAL_CASES[data_owner.upper()]
            # Check country match
            if case['country'] != country.upper():
                self.logger.log_warning(
                    f"Invalid country for {data_owner}: {country}"
                )
                return False
            # Check sheet name constraints
            if 'sheet_name' in case and case['sheet_name'].lower() != sheet_name.lower():
                self.logger.log_warning(
                    f"Invalid sheet name for {data_owner}: {sheet_name}"
                )
                return False
            if 'valid_sheets' in case and sheet_name.lower() not in case['valid_sheets']:
                self.logger.log_warning(
                    f"Sheet name {sheet_name} not in valid sheets for {data_owner}"
                )
                return False
        return True
    def extract_data(self, file_info):
        """
        Extract data based on file information.
        Args:
            file_info (dict): Dictionary containing file information
                Required keys:
                - file_path: Path to the file
                - file_type: Type of file (excel/csv)
                - sheet_name: Sheet name (for Excel files)
                - data_owner: Data owner name
                - country: Country name
        Returns:
            DataFrame: pandas DataFrame with the extracted data
        """
        # Validate required keys
        required_keys = ['file_path', 'file_type', 'data_owner', 'country']
        missing_keys = [key for key in required_keys if key not in file_info]
        if missing_keys:
            self.logger.log_error(f"Missing required keys in file_info: {missing_keys}")
            return None
        # Validate file source
        sheet_name = file_info.get('sheet_name')
        if not self.validate_file_source(
            file_info['data_owner'],
            file_info['country'],
            sheet_name or ''
        ):
            return None
        # Extract data based on file type
        file_path = f"{self.s3_bucket}/{file_info['file_path']}"
        if file_info['file_type'].lower() == 'excel':
            return self.read_excel_file(file_path, sheet_name)
        elif file_info['file_type'].lower() == 'csv':
            return self.read_csv_file(file_path)
        else:
            self.logger.log_error(f"Unsupported file type: {file_info['file_type']}")
            return None
</file>

<file path="deployment/src/etl/loaders.py">
"""
Data loading module for the ETL job.
"""
from datetime import datetime
from src.models.stock import StockData
from src.models.sales import SalesData
class DataLoader:
    def __init__(self, db_connection, logger):
        """
        Initialize the data loader.
        Args:
            db_connection: Database connection object
            logger: Logger instance
        """
        self.db = db_connection
        self.logger = logger
    def load_stock_data(self, stock_data, table_name):
        """
        Load stock data into the database.
        Args:
            stock_data (StockData): Stock data to load
            table_name (str): Target table name
        Returns:
            bool: True if loading successful, False otherwise
        """
        if not isinstance(stock_data, StockData):
            self.logger.log_error("Invalid stock data type")
            return False
        try:
            # Convert to Spark DataFrame
            spark = self.db.get_spark_session()
            spark_df = stock_data.to_spark_df(spark)
            # Write to database
            (spark_df.write
                .format("jdbc")
                .option("url", self.db.jdbc_url)
                .option("dbtable", table_name)
                .option("user", self.db.jdbc_properties['user'])
                .option("password", self.db.jdbc_properties['password'])
                .option("driver", self.db.jdbc_properties['driver'])
                .mode("append")
                .save())
            self.logger.log_info(f"Successfully loaded stock data to table: {table_name}")
            return True
        except Exception as e:
            self.logger.log_error(f"Error loading stock data: {str(e)}", exc_info=e)
            return False
    def load_sales_data(self, sales_data, table_name):
        """
        Load sales data into the database.
        Args:
            sales_data (SalesData): Sales data to load
            table_name (str): Target table name
        Returns:
            bool: True if loading successful, False otherwise
        """
        if not isinstance(sales_data, SalesData):
            self.logger.log_error("Invalid sales data type")
            return False
        try:
            # Convert to Spark DataFrame
            spark = self.db.get_spark_session()
            spark_df = sales_data.to_spark_df(spark)
            # Write to database
            (spark_df.write
                .format("jdbc")
                .option("url", self.db.jdbc_url)
                .option("dbtable", table_name)
                .option("user", self.db.jdbc_properties['user'])
                .option("password", self.db.jdbc_properties['password'])
                .option("driver", self.db.jdbc_properties['driver'])
                .mode("append")
                .save())
            self.logger.log_info(f"Successfully loaded sales data to table: {table_name}")
            return True
        except Exception as e:
            self.logger.log_error(f"Error loading sales data: {str(e)}", exc_info=e)
            return False
    def create_temp_table(self, table_name, schema):
        """
        Create a temporary table for data loading.
        Args:
            table_name (str): Table name
            schema (dict): Column schema dictionary
        Returns:
            bool: True if creation successful, False otherwise
        """
        try:
            # Generate CREATE TABLE statement
            columns = []
            for col_name, col_type in schema.items():
                sql_type = self._map_pandas_to_sql_type(col_type)
                columns.append(f"{col_name} {sql_type}")
            create_stmt = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                {', '.join(columns)},
                LOAD_DATETIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
            # Execute creation
            self.db.execute_update(create_stmt)
            self.logger.log_info(f"Successfully created temporary table: {table_name}")
            return True
        except Exception as e:
            self.logger.log_error(f"Error creating temporary table: {str(e)}", exc_info=e)
            return False
    def _map_pandas_to_sql_type(self, pandas_type):
        """
        Map pandas dtype to SQL type.
        Args:
            pandas_type (str): pandas dtype
        Returns:
            str: Corresponding SQL type
        """
        type_mapping = {
            'datetime64[ns]': 'TIMESTAMP',
            'object': 'TEXT',
            'Int64': 'BIGINT',
            'float64': 'DECIMAL(22,2)',
            'bool': 'BOOLEAN'
        }
        return type_mapping.get(pandas_type, 'TEXT')
    def cleanup_temp_tables(self, retention_days=7):
        """
        Clean up old temporary tables.
        Args:
            retention_days (int): Number of days to retain tables
        Returns:
            bool: True if cleanup successful, False otherwise
        """
        try:
            cleanup_query = f"""
            DELETE FROM temp_load_info
            WHERE load_datetime < CURRENT_TIMESTAMP - INTERVAL '{retention_days} days'
            """
            self.db.execute_update(cleanup_query)
            self.logger.log_info(f"Successfully cleaned up temporary tables older than {retention_days} days")
            return True
        except Exception as e:
            self.logger.log_error(f"Error cleaning up temporary tables: {str(e)}", exc_info=e)
            return False
    def update_processing_status(self, file_id):
        """Add comprehensive status update"""
        try:
            self.db.execute_update(
                "UPDATE daq_log_info SET is_processed = 1 WHERE id = %s",
                (file_id,)
            )
            self.db.commit()
        except Exception as e:
            self.logger.log_error(f"Status update failed: {str(e)}")
            self.db.rollback()
</file>

<file path="deployment/src/etl/transformers.py">
"""
Data transformation module for the ETL job.
"""
import pandas as pd
import numpy as np
from src.utils.date_utils import parse_date, get_last_day_of_month
from src.models.stock import StockData
from src.models.sales import SalesData
from typing import List
class DataTransformer:
    def __init__(self, logger):
        """
        Initialize the data transformer.
        Args:
            logger: Logger instance
        """
        self.logger = logger
    def clean_column_names(self, df):
        """
        Clean and standardize column names.
        Args:
            df (DataFrame): Input DataFrame
        Returns:
            DataFrame: DataFrame with cleaned column names
        """
        try:
            # Remove special characters and standardize spacing
            df.columns = df.columns.str.strip()
            df.columns = df.columns.str.replace(r'[^\w\s]', '', regex=True)
            df.columns = df.columns.str.replace(r'\s+', '_', regex=True)
            df.columns = df.columns.str.upper()
            self.logger.log_info("Column names cleaned successfully")
            return df
        except Exception as e:
            self.logger.log_error(f"Error cleaning column names: {str(e)}", exc_info=e)
            return df
    def map_columns(self, df, column_mapping):
        """
        Map DataFrame columns according to provided mapping.
        Args:
            df (DataFrame): Input DataFrame
            column_mapping (dict): Dictionary mapping original to new column names
        Returns:
            DataFrame: DataFrame with mapped columns
        """
        try:
            # Create a copy to avoid modifying the original
            df_mapped = df.copy()
            # Rename columns according to mapping
            df_mapped.rename(columns=column_mapping, inplace=True)
            # Log mapped columns
            self.logger.log_info("Columns mapped successfully")
            self.logger.log_debug(f"Column mapping: {column_mapping}")
            return df_mapped
        except Exception as e:
            self.logger.log_error(f"Error mapping columns: {str(e)}", exc_info=e)
            return df
    def transform_stock_data(self, df, attribute_details):
        """
        Transform raw data into stock data format.
        Args:
            df (DataFrame): Raw input DataFrame
            attribute_details (DataFrame): DataFrame containing attribute mapping details
        Returns:
            StockData: Transformed stock data
        """
        try:
            # Initialize stock data model
            stock_data = StockData()
            # Clean and map columns
            df = self.clean_column_names(df)
            # Create column mapping from attribute details
            column_mapping = {
                attr['original_column_name']: attr['etl_column_name']
                for attr in attribute_details
                if attr['original_column_name'] in df.columns
            }
            df = self.map_columns(df, column_mapping)
            # Convert data types
            for col in df.columns:
                if col in ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif col in ['DATA_DATE', 'EXPIRY_DATE']:
                    df[col] = df[col].apply(parse_date)
            # Add records to stock data model
            stock_data.add_records(df.to_dict('records'))
            # Validate and clean data
            if stock_data.validate_data():
                stock_data.clean_data()
                self.logger.log_info("Stock data transformed successfully")
                return stock_data
            else:
                self.logger.log_error("Stock data validation failed")
                return None
        except Exception as e:
            self.logger.log_error(f"Error transforming stock data: {str(e)}", exc_info=e)
            return None
    def transform_sales_data(self, df, attribute_details):
        """
        Transform raw data into sales data format.
        Args:
            df (DataFrame): Raw input DataFrame
            attribute_details (dict): Dictionary containing attribute mapping details
        Returns:
            SalesData: Transformed sales data
        """
        try:
            # Initialize sales data model
            sales_data = SalesData()
            # Clean and map columns
            df = self.clean_column_names(df)
            column_mapping = {
                attr['original_column_name']: attr['etl_column_name']
                for attr in attribute_details
                if attr['original_column_name'] in df.columns
            }
            df = self.map_columns(df, column_mapping)
            # Convert data types
            for col in df.columns:
                if col in ['SALES_QUANTITY', 'RETURN_QUANTITY']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif col in ['SALES_VALUE', 'RETURN_VALUE']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif col in ['DATA_DATE', 'INVOICE_DATE']:
                    df[col] = df[col].apply(parse_date)
            # Add records to sales data model
            sales_data.add_records(df.to_dict('records'))
            # Validate and clean data
            if sales_data.validate_data():
                sales_data.clean_data()
                self.logger.log_info("Sales data transformed successfully")
                return sales_data
            else:
                self.logger.log_error("Sales data validation failed")
                return None
        except Exception as e:
            self.logger.log_error(f"Error transforming sales data: {str(e)}", exc_info=e)
            return None
    def remove_unwanted_characters(self, df, columns):
        """
        Remove unwanted characters from specified columns.
        Args:
            df (DataFrame): Input DataFrame
            columns (list): List of columns to clean
        Returns:
            DataFrame: Cleaned DataFrame
        """
        try:
            df_clean = df.copy()
            for col in columns:
                if col in df_clean.columns:
                    # Remove special characters and extra spaces
                    df_clean[col] = df_clean[col].astype(str)
                    df_clean[col] = df_clean[col].str.replace(r'[^\w\s-]', '', regex=True)
                    df_clean[col] = df_clean[col].str.strip()
            self.logger.log_info(f"Cleaned unwanted characters from columns: {columns}")
            return df_clean
        except Exception as e:
            self.logger.log_error(f"Error removing unwanted characters: {str(e)}", exc_info=e)
            return df
    def handle_special_values(self, df):
        """Add SURGIPHARM special case handling"""
        if self.data_owner == 'SURGIPHARM' and self.country == 'KENYA':
            df.loc[df['BRANCH_NAME'].str.startswith('MAINSTORES'), 'BRANCH_NAME'] = None
            df.loc[df['SALES_CATEGORY'] == 'SALESSTOCK', 'SALES_CATEGORY'] = None
    def process_dates(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:
        """
        Process date columns in DataFrame.
        Args:
            df (pd.DataFrame): Input DataFrame
            date_columns (List[str]): List of date column names
        Returns:
            pd.DataFrame: DataFrame with processed dates
        """
        df = df.copy()
        for col in date_columns:
            if col in df.columns:
                df[col] = df[col].apply(parse_date)
                # Handle special cases
                if col == 'DATA_DATE' and self.frequency == 'MONTHLY':
                    df[col] = df[col].apply(
                        lambda x: get_last_day_of_month(x) if x else None
                    )
        return df
</file>

<file path="deployment/src/models/__init__.py">
"""
Data models for stock and sales data.
"""
from .stock import StockData
from .sales import SalesData
__all__ = [
    'StockData',
    'SalesData'
]
</file>

<file path="deployment/src/models/sales.py">
"""
Sales data model for handling sales transaction data.
"""
import pandas as pd
from config.settings import SALES_SCHEMA
class SalesData:
    def __init__(self):
        """Initialize an empty sales DataFrame with the predefined schema."""
        self.data = pd.DataFrame({col: pd.Series(dtype=dtype) 
                                for col, dtype in SALES_SCHEMA.items()})
    def add_record(self, record_dict):
        """
        Add a record to the sales DataFrame.
        Args:
            record_dict (dict): Dictionary containing sales record data
        """
        # Convert to DataFrame with single row
        record_df = pd.DataFrame([record_dict])
        # Ensure data types match schema
        for col, dtype in SALES_SCHEMA.items():
            if col in record_df.columns:
                record_df[col] = record_df[col].astype(dtype)
        # Append to existing data
        self.data = pd.concat([self.data, record_df], ignore_index=True)
    def add_records(self, records_list):
        """
        Add multiple records to the sales DataFrame.
        Args:
            records_list (list): List of dictionaries containing sales records
        """
        for record in records_list:
            self.add_record(record)
    def validate_data(self):
        """
        Validate the data in the DataFrame.
        Returns:
            bool: True if data is valid, False otherwise
        """
        # Check for required columns
        required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'SALES_QUANTITY',
            'SALES_VALUE'
        ]
        missing_columns = [col for col in required_columns 
                         if col not in self.data.columns]
        if missing_columns:
            print(f"Missing required columns: {missing_columns}")
            return False
        # Check for null values in required fields
        null_counts = self.data[required_columns].isnull().sum()
        if null_counts.any():
            print("Null values found in required columns:")
            print(null_counts[null_counts > 0])
            return False
        # Validate numeric fields
        numeric_columns = ['SALES_QUANTITY', 'RETURN_QUANTITY', 
                         'SALES_VALUE', 'RETURN_VALUE']
        for col in numeric_columns:
            if col in self.data.columns:
                if not pd.to_numeric(self.data[col], errors='coerce').notnull().all():
                    print(f"Invalid numeric values found in {col}")
                    return False
        return True
    def clean_data(self):
        """Clean and standardize the data in the DataFrame."""
        # Remove any leading/trailing whitespace
        string_columns = [col for col, dtype in SALES_SCHEMA.items() 
                         if dtype == 'object']
        for col in string_columns:
            if col in self.data.columns:
                self.data[col] = self.data[col].str.strip()
        # Convert quantities to integers
        quantity_columns = ['SALES_QUANTITY', 'RETURN_QUANTITY']
        for col in quantity_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
                self.data[col] = self.data[col].fillna(0).astype('Int64')
        # Convert monetary values to float
        value_columns = ['SALES_VALUE', 'RETURN_VALUE']
        for col in value_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
                self.data[col] = self.data[col].fillna(0.0).astype('float64')
        # Standardize date formats
        date_columns = ['DATA_DATE', 'INVOICE_DATE']
        for col in date_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_datetime(self.data[col], errors='coerce')
    def calculate_net_sales(self):
        """
        Calculate net sales quantities and values.
        Returns:
            tuple: (net_quantity, net_value)
        """
        net_quantity = (self.data['SALES_QUANTITY'].sum() - 
                       self.data['RETURN_QUANTITY'].sum())
        net_value = (self.data['SALES_VALUE'].sum() - 
                    self.data['RETURN_VALUE'].sum())
        return net_quantity, net_value
    def to_spark_df(self, spark):
        """
        Convert pandas DataFrame to Spark DataFrame.
        Args:
            spark: SparkSession instance
        Returns:
            DataFrame: Spark DataFrame
        """
        return spark.createDataFrame(self.data)
</file>

<file path="deployment/src/models/stock.py">
"""
Stock data model for handling inventory data.
"""
import pandas as pd
from config.settings import STOCK_SCHEMA
class StockData:
    def __init__(self):
        """Initialize an empty stock DataFrame with the predefined schema."""
        self.data = pd.DataFrame({col: pd.Series(dtype=dtype) 
                                for col, dtype in STOCK_SCHEMA.items()})
    def add_record(self, record_dict):
        """
        Add a record to the stock DataFrame.
        Args:
            record_dict (dict): Dictionary containing stock record data
        """
        # Convert to DataFrame with single row
        record_df = pd.DataFrame([record_dict])
        # Ensure data types match schema
        for col, dtype in STOCK_SCHEMA.items():
            if col in record_df.columns:
                record_df[col] = record_df[col].astype(dtype)
        # Append to existing data
        self.data = pd.concat([self.data, record_df], ignore_index=True)
    def add_records(self, records_list):
        """
        Add multiple records to the stock DataFrame.
        Args:
            records_list (list): List of dictionaries containing stock records
        """
        for record in records_list:
            self.add_record(record)
    def validate_data(self):
        """
        Validate the data in the DataFrame.
        Returns:
            bool: True if data is valid, False otherwise
        """
        # Check for required columns
        required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'AVAILABLE_QUANTITY'
        ]
        missing_columns = [col for col in required_columns 
                         if col not in self.data.columns]
        if missing_columns:
            print(f"Missing required columns: {missing_columns}")
            return False
        # Check for null values in required fields
        null_counts = self.data[required_columns].isnull().sum()
        if null_counts.any():
            print("Null values found in required columns:")
            print(null_counts[null_counts > 0])
            return False
        # Validate numeric fields
        numeric_columns = ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']
        for col in numeric_columns:
            if col in self.data.columns:
                if not pd.to_numeric(self.data[col], errors='coerce').notnull().all():
                    print(f"Invalid numeric values found in {col}")
                    return False
        return True
    def clean_data(self):
        """Clean and standardize the data in the DataFrame."""
        # Remove any leading/trailing whitespace
        string_columns = [col for col, dtype in STOCK_SCHEMA.items() 
                         if dtype == 'object']
        for col in string_columns:
            if col in self.data.columns:
                self.data[col] = self.data[col].str.strip()
        # Convert quantities to integers
        quantity_columns = ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']
        for col in quantity_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
                self.data[col] = self.data[col].fillna(0).astype('Int64')
        # Standardize date formats
        date_columns = ['DATA_DATE', 'EXPIRY_DATE']
        for col in date_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_datetime(self.data[col], errors='coerce')
    def to_spark_df(self, spark):
        """
        Convert pandas DataFrame to Spark DataFrame.
        Args:
            spark: SparkSession instance
        Returns:
            DataFrame: Spark DataFrame
        """
        return spark.createDataFrame(self.data)
    def validate_quantities(self):
        """Add quantity validation"""
        self.data['AVAILABLE_QUANTITY'] = pd.to_numeric(
            self.data['AVAILABLE_QUANTITY'], 
            errors='coerce'
        ).fillna(0)
        self.data = self.data[self.data['AVAILABLE_QUANTITY'] >= 0]
</file>

<file path="deployment/src/utils/__init__.py">
"""
Utility modules for common functionality.
"""
from .date_utils import parse_date, format_date, get_last_day_of_month, validate_date_range, get_date_parts
from .logging_utils import ETLLogger
from .data_processing import (
    clean_column_names,
    remove_empty_rows,
    remove_empty_columns,
    standardize_dates,
    standardize_numeric_columns,
    handle_duplicate_values,
    process_special_characters,
    aggregate_data,
    validate_mandatory_columns,
    fill_missing_values
)
from .s3_utils import S3Manager
__all__ = [
    'parse_date',
    'format_date',
    'get_last_day_of_month',
    'validate_date_range',
    'get_date_parts',
    'ETLLogger',
    'clean_column_names',
    'remove_empty_rows',
    'remove_empty_columns',
    'standardize_dates',
    'standardize_numeric_columns',
    'handle_duplicate_values',
    'process_special_characters',
    'aggregate_data',
    'validate_mandatory_columns',
    'fill_missing_values',
    'S3Manager'
]
</file>

<file path="deployment/src/utils/data_processing.py">
"""
Utility module for common data processing functions.
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Clean column names by removing whitespace and special characters.
    Args:
        df (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: DataFrame with cleaned column names
    """
    df.columns = df.columns.str.strip()
    df.columns = df.columns.str.upper()
    df.columns = df.columns.str.replace(' ', '_')
    return df
def remove_empty_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove rows where all values are null.
    Args:
        df (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: DataFrame with empty rows removed
    """
    return df.dropna(how='all')
def remove_empty_columns(df: pd.DataFrame, threshold: float = 0.9) -> pd.DataFrame:
    """
    Remove columns with high percentage of null values.
    Args:
        df (pd.DataFrame): Input DataFrame
        threshold (float): Threshold for null value percentage (default: 0.9)
    Returns:
        pd.DataFrame: DataFrame with empty columns removed
    """
    null_percent = df.isnull().sum() / len(df)
    cols_to_drop = null_percent[null_percent > threshold].index
    return df.drop(columns=cols_to_drop)
def standardize_dates(
    df: pd.DataFrame, 
    date_columns: List[str]
) -> pd.DataFrame:
    """
    Standardize date columns to datetime format.
    Args:
        df (pd.DataFrame): Input DataFrame
        date_columns (List[str]): List of date column names
    Returns:
        pd.DataFrame: DataFrame with standardized date columns
    """
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    return df
def standardize_numeric_columns(
    df: pd.DataFrame, 
    numeric_columns: List[str]
) -> pd.DataFrame:
    """
    Standardize numeric columns by converting to float and handling special cases.
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_columns (List[str]): List of numeric column names
    Returns:
        pd.DataFrame: DataFrame with standardized numeric columns
    """
    for col in numeric_columns:
        if col in df.columns:
            # Replace any non-numeric values with NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            # Replace negative values with 0 for quantity and value columns
            if any(x in col.upper() for x in ['QUANTITY', 'VALUE']):
                df[col] = df[col].clip(lower=0)
    return df
def handle_duplicate_values(
    df: pd.DataFrame,
    column: str,
    keep: str = 'first'
) -> pd.DataFrame:
    """
    Handle duplicate values in a specified column.
    Args:
        df (pd.DataFrame): Input DataFrame
        column (str): Column name to check for duplicates
        keep (str): Which duplicate to keep ('first', 'last', False)
    Returns:
        pd.DataFrame: DataFrame with handled duplicates
    """
    if df[column].duplicated().any():
        df = df.sort_values(by=[column])
        df[column] = df[column] + df.groupby(column).cumcount().astype(str)
        df[column] = df[column].str.replace('0', '')
    return df
def process_special_characters(
    df: pd.DataFrame,
    columns: List[str]
) -> pd.DataFrame:
    """
    Process special characters in specified columns.
    Args:
        df (pd.DataFrame): Input DataFrame
        columns (List[str]): List of column names to process
    Returns:
        pd.DataFrame: DataFrame with processed special characters
    """
    for col in columns:
        if col in df.columns:
            # Replace tilde with separate rows
            if df[col].str.contains('~', na=False).any():
                df = df.assign(**{col: df[col].str.split('~')}).explode(col)
            # Clean up other special characters
            df[col] = df[col].str.strip()
            df[col] = df[col].str.replace('[^\w\s-]', '', regex=True)
    return df
def aggregate_data(
    df: pd.DataFrame,
    group_columns: List[str],
    agg_columns: Dict[str, str]
) -> pd.DataFrame:
    """
    Aggregate data based on specified columns and aggregation functions.
    Args:
        df (pd.DataFrame): Input DataFrame
        group_columns (List[str]): Columns to group by
        agg_columns (Dict[str, str]): Dictionary of column names and aggregation functions
    Returns:
        pd.DataFrame: Aggregated DataFrame
    """
    return df.groupby(group_columns, as_index=False).agg(agg_columns)
def validate_mandatory_columns(
    df: pd.DataFrame,
    mandatory_columns: List[str]
) -> List[str]:
    """
    Validate presence of mandatory columns.
    Args:
        df (pd.DataFrame): Input DataFrame
        mandatory_columns (List[str]): List of mandatory column names
    Returns:
        List[str]: List of missing mandatory columns
    """
    return [col for col in mandatory_columns if col not in df.columns]
def fill_missing_values(
    df: pd.DataFrame,
    fill_values: Dict[str, Any]
) -> pd.DataFrame:
    """
    Fill missing values in specified columns.
    Args:
        df (pd.DataFrame): Input DataFrame
        fill_values (Dict[str, Any]): Dictionary of column names and fill values
    Returns:
        pd.DataFrame: DataFrame with filled missing values
    """
    for col, value in fill_values.items():
        if col in df.columns:
            df[col] = df[col].fillna(value)
    return df
</file>

<file path="deployment/src/utils/date_utils.py">
"""
Date handling utilities for the ETL job.
"""
from datetime import datetime, timedelta
import pandas as pd
from config.settings import DATE_FORMATS
from typing import Optional
def parse_date(date_str: str) -> Optional[datetime]:
    """
    Parse date string using multiple formats.
    Args:
        date_str (str): Date string to parse
    Returns:
        datetime: Parsed datetime object or None if parsing fails
    """
    if not date_str or pd.isna(date_str):
        return None
    date_str = str(date_str).strip()
    # Try pandas to_datetime first
    try:
        return pd.to_datetime(date_str)
    except:
        pass
    # Try all defined formats
    for fmt in DATE_FORMATS:
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    return None
def format_date(date: datetime, fmt: str = "%Y-%m-%d") -> str:
    """
    Format datetime object to string.
    Args:
        date (datetime): Date to format
        fmt (str): Output format
    Returns:
        str: Formatted date string
    """
    if not date:
        return None
    return date.strftime(fmt)
def get_last_day_of_month(date: datetime) -> datetime:
    """
    Get the last day of the month for a given date.
    Args:
        date (datetime): Input date
    Returns:
        datetime: Last day of the month
    """
    if date.month == 12:
        return date.replace(day=31)
    next_month = date.replace(day=1, month=date.month + 1)
    return next_month - timedelta(days=1)
def validate_date_range(date: datetime, min_date: Optional[datetime] = None, 
                       max_date: Optional[datetime] = None) -> bool:
    """
    Validate if date is within specified range.
    Args:
        date (datetime): Date to validate
        min_date (datetime, optional): Minimum allowed date
        max_date (datetime, optional): Maximum allowed date
    Returns:
        bool: True if date is within range
    """
    if not date:
        return False
    if min_date and date < min_date:
        return False
    if max_date and date > max_date:
        return False
    return True
def get_date_parts(date_obj):
    """
    Extract year, month, and day from a date object.
    Args:
        date_obj (datetime): Date to extract parts from
    Returns:
        tuple: (year, month, day) or None if extraction fails
    """
    if not date_obj:
        return None
    try:
        return date_obj.year, date_obj.month, date_obj.day
    except AttributeError:
        return None
</file>

<file path="deployment/src/utils/logging_utils.py">
"""
Logging utilities for the ETL job.
"""
import logging
import sys
from datetime import datetime
class ETLLogger:
    def __init__(self, job_name, log_level=logging.INFO):
        """
        Initialize the ETL logger.
        Args:
            job_name (str): Name of the ETL job
            log_level (int): Logging level (default: logging.INFO)
        """
        self.logger = logging.getLogger(job_name)
        self.logger.setLevel(log_level)
        # Create formatters and handlers
        self._setup_handlers()
        self.job_name = job_name
        self.start_time = None
        self.end_time = None
    def _setup_handlers(self):
        """Set up console and file handlers with formatters."""
        # Create formatters
        console_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(console_formatter)
        self.logger.addHandler(console_handler)
        # File handler
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_handler = logging.FileHandler(
            f'logs/etl_job_{timestamp}.log'
        )
        file_handler.setFormatter(file_formatter)
        self.logger.addHandler(file_handler)
    def start_job(self):
        """Log the start of the ETL job."""
        self.start_time = datetime.now()
        self.logger.info(f"Starting ETL job: {self.job_name}")
        self.logger.info(f"Start time: {self.start_time}")
    def end_job(self, status="completed"):
        """
        Log the end of the ETL job.
        Args:
            status (str): Job status (default: "completed")
        """
        self.end_time = datetime.now()
        duration = self.end_time - self.start_time if self.start_time else None
        self.logger.info(f"ETL job {status}: {self.job_name}")
        self.logger.info(f"End time: {self.end_time}")
        if duration:
            self.logger.info(f"Duration: {duration}")
    def log_step(self, step_name, status="started"):
        """
        Log an ETL step.
        Args:
            step_name (str): Name of the step
            status (str): Step status (default: "started")
        """
        self.logger.info(f"Step {status}: {step_name}")
    def log_error(self, error_msg, exc_info=None):
        """
        Log an error message.
        Args:
            error_msg (str): Error message
            exc_info (Exception, optional): Exception information
        """
        if exc_info:
            self.logger.error(error_msg, exc_info=exc_info)
        else:
            self.logger.error(error_msg)
    def log_warning(self, warning_msg):
        """
        Log a warning message.
        Args:
            warning_msg (str): Warning message
        """
        self.logger.warning(warning_msg)
    def log_info(self, info_msg):
        """
        Log an info message.
        Args:
            info_msg (str): Info message
        """
        self.logger.info(info_msg)
    def log_debug(self, debug_msg):
        """
        Log a debug message.
        Args:
            debug_msg (str): Debug message
        """
        self.logger.debug(debug_msg)
    def log_dataframe_info(self, df, df_name):
        """
        Log information about a DataFrame.
        Args:
            df: pandas or Spark DataFrame
            df_name (str): Name of the DataFrame
        """
        try:
            # Handle both pandas and Spark DataFrames
            if hasattr(df, 'shape'):  # pandas DataFrame
                rows, cols = df.shape
                self.logger.info(f"DataFrame {df_name}: {rows} rows, {cols} columns")
                self.logger.debug(f"Columns: {list(df.columns)}")
            else:  # Spark DataFrame
                rows = df.count()
                cols = len(df.columns)
                self.logger.info(f"DataFrame {df_name}: {rows} rows, {cols} columns")
                self.logger.debug(f"Columns: {df.columns}")
        except Exception as e:
            self.log_error(f"Error logging DataFrame info: {str(e)}")
    def log_column_mismatch(self, expected, actual):
        """Add detailed column mismatch logging"""
        self.log_error(
            f"Column mismatch found:\nExpected: {expected}\nActual: {actual}"
        )
</file>

<file path="deployment/src/utils/s3_utils.py">
"""
S3 utility functions for file operations.
"""
import pandas as pd
import s3fs
from typing import Union, Optional
class S3Manager:
    """Utility class for S3 operations."""
    def __init__(self, region_name: Optional[str] = None):
        """
        Initialize S3 manager.
        Args:
            region_name (str, optional): AWS region name
        """
        self.s3 = s3fs.S3FileSystem(anon=False)
        if region_name:
            self.s3.region_name = region_name
    def get_file(
        self, 
        s3_path: str,
        file_type: str = 'excel'
    ) -> pd.DataFrame:
        """
        Get file from S3.
        Args:
            s3_path (str): S3 path to the file
            file_type (str): Type of file ('excel' or 'parquet')
        Returns:
            pd.DataFrame: DataFrame containing file contents
        """
        with self.s3.open(s3_path, 'rb') as f:
            if file_type.lower() == 'excel':
                return pd.read_excel(f, engine='openpyxl')
            elif file_type.lower() == 'parquet':
                return pd.read_parquet(f)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
    def save_file(
        self, 
        data: pd.DataFrame,
        s3_path: str,
        file_type: str = 'parquet'
    ) -> None:
        """
        Save processed data back to S3.
        Args:
            data (pd.DataFrame): Data to save
            s3_path (str): S3 path to save to
            file_type (str): Type of file to save ('parquet' or 'excel')
        """
        with self.s3.open(s3_path, 'wb') as f:
            if file_type.lower() == 'parquet':
                data.to_parquet(f)
            elif file_type.lower() == 'excel':
                data.to_excel(f, engine='openpyxl', index=False)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
    def list_files(
        self, 
        s3_path: str,
        pattern: Optional[str] = None
    ) -> list:
        """
        List files in S3 path.
        Args:
            s3_path (str): S3 path to list
            pattern (str, optional): File pattern to match
        Returns:
            list: List of file paths
        """
        if pattern:
            return self.s3.glob(f"{s3_path}/{pattern}")
        return self.s3.ls(s3_path)
</file>

<file path="deployment/src/validation/__init__.py">
"""
Data validation module for schema and business rules.
"""
from .schema_validator import SchemaValidator
from .data_validator import StockDataValidator, SalesDataValidator, DataConsistencyValidator
__all__ = [
    'SchemaValidator',
    'StockDataValidator',
    'SalesDataValidator',
    'DataConsistencyValidator'
]
</file>

<file path="deployment/src/validation/data_validator.py">
"""
Business data validation module.
"""
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
class StockDataValidator:
    """Validator for stock data."""
    def __init__(self):
        """Initialize stock data validator."""
        self.errors = []
        self.warnings = []
        # Define required columns
        self.required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'AVAILABLE_QUANTITY'
        ]
        # Define numeric columns
        self.numeric_columns = [
            'AVAILABLE_QUANTITY',
            'BLOCKED_QUANTITY'
        ]
        # Define date columns
        self.date_columns = [
            'DATA_DATE',
            'EXPIRY_DATE'
        ]
        # Define categorical columns and their allowed values
        self.categorical_validations = {
            'INVENTORY_CATEGORY': ['GN', 'PR', 'PU']
        }
    def validate_data(self, df: pd.DataFrame) -> bool:
        """
        Validate stock data.
        Args:
            df (pd.DataFrame): DataFrame to validate
        Returns:
            bool: True if validation passes, False otherwise
        """
        is_valid = True
        # Check required columns
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        if missing_columns:
            self.errors.append(f"Missing required columns: {missing_columns}")
            is_valid = False
        # Check numeric columns
        for col in self.numeric_columns:
            if col in df.columns:
                non_numeric = df[~pd.to_numeric(df[col], errors='coerce').notnull()][col]
                if not non_numeric.empty:
                    self.errors.append(f"Non-numeric values found in {col}: {non_numeric.unique()}")
                    is_valid = False
        # Check date columns
        for col in self.date_columns:
            if col in df.columns:
                non_dates = df[~pd.to_datetime(df[col], errors='coerce').notnull()][col]
                if not non_dates.empty:
                    self.errors.append(f"Invalid dates found in {col}: {non_dates.unique()}")
                    is_valid = False
        # Check categorical values
        for col, allowed_values in self.categorical_validations.items():
            if col in df.columns:
                invalid_values = df[df[col].notna()][~df[col].isin(allowed_values)][col].unique()
                if len(invalid_values) > 0:
                    self.errors.append(f"Invalid values in {col}: {invalid_values}")
                    is_valid = False
        # Check for negative quantities
        for col in ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']:
            if col in df.columns:
                negative_values = df[df[col] < 0]
                if not negative_values.empty:
                    self.warnings.append(f"Negative values found in {col}")
        return is_valid
class SalesDataValidator:
    """Validator for sales data."""
    def __init__(self):
        """Initialize sales data validator."""
        self.errors = []
        self.warnings = []
        # Define required columns
        self.required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'SALES_QUANTITY',
            'SALES_VALUE'
        ]
        # Define numeric columns
        self.numeric_columns = [
            'SALES_QUANTITY',
            'RETURN_QUANTITY',
            'SALES_VALUE',
            'RETURN_VALUE',
            'TAX_IDENTIFICATION_NUMBER'
        ]
        # Define date columns
        self.date_columns = [
            'DATA_DATE',
            'INVOICE_DATE'
        ]
        # Define categorical columns and their allowed values
        self.categorical_validations = {
            'SALES_CATEGORY': ['GN', 'PR', 'PU']
        }
    def validate_data(self, df: pd.DataFrame) -> bool:
        """
        Validate sales data.
        Args:
            df (pd.DataFrame): DataFrame to validate
        Returns:
            bool: True if validation passes, False otherwise
        """
        is_valid = True
        # Check required columns
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        if missing_columns:
            self.errors.append(f"Missing required columns: {missing_columns}")
            is_valid = False
        # Check numeric columns
        for col in self.numeric_columns:
            if col in df.columns:
                non_numeric = df[~pd.to_numeric(df[col], errors='coerce').notnull()][col]
                if not non_numeric.empty:
                    self.errors.append(f"Non-numeric values found in {col}: {non_numeric.unique()}")
                    is_valid = False
        # Check date columns
        for col in self.date_columns:
            if col in df.columns:
                non_dates = df[~pd.to_datetime(df[col], errors='coerce').notnull()][col]
                if not non_dates.empty:
                    self.errors.append(f"Invalid dates found in {col}: {non_dates.unique()}")
                    is_valid = False
        # Check categorical values
        for col, allowed_values in self.categorical_validations.items():
            if col in df.columns:
                invalid_values = df[df[col].notna()][~df[col].isin(allowed_values)][col].unique()
                if len(invalid_values) > 0:
                    self.errors.append(f"Invalid values in {col}: {invalid_values}")
                    is_valid = False
        # Check for negative quantities and values
        if 'SALES_QUANTITY' in df.columns and 'RETURN_QUANTITY' in df.columns:
            net_quantity = df['SALES_QUANTITY'] - df['RETURN_QUANTITY']
            if (net_quantity < 0).any():
                self.warnings.append("Return quantity exceeds sales quantity")
        if 'SALES_VALUE' in df.columns and 'RETURN_VALUE' in df.columns:
            net_value = df['SALES_VALUE'] - df['RETURN_VALUE']
            if (net_value < 0).any():
                self.warnings.append("Return value exceeds sales value")
        return is_valid
class DataConsistencyValidator:
    """Validator for data consistency across related datasets."""
    def validate_stock_sales_consistency(
        self, 
        stock_df: pd.DataFrame, 
        sales_df: pd.DataFrame
    ) -> List[str]:
        """
        Validate consistency between stock and sales data.
        Args:
            stock_df (pd.DataFrame): Stock DataFrame
            sales_df (pd.DataFrame): Sales DataFrame
        Returns:
            List[str]: List of inconsistency warnings
        """
        warnings = []
        # Check product consistency
        stock_products = set(stock_df['PRODUCT_ID'].unique())
        sales_products = set(sales_df['PRODUCT_ID'].unique())
        products_in_sales_not_stock = sales_products - stock_products
        if products_in_sales_not_stock:
            warnings.append(
                f"Products in sales but not in stock: {products_in_sales_not_stock}"
            )
        # Check organization consistency
        stock_orgs = set(stock_df['ORGANIZATION_NAME'].unique())
        sales_orgs = set(sales_df['ORGANIZATION_NAME'].unique())
        orgs_mismatch = stock_orgs.symmetric_difference(sales_orgs)
        if orgs_mismatch:
            warnings.append(
                f"Organizations not matching between stock and sales: {orgs_mismatch}"
            )
        return warnings
</file>

<file path="deployment/src/validation/schema_validator.py">
"""
Schema validation for data frames.
"""
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
from src.utils.date_utils import validate_date_range
class SchemaValidator:
    """Validator for DataFrame schemas."""
    def __init__(self, schema: Dict[str, str]):
        """
        Initialize schema validator.
        Args:
            schema (Dict[str, str]): Schema dictionary mapping column names to data types
        """
        self.schema = schema
        self.errors = []
    def validate_columns(self, df: pd.DataFrame) -> bool:
        """
        Validate presence and types of columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
        Returns:
            bool: True if validation passes, False otherwise
        """
        # Check for missing columns
        missing_columns = [col for col in self.schema.keys() if col not in df.columns]
        if missing_columns:
            self.errors.append(f"Missing columns: {missing_columns}")
            return False
        # Check column types
        for col, dtype in self.schema.items():
            try:
                if dtype == 'datetime64[ns]':
                    pd.to_datetime(df[col], errors='raise')
                elif dtype == 'Int64':
                    pd.to_numeric(df[col], errors='raise')
                elif dtype == 'float64':
                    pd.to_numeric(df[col], errors='raise')
            except (ValueError, TypeError):
                self.errors.append(f"Invalid type for column {col}. Expected {dtype}")
                return False
        return True
    def validate_mandatory_values(self, df: pd.DataFrame, mandatory_columns: List[str]) -> bool:
        """
        Validate presence of values in mandatory columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            mandatory_columns (List[str]): List of mandatory column names
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col in mandatory_columns:
            if col not in df.columns:
                self.errors.append(f"Missing mandatory column: {col}")
                return False
            null_count = df[col].isnull().sum()
            if null_count > 0:
                self.errors.append(f"Found {null_count} null values in mandatory column {col}")
                return False
        return True
    def validate_numeric_ranges(
        self, 
        df: pd.DataFrame, 
        range_validations: Dict[str, Dict[str, float]]
    ) -> bool:
        """
        Validate numeric ranges for specified columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            range_validations (Dict[str, Dict[str, float]]): Dictionary mapping column names
                to their min/max range values
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col, ranges in range_validations.items():
            if col not in df.columns:
                continue
            min_val = ranges.get('min')
            max_val = ranges.get('max')
            if min_val is not None and (df[col] < min_val).any():
                self.errors.append(f"Values below minimum {min_val} found in column {col}")
                return False
            if max_val is not None and (df[col] > max_val).any():
                self.errors.append(f"Values above maximum {max_val} found in column {col}")
                return False
        return True
    def validate_date_ranges(
        self, 
        df: pd.DataFrame, 
        date_columns: List[str],
        min_date: Optional[datetime] = None,
        max_date: Optional[datetime] = None
    ) -> bool:
        """
        Validate date ranges for specified columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            date_columns (List[str]): List of date column names
            min_date (datetime, optional): Minimum allowed date
            max_date (datetime, optional): Maximum allowed date
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col in date_columns:
            if col not in df.columns:
                continue
            dates = pd.to_datetime(df[col], errors='coerce')
            if min_date is not None and (dates < min_date).any():
                self.errors.append(f"Dates before {min_date} found in column {col}")
                return False
            if max_date is not None and (dates > max_date).any():
                self.errors.append(f"Dates after {max_date} found in column {col}")
                return False
        return True
    def validate_categorical_values(
        self, 
        df: pd.DataFrame, 
        categorical_validations: Dict[str, List[str]]
    ) -> bool:
        """
        Validate categorical values for specified columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            categorical_validations (Dict[str, List[str]]): Dictionary mapping column names
                to their allowed values
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col, allowed_values in categorical_validations.items():
            if col not in df.columns:
                continue
            invalid_values = df[df[col].notna()][~df[col].isin(allowed_values)][col].unique()
            if len(invalid_values) > 0:
                self.errors.append(
                    f"Invalid values found in column {col}: {invalid_values}"
                )
                return False
        return True
    def get_validation_errors(self) -> List[str]:
        """
        Get list of validation errors.
        Returns:
            List[str]: List of error messages
        """
        return self.errors 
    def validate_mandatory_columns(self, df, context):
        """Add column count validation"""
        mandatory_cols = self.get_mandatory_columns(context)
        matching_cols = [col for col in df.columns if col in mandatory_cols]
        return len(matching_cols) == len(mandatory_cols) 
    def validate_dates(self, df: pd.DataFrame, date_columns: Dict[str, Dict]) -> bool:
        """
        Validate date columns in DataFrame.
        Args:
            df (pd.DataFrame): DataFrame to validate
            date_columns (Dict[str, Dict]): Dictionary mapping column names to validation rules
        Returns:
            bool: True if validation passes
        """
        for col, rules in date_columns.items():
            if col not in df.columns:
                continue
            min_date = rules.get('min_date')
            max_date = rules.get('max_date')
            invalid_dates = df[~df[col].apply(
                lambda x: validate_date_range(x, min_date, max_date)
            )]
            if not invalid_dates.empty:
                self.errors.append(
                    f"Invalid dates found in column {col}"
                )
                return False
        return True
</file>

<file path="deployment/file_standardization_job.py">
"""
Main ETL job script for file standardization.
"""
import sys
import os
# Add deployment package to Python path
if 'GLUE_DEPLOYMENT_PATH' in os.environ:
    deployment_path = os.environ['GLUE_DEPLOYMENT_PATH']
else:
    # Default to the script's directory
    deployment_path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(deployment_path)
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from src.database.connection import DatabaseConnection
from src.database.operations import DatabaseOperations
from src.etl.extractors import DataExtractor
from src.etl.transformers import DataTransformer
from src.etl.loaders import DataLoader
from src.utils.logging_utils import ETLLogger
def run_etl_job():
    """Main ETL job execution function."""
    # Initialize logging
    logger = ETLLogger("FileStandardizationJob")
    logger.start_job()
    try:
        # Get job parameters
        args = getResolvedOptions(sys.argv, ['JOB_NAME', 'secret_name', 'env'])
        # Initialize Spark context
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        logger.log_info(f"Environment: {args['env']}")
        # Initialize components
        db_connection = DatabaseConnection(args['secret_name'])
        db_operations = DatabaseOperations(db_connection)
        extractor = DataExtractor(args['env'], logger)
        transformer = DataTransformer(logger)
        loader = DataLoader(db_connection, logger)
        # Get unprocessed files
        logger.log_step("Fetching unprocessed files")
        unprocessed_files = db_operations.get_unprocessed_files()
        if not unprocessed_files.count():
            logger.log_info("No unprocessed files found")
            return
        # Process each file
        for file_row in unprocessed_files.collect():
            try:
                logger.log_step(f"Processing file: {file_row.file}")
                # Get entity details using sequential matching
                entity_details = db_operations.get_entity_details(
                    daq_sheet_name=file_row.daq_sheet_name,
                    sender_address=file_row.sender_address,
                    file_extension=file_row.file_extension,
                    receiver_address=file_row.receiver_address,
                    file_id=file_row.id
                )
                if not entity_details.count():
                    logger.log_warning(f"No entity details found for file: {file_row.file}")
                    continue
                # Process each entity
                for entity_row in entity_details.collect():
                    try:
                        logger.log_step(f"Processing entity: {entity_row.data_owner}")
                        # Get attribute details
                        attribute_details = db_operations.get_attribute_details(
                            entity_row.data_owner,
                            entity_row.context,
                            entity_row.entity_file_table_name,
                            entity_row.entity_sheet_name
                        ).collect()
                        # Extract data
                        file_info = {
                            'file_path': file_row.file,
                            'file_type': file_row.file_extension,
                            'sheet_name': file_row.daq_sheet_name,
                            'data_owner': entity_row.data_owner,
                            'country': entity_row.country
                        }
                        raw_data = extractor.extract_data(file_info)
                        if raw_data is None:
                            continue
                        # Transform data based on context
                        if entity_row.context.upper() == 'STOCK':
                            processed_data = transformer.transform_stock_data(
                                raw_data,
                                attribute_details
                            )
                            if processed_data:
                                success = loader.load_stock_data(
                                    processed_data,
                                    f"temp_{file_row.id}"
                                )
                        else:  # SALES
                            processed_data = transformer.transform_sales_data(
                                raw_data,
                                attribute_details
                            )
                            if processed_data:
                                success = loader.load_sales_data(
                                    processed_data,
                                    f"temp_{file_row.id}"
                                )
                        if success:
                            # Insert load info
                            db_operations.insert_temp_load_info(
                                f"temp_{file_row.id}",
                                entity_row.context,
                                entity_row.frequency,
                                1 if entity_row.context.upper() == 'SALES' else 0,
                                1
                            )
                    except Exception as entity_error:
                        logger.log_error(
                            f"Error processing entity {entity_row.data_owner}: {str(entity_error)}",
                            exc_info=entity_error
                        )
                        continue
                # Mark file as processed
                db_operations.mark_file_as_processed(file_row.id)
            except Exception as file_error:
                logger.log_error(
                    f"Error processing file {file_row.file}: {str(file_error)}",
                    exc_info=file_error
                )
                continue
        # Cleanup
        loader.cleanup_temp_tables()
        logger.end_job()
        job.commit()
    except Exception as e:
        logger.log_error("Job failed", exc_info=e)
        logger.end_job("failed")
        raise
if __name__ == "__main__":
    run_etl_job()
</file>

<file path="src/business/__init__.py">
"""
Business logic module for data processing and validation.
"""
from .rules import DataOwnerRules, TransformationRules, ValidationRules, SpecialCaseRules
from .transformations import StockTransformations, SalesTransformations, DateTransformations
from .special_cases import QuimicaSuizaHandler, RafedHandler, CitypharmacyHandler, KuwaitHandler
__all__ = [
    'DataOwnerRules',
    'TransformationRules',
    'ValidationRules',
    'SpecialCaseRules',
    'StockTransformations',
    'SalesTransformations',
    'DateTransformations',
    'QuimicaSuizaHandler',
    'RafedHandler',
    'CitypharmacyHandler',
    'KuwaitHandler'
]
</file>

<file path="src/business/rules.py">
"""
Business rules for data validation and transformation.
"""
from typing import Dict, List, Any
import pandas as pd
import numpy as np
class DataOwnerRules:
    """Rules for specific data owners."""
    @staticmethod
    def validate_alliance(data_owner: str, country: str, sheet_name: str) -> bool:
        """
        Validate ALLIANCE data.
        Args:
            data_owner (str): Data owner name
            country (str): Country name
            sheet_name (str): Sheet name
        Returns:
            bool: True if valid, False otherwise
        """
        return (
            data_owner == 'ALLIANCE' 
            and country == 'TURKIYE' 
            and sheet_name.lower() == 'urundepobazinda'
        )
    @staticmethod
    def validate_rafed(data_owner: str, country: str, sheet_name: str) -> bool:
        """
        Validate RAFED data.
        Args:
            data_owner (str): Data owner name
            country (str): Country name
            sheet_name (str): Sheet name
        Returns:
            bool: True if valid, False otherwise
        """
        valid_sheets = ['mafraq-ssmc data', 'tawam data']
        return (
            data_owner == 'RAFED'
            and country == 'UAE'
            and sheet_name.lower() in valid_sheets
        )
class TransformationRules:
    """Rules for data transformations."""
    @staticmethod
    def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
        """
        Clean and standardize column names.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with cleaned column names
        """
        df = df.copy()
        df.columns = (
            df.columns.str.strip()
            .str.replace(r'[^\w\s]', '', regex=True)
            .str.replace(r'\s+', '_', regex=True)
            .str.upper()
        )
        return df
    @staticmethod
    def remove_empty_columns(df: pd.DataFrame) -> pd.DataFrame:
        """
        Remove empty columns from DataFrame.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with empty columns removed
        """
        df = df.copy()
        for col in df.columns:
            if df[col].astype(str).str.strip().eq('').all():
                df = df.drop(columns=[col])
        return df
    @staticmethod
    def remove_empty_rows(df: pd.DataFrame) -> pd.DataFrame:
        """
        Remove empty rows from DataFrame.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with empty rows removed
        """
        df = df.copy()
        return df.dropna(how='all').reset_index(drop=True)
class ValidationRules:
    """Rules for data validation."""
    @staticmethod
    def validate_mandatory_columns(df: pd.DataFrame, required_columns: List[str]) -> bool:
        """
        Validate presence of mandatory columns.
        Args:
            df (pd.DataFrame): Input DataFrame
            required_columns (List[str]): List of required column names
        Returns:
            bool: True if all required columns present, False otherwise
        """
        return all(col in df.columns for col in required_columns)
    @staticmethod
    def validate_numeric_columns(df: pd.DataFrame, numeric_columns: List[str]) -> bool:
        """
        Validate numeric columns.
        Args:
            df (pd.DataFrame): Input DataFrame
            numeric_columns (List[str]): List of numeric column names
        Returns:
            bool: True if all numeric columns are valid, False otherwise
        """
        for col in numeric_columns:
            if col in df.columns:
                if not pd.to_numeric(df[col], errors='coerce').notnull().all():
                    return False
        return True
class SpecialCaseRules:
    """Rules for special cases."""
    @staticmethod
    def process_kuwait_data(df: pd.DataFrame) -> pd.DataFrame:
        """
        Process data for Kuwait.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        new_rows = []
        for _, row in df.iterrows():
            # Find columns containing '~'
            split_cols = [col for col in df.columns if '~' in str(row[col])]
            if not split_cols:
                new_rows.append(row)
                continue
            # Find max splits needed
            max_splits = max(len(str(row[col]).split('~')) for col in split_cols)
            # Process splits
            for i in range(max_splits):
                new_row = row.copy()
                for col in df.columns:
                    if col in split_cols:
                        splits = str(row[col]).split('~')
                        new_row[col] = splits[i] if i < len(splits) else None
                new_rows.append(new_row)
        return pd.DataFrame(new_rows)
    @staticmethod
    def process_citypharmacy_data(df: pd.DataFrame, context: str) -> pd.DataFrame:
        """
        Process data for Citypharmacy.
        Args:
            df (pd.DataFrame): Input DataFrame
            context (str): Processing context (STOCK/SALES)
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if context == 'STOCK':
            # Filter out 'I' category
            if 'STOCK_CATEGORY' in df.columns:
                df = df[df['STOCK_CATEGORY'] != 'I']
            # Update inventory category
            if 'INVENTORY_CATEGORY' in df.columns:
                df['INVENTORY_CATEGORY'] = df.apply(
                    lambda row: 'PU' if row.get('STOCK_CATEGORY') == 'I'
                    else ('PR' if row.get('STOCK_CATEGORY') == 'N' else 'GN'),
                    axis=1
                )
        elif context == 'SALES':
            # Update sales category
            if 'SALES_CATEGORY' in df.columns:
                df['SALES_CATEGORY'] = df.apply(
                    lambda row: 'PU' if row.get('SALES_CATEGORY') == 'I'
                    else ('PR' if row.get('SALES_CATEGORY') == 'N' else 'GN'),
                    axis=1
                )
        return df 
class BusinessRules:
    def apply_organization_rules(self, df, data_owner):
        """Add organization-specific rules"""
        if data_owner == 'ALLIANCE':
            df = self.handle_alliance_data(df)
        elif data_owner == 'RAFED':
            df = self.handle_rafed_data(df)
        return df
</file>

<file path="src/business/special_cases.py">
"""
Special case handling for specific organizations and countries.
"""
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
from decimal import Decimal
class QuimicaSuizaHandler:
    """Handler for Quimica Suiza specific processing."""
    @staticmethod
    def process_stock_data(df: pd.DataFrame) -> pd.DataFrame:
        """
        Process stock data for Quimica Suiza.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        # Fill NULL values in BATCH_NUMBER with 'NA'
        if 'BATCH_NUMBER' in df.columns:
            df['BATCH_NUMBER'] = df['BATCH_NUMBER'].fillna('NA')
        # Process INVENTORY_CATEGORY based on BRANCH_NAME
        if 'BRANCH_NAME' in df.columns:
            df['INVENTORY_CATEGORY'] = df['BRANCH_NAME'].apply(
                lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
            )
        # Calculate BLOCKED_QUANTITY from transit quantities
        transit_cols = ['QUANTITY_IN_TRANSIT1', 'QUANTITY_IN_TRANSIT2']
        if all(col in df.columns for col in transit_cols):
            df['BLOCKED_QUANTITY'] = (
                pd.to_numeric(df['QUANTITY_IN_TRANSIT1'], errors='coerce').fillna(0) +
                pd.to_numeric(df['QUANTITY_IN_TRANSIT2'], errors='coerce').fillna(0)
            ) * 1000
        # Add missing header row handling
        if len(df.columns) == len(df.iloc[0].unique()):
            empty_row = pd.DataFrame([{col: None for col in df.columns}])
            df = pd.concat([empty_row, df], ignore_index=True)
        return df
    @staticmethod
    def process_sales_data(df: pd.DataFrame) -> pd.DataFrame:
        """
        Process sales data for Quimica Suiza.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        # Process SALES_CATEGORY based on BRANCH_NAME
        if 'BRANCH_NAME' in df.columns:
            df['SALES_CATEGORY'] = df['BRANCH_NAME'].apply(
                lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
            )
        # Scale quantity columns
        quantity_cols = ['SALES_QUANTITY', 'RETURN_QUANTITY']
        for col in quantity_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) * 1000
        # Scale value columns
        value_cols = ['SALES_VALUE', 'RETURN_VALUE']
        for col in value_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) * 1000
        return df
class RafedHandler:
    """Handler for RAFED specific processing."""
    @staticmethod
    def process_organization(df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:
        """
        Process organization name based on sheet name.
        Args:
            df (pd.DataFrame): Input DataFrame
            sheet_name (str): Sheet name
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if sheet_name.lower() == 'tawam data':
            df['ORGANIZATION_NAME'] = 'TAWAM'
            if 'SALES_QUANTITY' in df.columns:
                df['SALES_QUANTITY'] = df['SALES_QUANTITY'] * -1
        elif sheet_name.lower() == 'mafraq-ssmc data':
            df['ORGANIZATION_NAME'] = 'SSMC'
        return df
class CitypharmacyHandler:
    """Handler for Citypharmacy specific processing."""
    @staticmethod
    def process_branch_name(df: pd.DataFrame, country: str) -> pd.DataFrame:
        """
        Process branch name based on region.
        Args:
            df (pd.DataFrame): Input DataFrame
            country (str): Country name
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if 'REGION_NAME' in df.columns and country == 'UAE':
            df['REGION_NAME'] = df['REGION_NAME'].str.upper()
            df['BRANCH_NAME'] = df.apply(
                lambda row: (
                    f"{row['ORGANIZATION_NAME']} KIZAD"
                    if pd.notna(row['REGION_NAME']) and row['REGION_NAME'] in ['ABU DHABI', 'AL AIN']
                    else (
                        f"{row['ORGANIZATION_NAME']} SHARJAH"
                        if pd.notna(row['REGION_NAME'])
                        else f"{row['ORGANIZATION_NAME']} SHARJAH"
                    )
                ),
                axis=1
            )
            df = df.drop(columns=['REGION_NAME'])
        return df
    @staticmethod
    def process_categories(df: pd.DataFrame, context: str) -> pd.DataFrame:
        """
        Process inventory/sales categories.
        Args:
            df (pd.DataFrame): Input DataFrame
            context (str): Processing context (STOCK/SALES)
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        df = df.copy()
        if context == 'STOCK':
            if 'STOCK_CATEGORY' in df.columns:
                df['INVENTORY_CATEGORY'] = df['STOCK_CATEGORY'].apply(
                    lambda x: 'PU' if x == 'I' else ('PR' if x == 'N' else 'GN')
                )
                df = df.drop(columns=['STOCK_CATEGORY'])
        elif context == 'SALES':
            if 'SALES_CATEGORY' in df.columns:
                df['SALES_CATEGORY'] = df['SALES_CATEGORY'].apply(
                    lambda x: 'PU' if x == 'I' else ('PR' if x == 'N' else 'GN')
                )
        return df
class KuwaitHandler:
    """Handler for Kuwait specific processing."""
    @staticmethod
    def split_tilde_rows(df: pd.DataFrame) -> pd.DataFrame:
        """
        Split rows containing tilde (~) character.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: Processed DataFrame with split rows
        """
        df = df.copy()
        new_rows = []
        for _, row in df.iterrows():
            # Find columns containing tilde
            tilde_cols = [col for col in df.columns if '~' in str(row[col])]
            if not tilde_cols:
                new_rows.append(row)
                continue
            # Find maximum number of splits needed
            max_splits = max(len(str(row[col]).split('~')) for col in tilde_cols)
            # Create new rows for each split
            for i in range(max_splits):
                new_row = row.copy()
                for col in df.columns:
                    if col in tilde_cols:
                        splits = str(row[col]).split('~')
                        new_row[col] = splits[i] if i < len(splits) else None
                    else:
                        # Handle numeric values
                        if str(row[col]).isdigit() and str(col) != '0':
                            new_row[col] = row[col] if i == 0 else None
                new_rows.append(new_row)
        return pd.DataFrame(new_rows)
</file>

<file path="src/business/transformations.py">
"""
Business-specific data transformations.
"""
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from decimal import Decimal
from datetime import datetime, date
class StockTransformations:
    """Transformations for stock data."""
    @staticmethod
    def standardize_quantities(df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize quantity columns in stock data.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with standardized quantities
        """
        df = df.copy()
        quantity_columns = ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']
        for col in quantity_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0).astype('Int64')
        return df
    @staticmethod
    def process_inventory_category(df: pd.DataFrame, data_owner: str) -> pd.DataFrame:
        """
        Process inventory category based on data owner rules.
        Args:
            df (pd.DataFrame): Input DataFrame
            data_owner (str): Data owner name
        Returns:
            pd.DataFrame: DataFrame with processed inventory category
        """
        df = df.copy()
        if 'INVENTORY_CATEGORY' not in df.columns:
            df['INVENTORY_CATEGORY'] = 'GN'
        if data_owner == 'QUIMICA SUIZA':
            if 'BRANCH_NAME' in df.columns:
                df['INVENTORY_CATEGORY'] = df['BRANCH_NAME'].apply(
                    lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                    else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
                )
        return df
class SalesTransformations:
    """Transformations for sales data."""
    @staticmethod
    def standardize_quantities(df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize quantity columns in sales data.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with standardized quantities
        """
        df = df.copy()
        # Process sales quantity
        if 'SALES_QUANTITY' in df.columns:
            df['SALES_QUANTITY'] = pd.to_numeric(df['SALES_QUANTITY'], errors='coerce')
            df['SALES_QUANTITY'] = df['SALES_QUANTITY'].fillna(0).astype('Int64')
        # Process return quantity
        if 'RETURN_QUANTITY' in df.columns:
            df['RETURN_QUANTITY'] = pd.to_numeric(df['RETURN_QUANTITY'], errors='coerce')
            df['RETURN_QUANTITY'] = df['RETURN_QUANTITY'].fillna(0).astype('Int64')
        return df
    @staticmethod
    def standardize_values(df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize monetary value columns in sales data.
        Args:
            df (pd.DataFrame): Input DataFrame
        Returns:
            pd.DataFrame: DataFrame with standardized values
        """
        df = df.copy()
        value_columns = ['SALES_VALUE', 'RETURN_VALUE']
        for col in value_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0).astype('float64')
                df[col] = df[col].apply(Decimal)
        return df
    @staticmethod
    def process_sales_category(df: pd.DataFrame, data_owner: str) -> pd.DataFrame:
        """
        Process sales category based on data owner rules.
        Args:
            df (pd.DataFrame): Input DataFrame
            data_owner (str): Data owner name
        Returns:
            pd.DataFrame: DataFrame with processed sales category
        """
        df = df.copy()
        if 'SALES_CATEGORY' not in df.columns:
            df['SALES_CATEGORY'] = 'GN'
        if data_owner == 'QUIMICA SUIZA':
            if 'BRANCH_NAME' in df.columns:
                df['SALES_CATEGORY'] = df['BRANCH_NAME'].apply(
                    lambda x: 'PR' if 'PRIVATE' in str(x).upper()
                    else ('PU' if 'PUBLIC' in str(x).upper() else 'GN')
                )
        return df
class DateTransformations:
    """Transformations for date fields."""
    @staticmethod
    def standardize_dates(df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:
        """
        Standardize date columns.
        Args:
            df (pd.DataFrame): Input DataFrame
            date_columns (List[str]): List of date column names
        Returns:
            pd.DataFrame: DataFrame with standardized dates
        """
        df = df.copy()
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                df[col] = df[col].fillna(pd.Timestamp('2199-12-31'))
        return df
    @staticmethod
    def process_data_date(df: pd.DataFrame, frequency: str, mail_date: datetime) -> pd.DataFrame:
        """
        Process DATA_DATE based on frequency.
        Args:
            df (pd.DataFrame): Input DataFrame
            frequency (str): Data frequency (MONTHLY/WEEKLY/DAILY)
            mail_date (datetime): Mail date
        Returns:
            pd.DataFrame: DataFrame with processed DATA_DATE
        """
        df = df.copy()
        if 'DATA_DATE' not in df.columns:
            if frequency == 'MONTHLY':
                # Set to last day of previous month
                first_day = date(mail_date.year, mail_date.month, 1)
                last_day = first_day - pd.Timedelta(days=1)
                df['DATA_DATE'] = last_day
            else:
                df['DATA_DATE'] = mail_date
        return df
</file>

<file path="src/config/validation_config.py">
"""
Configuration module for data validation rules and constants.
"""
from typing import Dict, List, Any
from datetime import datetime, date
# Stock Data Validation Rules
STOCK_VALIDATION_RULES = {
    'required_columns': [
        'DATA_DATE',
        'COUNTRY_NAME',
        'ORGANIZATION_NAME',
        'PRODUCT_ID',
        'AVAILABLE_QUANTITY'
    ],
    'numeric_columns': [
        'AVAILABLE_QUANTITY',
        'BLOCKED_QUANTITY'
    ],
    'date_columns': [
        'DATA_DATE',
        'EXPIRY_DATE'
    ],
    'categorical_columns': {
        'INVENTORY_CATEGORY': ['GN', 'PR', 'PU']
    },
    'min_date': date(2020, 1, 1),
    'max_date': datetime.now().date()
}
# Sales Data Validation Rules
SALES_VALIDATION_RULES = {
    'required_columns': [
        'DATA_DATE',
        'COUNTRY_NAME',
        'ORGANIZATION_NAME',
        'PRODUCT_ID',
        'SALES_QUANTITY',
        'SALES_VALUE'
    ],
    'numeric_columns': [
        'SALES_QUANTITY',
        'RETURN_QUANTITY',
        'SALES_VALUE',
        'RETURN_VALUE',
        'TAX_IDENTIFICATION_NUMBER'
    ],
    'date_columns': [
        'DATA_DATE',
        'INVOICE_DATE'
    ],
    'categorical_columns': {
        'SALES_CATEGORY': ['GN', 'PR', 'PU']
    },
    'min_date': date(2020, 1, 1),
    'max_date': datetime.now().date()
}
# Organization-specific Rules
ORGANIZATION_RULES = {
    'CITYPHARMACY': {
        'required_branch_name': True,
        'allowed_categories': ['GN', 'PR', 'PU'],
        'country_specific_rules': {
            'KUWAIT': {
                'split_tilde_rows': True
            }
        }
    },
    'RAFED': {
        'sheet_name_mapping': {
            'Stock': 'RAFED',
            'Sales': 'RAFED'
        }
    },
    'ALLIANCE': {
        'country_specific_rules': {
            'KUWAIT': {
                'process_branch_name': True
            }
        }
    }
}
# Data Type Mappings
COLUMN_DATA_TYPES = {
    'DATA_DATE': 'datetime64[ns]',
    'COUNTRY_NAME': 'str',
    'ORGANIZATION_NAME': 'str',
    'PRODUCT_ID': 'str',
    'AVAILABLE_QUANTITY': 'float64',
    'BLOCKED_QUANTITY': 'float64',
    'SALES_QUANTITY': 'float64',
    'RETURN_QUANTITY': 'float64',
    'SALES_VALUE': 'float64',
    'RETURN_VALUE': 'float64',
    'TAX_IDENTIFICATION_NUMBER': 'str'
}
# Error Messages
ERROR_MESSAGES = {
    'missing_columns': "Missing required columns: {columns}",
    'invalid_numeric': "Non-numeric values found in {column}: {values}",
    'invalid_date': "Invalid dates found in {column}: {values}",
    'invalid_category': "Invalid values in {column}: {values}",
    'negative_quantity': "Negative values found in {column}",
    'return_exceeds_sales': "Return {type} exceeds sales {type}",
    'product_mismatch': "Products in sales but not in stock: {products}",
    'org_mismatch': "Organizations not matching between stock and sales: {orgs}"
}
</file>

<file path="src/database/__init__.py">
"""
Database module for connection and operations.
"""
from .connection import DatabaseConnection
from .operations import DatabaseOperations
__all__ = [
    'DatabaseConnection',
    'DatabaseOperations'
]
</file>

<file path="src/database/connection.py">
"""
Database connection handling module.
"""
import json
import boto3
from pyspark.sql import SparkSession
from config.settings import DB_CONFIG, AWS_REGION
class DatabaseConnection:
    def __init__(self, secret_name):
        """
        Initialize database connection with AWS Secrets Manager credentials.
        Args:
            secret_name (str): Name of the secret in AWS Secrets Manager
        """
        self.secret_name = secret_name
        self.credentials = self._get_secret()
        self.jdbc_url = f"jdbc:postgresql://{self.credentials['host']}:{self.credentials['port']}/{self.credentials['dbname']}"
        self.jdbc_properties = {
            'user': self.credentials['username'],
            'password': self.credentials['password'],
            'driver': DB_CONFIG['driver']
        }
    def _get_secret(self):
        """
        Retrieve database credentials from AWS Secrets Manager.
        Returns:
            dict: Database credentials
        """
        client = boto3.client('secretsmanager', region_name=AWS_REGION)
        response = client.get_secret_value(SecretId=self.secret_name)
        return json.loads(response['SecretString'])
    def get_spark_session(self):
        """
        Get or create a Spark session.
        Returns:
            SparkSession: Active Spark session
        """
        return SparkSession.builder.appName("PostgreSQL Connection").getOrCreate()
    def execute_query(self, query, params=None):
        """
        Execute a SQL query using Spark JDBC connection.
        Args:
            query (str): SQL query to execute
            params (dict, optional): Parameters to format the query with
        Returns:
            DataFrame: Spark DataFrame with query results
        """
        if params:
            query = query.format(**params)
        spark = self.get_spark_session()
        return (spark.read.format("jdbc")
                .option("url", self.jdbc_url)
                .option("query", query)
                .option("driver", DB_CONFIG['driver'])
                .option("user", self.credentials['username'])
                .option("password", self.credentials['password'])
                .load())
    def execute_update(self, query, params=None):
        """
        Execute an update SQL query using JDBC connection.
        Args:
            query (str): SQL update query to execute
            params (dict, optional): Parameters to format the query with
        """
        if params:
            query = query.format(**params)
        spark = self.get_spark_session()
        conn = spark.sparkContext._jvm.java.sql.DriverManager.getConnection(
            self.jdbc_url,
            self.credentials['username'],
            self.credentials['password']
        )
        try:
            stmt = conn.createStatement()
            stmt.execute(query)
        finally:
            if conn:
                conn.close()
</file>

<file path="src/database/operations.py">
"""
Database operations module for handling specific database queries and operations.
"""
from config.queries import (
    DAQ_LOG_INFO_QUERY,
    ENTITY_DETAIL_QUERY_WITH_SHEET,
    ENTITY_DETAIL_QUERY_WITH_FILE,
    ATTRIBUTE_DETAIL_QUERY,
    UPDATE_DAQ_LOG_INFO,
    ENTITY_DETAIL_QUERY_NO_SHEET,
    ENTITY_DETAIL_QUERY_COUNTRY
)
from config.settings import DB_CONFIG
class DatabaseOperations:
    def __init__(self, db_connection):
        """
        Initialize database operations with a database connection.
        Args:
            db_connection: DatabaseConnection instance
        """
        self.db_connection = db_connection
        self.schema = DB_CONFIG['schema']
    def get_unprocessed_files(self):
        """
        Get unprocessed files from DAQ_LOG_INFO table.
        Returns:
            DataFrame: Spark DataFrame with unprocessed files information
        """
        params = {'schema': self.schema}
        return self.db_connection.execute_query(DAQ_LOG_INFO_QUERY, params)
    def get_entity_details_by_sheet(self, daq_sheet_name, sender_address):
        """
        Get entity details filtered by sheet name.
        Args:
            daq_sheet_name (str): Sheet name from DAQ log
            sender_address (str): Sender's email address
        Returns:
            DataFrame: Spark DataFrame with entity details
        """
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'sender_address': sender_address
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_WITH_SHEET, params)
    def get_entity_details_by_file(self, daq_sheet_name, file_extension):
        """
        Get entity details filtered by file extension.
        Args:
            daq_sheet_name (str): Sheet name from DAQ log
            file_extension (str): File extension
        Returns:
            DataFrame: Spark DataFrame with entity details
        """
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'file_extension': file_extension
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_WITH_FILE, params)
    def get_attribute_details(self, data_owner, context, file_table_name, sheet_name):
        """
        Get attribute details for a specific entity.
        Args:
            data_owner (str): Data owner name
            context (str): Context
            file_table_name (str): File/table name
            sheet_name (str): Sheet name
        Returns:
            DataFrame: Spark DataFrame with attribute details
        """
        params = {
            'schema': self.schema,
            'data_owner': data_owner,
            'context': context,
            'file_table_name': file_table_name,
            'sheet_name': sheet_name
        }
        return self.db_connection.execute_query(ATTRIBUTE_DETAIL_QUERY, params)
    def mark_file_as_processed(self, file_id=None):
        """
        Mark file(s) as processed in DAQ_LOG_INFO table.
        Args:
            file_id (int, optional): Specific file ID to mark as processed.
                                   If None, marks all unprocessed files.
        """
        condition = f"id = {file_id}" if file_id else "is_corrupt = 0 and is_processed = 0"
        params = {
            'schema': self.schema,
            'condition': condition
        }
        self.db_connection.execute_update(UPDATE_DAQ_LOG_INFO, params)
    def insert_temp_load_info(self, table_name, context, frequency, is_invoice, is_process):
        """
        Insert record into temp_load_info table.
        Args:
            table_name (str): Table name
            context (str): Context
            frequency (str): Frequency
            is_invoice (int): Invoice flag
            is_process (int): Process flag
        """
        query = f"""
        INSERT INTO {self.schema}.temp_load_info 
        (table_name, context, frequency, is_invoice, is_process, load_datetime)
        VALUES 
        ('{table_name.upper()}', '{context}', '{frequency}', {is_invoice}, {is_process}, CURRENT_TIMESTAMP)
        """
        self.db_connection.execute_update(query)
    def get_entity_details(self, daq_sheet_name, sender_address, file_extension, receiver_address, file_id):
        """
        Get entity details using sequential matching strategy.
        Tries different matching criteria in sequence until a match is found.
        Args:
            daq_sheet_name (str): Sheet name from DAQ log
            sender_address (str): Sender's email address
            file_extension (str): File extension
            receiver_address (str): Receiver's email address
            file_id (int): File ID from DAQ_LOG_INFO
        Returns:
            DataFrame: Spark DataFrame with entity details from the first successful match
        """
        # Step 1: Try sheet name match first (strictest match)
        result = self.get_entity_details_by_sheet(daq_sheet_name, sender_address)
        if result.count() > 0:
            return result
        # Step 2: Try file extension match
        result = self.get_entity_details_by_file(daq_sheet_name, file_extension)
        if result.count() > 0:
            return result
        # Step 3: Try without sheet name restriction
        result = self.get_entity_details_no_sheet(daq_sheet_name, sender_address)
        if result.count() > 0:
            return result
        # Step 4: Finally try country and filename matching (most relaxed criteria)
        return self.get_entity_details_by_country(
            daq_sheet_name, file_extension, sender_address, receiver_address, file_id
        )
    def get_entity_details_no_sheet(self, daq_sheet_name, sender_address):
        """Get entity details without sheet name filter"""
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'sender_address': sender_address
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_NO_SHEET, params)
    def get_entity_details_by_country(self, daq_sheet_name, file_extension, sender_address, receiver_address, file_id):
        """Get entity details with country and filename matching"""
        params = {
            'schema': self.schema,
            'daq_sheet_name': daq_sheet_name,
            'file_extension': file_extension,
            'sender_address': sender_address,
            'receiver_address': receiver_address,
            'file_id': file_id
        }
        return self.db_connection.execute_query(ENTITY_DETAIL_QUERY_COUNTRY, params)
</file>

<file path="src/etl/__init__.py">
"""
ETL module for data extraction, transformation, and loading.
"""
from .extractors import DataExtractor
from .transformers import DataTransformer
from .loaders import DataLoader
__all__ = [
    'DataExtractor',
    'DataTransformer',
    'DataLoader'
]
</file>

<file path="src/etl/extractors.py">
"""
Data extraction module for the ETL job.
"""
import pandas as pd
import s3fs
from config.settings import S3_BUCKET_PREFIX, SPECIAL_CASES
class DataExtractor:
    def __init__(self, s3_connection, logger):
        """
        Initialize the data extractor.
        Args:
            s3_connection: S3 connection object
            logger: Logger instance
        """
        self.s3 = s3_connection
        self.logger = logger
        self.fs = s3fs.S3FileSystem(anon=False)
    def read_excel_file(self, file_path, sheet_name=None):
        """
        Read data from an Excel file.
        Args:
            file_path (str): Path to the Excel file
            sheet_name (str, optional): Name of the sheet to read
        Returns:
            DataFrame: pandas DataFrame with the file contents
        """
        try:
            with self.fs.open(file_path, 'rb') as f:
                if sheet_name:
                    df = pd.read_excel(f, sheet_name=sheet_name)
                else:
                    df = pd.read_excel(f)
            self.logger.log_info(f"Successfully read Excel file: {file_path}")
            self.logger.log_dataframe_info(df, "Excel Data")
            return df
        except Exception as e:
            self.logger.log_error(f"Error reading Excel file {file_path}: {str(e)}", exc_info=e)
            return None
    def read_csv_file(self, file_path, **kwargs):
        """
        Read data from a CSV file.
        Args:
            file_path (str): Path to the CSV file
            **kwargs: Additional arguments for pd.read_csv
        Returns:
            DataFrame: pandas DataFrame with the file contents
        """
        try:
            with self.fs.open(file_path, 'rb') as f:
                df = pd.read_csv(f, **kwargs)
            self.logger.log_info(f"Successfully read CSV file: {file_path}")
            self.logger.log_dataframe_info(df, "CSV Data")
            return df
        except Exception as e:
            self.logger.log_error(f"Error reading CSV file {file_path}: {str(e)}", exc_info=e)
            return None
    def validate_file_source(self, data_owner, country, sheet_name):
        """
        Validate file source based on special cases.
        Args:
            data_owner (str): Data owner name
            country (str): Country name
            sheet_name (str): Sheet name
        Returns:
            bool: True if source is valid, False otherwise
        """
        if data_owner.upper() in SPECIAL_CASES:
            case = SPECIAL_CASES[data_owner.upper()]
            # Check country match
            if case['country'] != country.upper():
                self.logger.log_warning(
                    f"Invalid country for {data_owner}: {country}"
                )
                return False
            # Check sheet name constraints
            if 'sheet_name' in case and case['sheet_name'].lower() != sheet_name.lower():
                self.logger.log_warning(
                    f"Invalid sheet name for {data_owner}: {sheet_name}"
                )
                return False
            if 'valid_sheets' in case and sheet_name.lower() not in case['valid_sheets']:
                self.logger.log_warning(
                    f"Sheet name {sheet_name} not in valid sheets for {data_owner}"
                )
                return False
        return True
    def extract_data(self, file_info):
        """
        Extract data based on file information.
        Args:
            file_info (dict): Dictionary containing file information
                Required keys:
                - file_path: Path to the file
                - file_type: Type of file (excel/csv)
                - sheet_name: Sheet name (for Excel files)
                - data_owner: Data owner name
                - country: Country name
        Returns:
            DataFrame: pandas DataFrame with the extracted data
        """
        # Validate required keys
        required_keys = ['file_path', 'file_type', 'data_owner', 'country']
        missing_keys = [key for key in required_keys if key not in file_info]
        if missing_keys:
            self.logger.log_error(f"Missing required keys in file_info: {missing_keys}")
            return None
        # Validate file source
        sheet_name = file_info.get('sheet_name')
        if not self.validate_file_source(
            file_info['data_owner'],
            file_info['country'],
            sheet_name or ''
        ):
            return None
        # Extract data based on file type
        file_path = f"{self.s3_bucket}/{file_info['file_path']}"
        if file_info['file_type'].lower() == 'excel':
            return self.read_excel_file(file_path, sheet_name)
        elif file_info['file_type'].lower() == 'csv':
            return self.read_csv_file(file_path)
        else:
            self.logger.log_error(f"Unsupported file type: {file_info['file_type']}")
            return None
</file>

<file path="src/etl/loaders.py">
"""
Data loading module for the ETL job.
"""
from datetime import datetime
from src.models.stock import StockData
from src.models.sales import SalesData
class DataLoader:
    def __init__(self, db_connection, logger):
        """
        Initialize the data loader.
        Args:
            db_connection: Database connection object
            logger: Logger instance
        """
        self.db = db_connection
        self.logger = logger
    def load_stock_data(self, stock_data, table_name):
        """
        Load stock data into the database.
        Args:
            stock_data (StockData): Stock data to load
            table_name (str): Target table name
        Returns:
            bool: True if loading successful, False otherwise
        """
        if not isinstance(stock_data, StockData):
            self.logger.log_error("Invalid stock data type")
            return False
        try:
            # Convert to Spark DataFrame
            spark = self.db.get_spark_session()
            spark_df = stock_data.to_spark_df(spark)
            # Write to database
            (spark_df.write
                .format("jdbc")
                .option("url", self.db.jdbc_url)
                .option("dbtable", table_name)
                .option("user", self.db.jdbc_properties['user'])
                .option("password", self.db.jdbc_properties['password'])
                .option("driver", self.db.jdbc_properties['driver'])
                .mode("append")
                .save())
            self.logger.log_info(f"Successfully loaded stock data to table: {table_name}")
            return True
        except Exception as e:
            self.logger.log_error(f"Error loading stock data: {str(e)}", exc_info=e)
            return False
    def load_sales_data(self, sales_data, table_name):
        """
        Load sales data into the database.
        Args:
            sales_data (SalesData): Sales data to load
            table_name (str): Target table name
        Returns:
            bool: True if loading successful, False otherwise
        """
        if not isinstance(sales_data, SalesData):
            self.logger.log_error("Invalid sales data type")
            return False
        try:
            # Convert to Spark DataFrame
            spark = self.db.get_spark_session()
            spark_df = sales_data.to_spark_df(spark)
            # Write to database
            (spark_df.write
                .format("jdbc")
                .option("url", self.db.jdbc_url)
                .option("dbtable", table_name)
                .option("user", self.db.jdbc_properties['user'])
                .option("password", self.db.jdbc_properties['password'])
                .option("driver", self.db.jdbc_properties['driver'])
                .mode("append")
                .save())
            self.logger.log_info(f"Successfully loaded sales data to table: {table_name}")
            return True
        except Exception as e:
            self.logger.log_error(f"Error loading sales data: {str(e)}", exc_info=e)
            return False
    def create_temp_table(self, table_name, schema):
        """
        Create a temporary table for data loading.
        Args:
            table_name (str): Table name
            schema (dict): Column schema dictionary
        Returns:
            bool: True if creation successful, False otherwise
        """
        try:
            # Generate CREATE TABLE statement
            columns = []
            for col_name, col_type in schema.items():
                sql_type = self._map_pandas_to_sql_type(col_type)
                columns.append(f"{col_name} {sql_type}")
            create_stmt = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                {', '.join(columns)},
                LOAD_DATETIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
            # Execute creation
            self.db.execute_update(create_stmt)
            self.logger.log_info(f"Successfully created temporary table: {table_name}")
            return True
        except Exception as e:
            self.logger.log_error(f"Error creating temporary table: {str(e)}", exc_info=e)
            return False
    def _map_pandas_to_sql_type(self, pandas_type):
        """
        Map pandas dtype to SQL type.
        Args:
            pandas_type (str): pandas dtype
        Returns:
            str: Corresponding SQL type
        """
        type_mapping = {
            'datetime64[ns]': 'TIMESTAMP',
            'object': 'TEXT',
            'Int64': 'BIGINT',
            'float64': 'DECIMAL(22,2)',
            'bool': 'BOOLEAN'
        }
        return type_mapping.get(pandas_type, 'TEXT')
    def cleanup_temp_tables(self, retention_days=7):
        """
        Clean up old temporary tables.
        Args:
            retention_days (int): Number of days to retain tables
        Returns:
            bool: True if cleanup successful, False otherwise
        """
        try:
            cleanup_query = f"""
            DELETE FROM temp_load_info
            WHERE load_datetime < CURRENT_TIMESTAMP - INTERVAL '{retention_days} days'
            """
            self.db.execute_update(cleanup_query)
            self.logger.log_info(f"Successfully cleaned up temporary tables older than {retention_days} days")
            return True
        except Exception as e:
            self.logger.log_error(f"Error cleaning up temporary tables: {str(e)}", exc_info=e)
            return False
    def update_processing_status(self, file_id):
        """Add comprehensive status update"""
        try:
            self.db.execute_update(
                "UPDATE daq_log_info SET is_processed = 1 WHERE id = %s",
                (file_id,)
            )
            self.db.commit()
        except Exception as e:
            self.logger.log_error(f"Status update failed: {str(e)}")
            self.db.rollback()
</file>

<file path="src/etl/transformers.py">
"""
Data transformation module for the ETL job.
"""
import pandas as pd
import numpy as np
from src.utils.date_utils import parse_date, get_last_day_of_month
from src.models.stock import StockData
from src.models.sales import SalesData
from typing import List
class DataTransformer:
    def __init__(self, logger):
        """
        Initialize the data transformer.
        Args:
            logger: Logger instance
        """
        self.logger = logger
    def clean_column_names(self, df):
        """
        Clean and standardize column names.
        Args:
            df (DataFrame): Input DataFrame
        Returns:
            DataFrame: DataFrame with cleaned column names
        """
        try:
            # Remove special characters and standardize spacing
            df.columns = df.columns.str.strip()
            df.columns = df.columns.str.replace(r'[^\w\s]', '', regex=True)
            df.columns = df.columns.str.replace(r'\s+', '_', regex=True)
            df.columns = df.columns.str.upper()
            self.logger.log_info("Column names cleaned successfully")
            return df
        except Exception as e:
            self.logger.log_error(f"Error cleaning column names: {str(e)}", exc_info=e)
            return df
    def map_columns(self, df, column_mapping):
        """
        Map DataFrame columns according to provided mapping.
        Args:
            df (DataFrame): Input DataFrame
            column_mapping (dict): Dictionary mapping original to new column names
        Returns:
            DataFrame: DataFrame with mapped columns
        """
        try:
            # Create a copy to avoid modifying the original
            df_mapped = df.copy()
            # Rename columns according to mapping
            df_mapped.rename(columns=column_mapping, inplace=True)
            # Log mapped columns
            self.logger.log_info("Columns mapped successfully")
            self.logger.log_debug(f"Column mapping: {column_mapping}")
            return df_mapped
        except Exception as e:
            self.logger.log_error(f"Error mapping columns: {str(e)}", exc_info=e)
            return df
    def transform_stock_data(self, df, attribute_details):
        """
        Transform raw data into stock data format.
        Args:
            df (DataFrame): Raw input DataFrame
            attribute_details (DataFrame): DataFrame containing attribute mapping details
        Returns:
            StockData: Transformed stock data
        """
        try:
            # Initialize stock data model
            stock_data = StockData()
            # Clean and map columns
            df = self.clean_column_names(df)
            # Create column mapping from attribute details
            column_mapping = {
                attr['original_column_name']: attr['etl_column_name']
                for attr in attribute_details
                if attr['original_column_name'] in df.columns
            }
            df = self.map_columns(df, column_mapping)
            # Convert data types
            for col in df.columns:
                if col in ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif col in ['DATA_DATE', 'EXPIRY_DATE']:
                    df[col] = df[col].apply(parse_date)
            # Add records to stock data model
            stock_data.add_records(df.to_dict('records'))
            # Validate and clean data
            if stock_data.validate_data():
                stock_data.clean_data()
                self.logger.log_info("Stock data transformed successfully")
                return stock_data
            else:
                self.logger.log_error("Stock data validation failed")
                return None
        except Exception as e:
            self.logger.log_error(f"Error transforming stock data: {str(e)}", exc_info=e)
            return None
    def transform_sales_data(self, df, attribute_details):
        """
        Transform raw data into sales data format.
        Args:
            df (DataFrame): Raw input DataFrame
            attribute_details (dict): Dictionary containing attribute mapping details
        Returns:
            SalesData: Transformed sales data
        """
        try:
            # Initialize sales data model
            sales_data = SalesData()
            # Clean and map columns
            df = self.clean_column_names(df)
            column_mapping = {
                attr['original_column_name']: attr['etl_column_name']
                for attr in attribute_details
                if attr['original_column_name'] in df.columns
            }
            df = self.map_columns(df, column_mapping)
            # Convert data types
            for col in df.columns:
                if col in ['SALES_QUANTITY', 'RETURN_QUANTITY']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif col in ['SALES_VALUE', 'RETURN_VALUE']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                elif col in ['DATA_DATE', 'INVOICE_DATE']:
                    df[col] = df[col].apply(parse_date)
            # Add records to sales data model
            sales_data.add_records(df.to_dict('records'))
            # Validate and clean data
            if sales_data.validate_data():
                sales_data.clean_data()
                self.logger.log_info("Sales data transformed successfully")
                return sales_data
            else:
                self.logger.log_error("Sales data validation failed")
                return None
        except Exception as e:
            self.logger.log_error(f"Error transforming sales data: {str(e)}", exc_info=e)
            return None
    def remove_unwanted_characters(self, df, columns):
        """
        Remove unwanted characters from specified columns.
        Args:
            df (DataFrame): Input DataFrame
            columns (list): List of columns to clean
        Returns:
            DataFrame: Cleaned DataFrame
        """
        try:
            df_clean = df.copy()
            for col in columns:
                if col in df_clean.columns:
                    # Remove special characters and extra spaces
                    df_clean[col] = df_clean[col].astype(str)
                    df_clean[col] = df_clean[col].str.replace(r'[^\w\s-]', '', regex=True)
                    df_clean[col] = df_clean[col].str.strip()
            self.logger.log_info(f"Cleaned unwanted characters from columns: {columns}")
            return df_clean
        except Exception as e:
            self.logger.log_error(f"Error removing unwanted characters: {str(e)}", exc_info=e)
            return df
    def handle_special_values(self, df):
        """Add SURGIPHARM special case handling"""
        if self.data_owner == 'SURGIPHARM' and self.country == 'KENYA':
            df.loc[df['BRANCH_NAME'].str.startswith('MAINSTORES'), 'BRANCH_NAME'] = None
            df.loc[df['SALES_CATEGORY'] == 'SALESSTOCK', 'SALES_CATEGORY'] = None
    def process_dates(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:
        """
        Process date columns in DataFrame.
        Args:
            df (pd.DataFrame): Input DataFrame
            date_columns (List[str]): List of date column names
        Returns:
            pd.DataFrame: DataFrame with processed dates
        """
        df = df.copy()
        for col in date_columns:
            if col in df.columns:
                df[col] = df[col].apply(parse_date)
                # Handle special cases
                if col == 'DATA_DATE' and self.frequency == 'MONTHLY':
                    df[col] = df[col].apply(
                        lambda x: get_last_day_of_month(x) if x else None
                    )
        return df
</file>

<file path="src/models/__init__.py">
"""
Data models for stock and sales data.
"""
from .stock import StockData
from .sales import SalesData
__all__ = [
    'StockData',
    'SalesData'
]
</file>

<file path="src/models/sales.py">
"""
Sales data model for handling sales transaction data.
"""
import pandas as pd
from config.settings import SALES_SCHEMA
class SalesData:
    def __init__(self):
        """Initialize an empty sales DataFrame with the predefined schema."""
        self.data = pd.DataFrame({col: pd.Series(dtype=dtype) 
                                for col, dtype in SALES_SCHEMA.items()})
    def add_record(self, record_dict):
        """
        Add a record to the sales DataFrame.
        Args:
            record_dict (dict): Dictionary containing sales record data
        """
        # Convert to DataFrame with single row
        record_df = pd.DataFrame([record_dict])
        # Ensure data types match schema
        for col, dtype in SALES_SCHEMA.items():
            if col in record_df.columns:
                record_df[col] = record_df[col].astype(dtype)
        # Append to existing data
        self.data = pd.concat([self.data, record_df], ignore_index=True)
    def add_records(self, records_list):
        """
        Add multiple records to the sales DataFrame.
        Args:
            records_list (list): List of dictionaries containing sales records
        """
        for record in records_list:
            self.add_record(record)
    def validate_data(self):
        """
        Validate the data in the DataFrame.
        Returns:
            bool: True if data is valid, False otherwise
        """
        # Check for required columns
        required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'SALES_QUANTITY',
            'SALES_VALUE'
        ]
        missing_columns = [col for col in required_columns 
                         if col not in self.data.columns]
        if missing_columns:
            print(f"Missing required columns: {missing_columns}")
            return False
        # Check for null values in required fields
        null_counts = self.data[required_columns].isnull().sum()
        if null_counts.any():
            print("Null values found in required columns:")
            print(null_counts[null_counts > 0])
            return False
        # Validate numeric fields
        numeric_columns = ['SALES_QUANTITY', 'RETURN_QUANTITY', 
                         'SALES_VALUE', 'RETURN_VALUE']
        for col in numeric_columns:
            if col in self.data.columns:
                if not pd.to_numeric(self.data[col], errors='coerce').notnull().all():
                    print(f"Invalid numeric values found in {col}")
                    return False
        return True
    def clean_data(self):
        """Clean and standardize the data in the DataFrame."""
        # Remove any leading/trailing whitespace
        string_columns = [col for col, dtype in SALES_SCHEMA.items() 
                         if dtype == 'object']
        for col in string_columns:
            if col in self.data.columns:
                self.data[col] = self.data[col].str.strip()
        # Convert quantities to integers
        quantity_columns = ['SALES_QUANTITY', 'RETURN_QUANTITY']
        for col in quantity_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
                self.data[col] = self.data[col].fillna(0).astype('Int64')
        # Convert monetary values to float
        value_columns = ['SALES_VALUE', 'RETURN_VALUE']
        for col in value_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
                self.data[col] = self.data[col].fillna(0.0).astype('float64')
        # Standardize date formats
        date_columns = ['DATA_DATE', 'INVOICE_DATE']
        for col in date_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_datetime(self.data[col], errors='coerce')
    def calculate_net_sales(self):
        """
        Calculate net sales quantities and values.
        Returns:
            tuple: (net_quantity, net_value)
        """
        net_quantity = (self.data['SALES_QUANTITY'].sum() - 
                       self.data['RETURN_QUANTITY'].sum())
        net_value = (self.data['SALES_VALUE'].sum() - 
                    self.data['RETURN_VALUE'].sum())
        return net_quantity, net_value
    def to_spark_df(self, spark):
        """
        Convert pandas DataFrame to Spark DataFrame.
        Args:
            spark: SparkSession instance
        Returns:
            DataFrame: Spark DataFrame
        """
        return spark.createDataFrame(self.data)
</file>

<file path="src/models/stock.py">
"""
Stock data model for handling inventory data.
"""
import pandas as pd
from config.settings import STOCK_SCHEMA
class StockData:
    def __init__(self):
        """Initialize an empty stock DataFrame with the predefined schema."""
        self.data = pd.DataFrame({col: pd.Series(dtype=dtype) 
                                for col, dtype in STOCK_SCHEMA.items()})
    def add_record(self, record_dict):
        """
        Add a record to the stock DataFrame.
        Args:
            record_dict (dict): Dictionary containing stock record data
        """
        # Convert to DataFrame with single row
        record_df = pd.DataFrame([record_dict])
        # Ensure data types match schema
        for col, dtype in STOCK_SCHEMA.items():
            if col in record_df.columns:
                record_df[col] = record_df[col].astype(dtype)
        # Append to existing data
        self.data = pd.concat([self.data, record_df], ignore_index=True)
    def add_records(self, records_list):
        """
        Add multiple records to the stock DataFrame.
        Args:
            records_list (list): List of dictionaries containing stock records
        """
        for record in records_list:
            self.add_record(record)
    def validate_data(self):
        """
        Validate the data in the DataFrame.
        Returns:
            bool: True if data is valid, False otherwise
        """
        # Check for required columns
        required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'AVAILABLE_QUANTITY'
        ]
        missing_columns = [col for col in required_columns 
                         if col not in self.data.columns]
        if missing_columns:
            print(f"Missing required columns: {missing_columns}")
            return False
        # Check for null values in required fields
        null_counts = self.data[required_columns].isnull().sum()
        if null_counts.any():
            print("Null values found in required columns:")
            print(null_counts[null_counts > 0])
            return False
        # Validate numeric fields
        numeric_columns = ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']
        for col in numeric_columns:
            if col in self.data.columns:
                if not pd.to_numeric(self.data[col], errors='coerce').notnull().all():
                    print(f"Invalid numeric values found in {col}")
                    return False
        return True
    def clean_data(self):
        """Clean and standardize the data in the DataFrame."""
        # Remove any leading/trailing whitespace
        string_columns = [col for col, dtype in STOCK_SCHEMA.items() 
                         if dtype == 'object']
        for col in string_columns:
            if col in self.data.columns:
                self.data[col] = self.data[col].str.strip()
        # Convert quantities to integers
        quantity_columns = ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']
        for col in quantity_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
                self.data[col] = self.data[col].fillna(0).astype('Int64')
        # Standardize date formats
        date_columns = ['DATA_DATE', 'EXPIRY_DATE']
        for col in date_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_datetime(self.data[col], errors='coerce')
    def to_spark_df(self, spark):
        """
        Convert pandas DataFrame to Spark DataFrame.
        Args:
            spark: SparkSession instance
        Returns:
            DataFrame: Spark DataFrame
        """
        return spark.createDataFrame(self.data)
    def validate_quantities(self):
        """Add quantity validation"""
        self.data['AVAILABLE_QUANTITY'] = pd.to_numeric(
            self.data['AVAILABLE_QUANTITY'], 
            errors='coerce'
        ).fillna(0)
        self.data = self.data[self.data['AVAILABLE_QUANTITY'] >= 0]
</file>

<file path="src/utils/__init__.py">
"""
Utility modules for common functionality.
"""
from .date_utils import parse_date, format_date, get_last_day_of_month, validate_date_range, get_date_parts
from .logging_utils import ETLLogger
from .data_processing import (
    clean_column_names,
    remove_empty_rows,
    remove_empty_columns,
    standardize_dates,
    standardize_numeric_columns,
    handle_duplicate_values,
    process_special_characters,
    aggregate_data,
    validate_mandatory_columns,
    fill_missing_values
)
from .s3_utils import S3Manager
__all__ = [
    'parse_date',
    'format_date',
    'get_last_day_of_month',
    'validate_date_range',
    'get_date_parts',
    'ETLLogger',
    'clean_column_names',
    'remove_empty_rows',
    'remove_empty_columns',
    'standardize_dates',
    'standardize_numeric_columns',
    'handle_duplicate_values',
    'process_special_characters',
    'aggregate_data',
    'validate_mandatory_columns',
    'fill_missing_values',
    'S3Manager'
]
</file>

<file path="src/utils/data_processing.py">
"""
Utility module for common data processing functions.
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Clean column names by removing whitespace and special characters.
    Args:
        df (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: DataFrame with cleaned column names
    """
    df.columns = df.columns.str.strip()
    df.columns = df.columns.str.upper()
    df.columns = df.columns.str.replace(' ', '_')
    return df
def remove_empty_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove rows where all values are null.
    Args:
        df (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: DataFrame with empty rows removed
    """
    return df.dropna(how='all')
def remove_empty_columns(df: pd.DataFrame, threshold: float = 0.9) -> pd.DataFrame:
    """
    Remove columns with high percentage of null values.
    Args:
        df (pd.DataFrame): Input DataFrame
        threshold (float): Threshold for null value percentage (default: 0.9)
    Returns:
        pd.DataFrame: DataFrame with empty columns removed
    """
    null_percent = df.isnull().sum() / len(df)
    cols_to_drop = null_percent[null_percent > threshold].index
    return df.drop(columns=cols_to_drop)
def standardize_dates(
    df: pd.DataFrame, 
    date_columns: List[str]
) -> pd.DataFrame:
    """
    Standardize date columns to datetime format.
    Args:
        df (pd.DataFrame): Input DataFrame
        date_columns (List[str]): List of date column names
    Returns:
        pd.DataFrame: DataFrame with standardized date columns
    """
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    return df
def standardize_numeric_columns(
    df: pd.DataFrame, 
    numeric_columns: List[str]
) -> pd.DataFrame:
    """
    Standardize numeric columns by converting to float and handling special cases.
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_columns (List[str]): List of numeric column names
    Returns:
        pd.DataFrame: DataFrame with standardized numeric columns
    """
    for col in numeric_columns:
        if col in df.columns:
            # Replace any non-numeric values with NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            # Replace negative values with 0 for quantity and value columns
            if any(x in col.upper() for x in ['QUANTITY', 'VALUE']):
                df[col] = df[col].clip(lower=0)
    return df
def handle_duplicate_values(
    df: pd.DataFrame,
    column: str,
    keep: str = 'first'
) -> pd.DataFrame:
    """
    Handle duplicate values in a specified column.
    Args:
        df (pd.DataFrame): Input DataFrame
        column (str): Column name to check for duplicates
        keep (str): Which duplicate to keep ('first', 'last', False)
    Returns:
        pd.DataFrame: DataFrame with handled duplicates
    """
    if df[column].duplicated().any():
        df = df.sort_values(by=[column])
        df[column] = df[column] + df.groupby(column).cumcount().astype(str)
        df[column] = df[column].str.replace('0', '')
    return df
def process_special_characters(
    df: pd.DataFrame,
    columns: List[str]
) -> pd.DataFrame:
    """
    Process special characters in specified columns.
    Args:
        df (pd.DataFrame): Input DataFrame
        columns (List[str]): List of column names to process
    Returns:
        pd.DataFrame: DataFrame with processed special characters
    """
    for col in columns:
        if col in df.columns:
            # Replace tilde with separate rows
            if df[col].str.contains('~', na=False).any():
                df = df.assign(**{col: df[col].str.split('~')}).explode(col)
            # Clean up other special characters
            df[col] = df[col].str.strip()
            df[col] = df[col].str.replace('[^\w\s-]', '', regex=True)
    return df
def aggregate_data(
    df: pd.DataFrame,
    group_columns: List[str],
    agg_columns: Dict[str, str]
) -> pd.DataFrame:
    """
    Aggregate data based on specified columns and aggregation functions.
    Args:
        df (pd.DataFrame): Input DataFrame
        group_columns (List[str]): Columns to group by
        agg_columns (Dict[str, str]): Dictionary of column names and aggregation functions
    Returns:
        pd.DataFrame: Aggregated DataFrame
    """
    return df.groupby(group_columns, as_index=False).agg(agg_columns)
def validate_mandatory_columns(
    df: pd.DataFrame,
    mandatory_columns: List[str]
) -> List[str]:
    """
    Validate presence of mandatory columns.
    Args:
        df (pd.DataFrame): Input DataFrame
        mandatory_columns (List[str]): List of mandatory column names
    Returns:
        List[str]: List of missing mandatory columns
    """
    return [col for col in mandatory_columns if col not in df.columns]
def fill_missing_values(
    df: pd.DataFrame,
    fill_values: Dict[str, Any]
) -> pd.DataFrame:
    """
    Fill missing values in specified columns.
    Args:
        df (pd.DataFrame): Input DataFrame
        fill_values (Dict[str, Any]): Dictionary of column names and fill values
    Returns:
        pd.DataFrame: DataFrame with filled missing values
    """
    for col, value in fill_values.items():
        if col in df.columns:
            df[col] = df[col].fillna(value)
    return df
</file>

<file path="src/utils/date_utils.py">
"""
Date handling utilities for the ETL job.
"""
from datetime import datetime, timedelta
import pandas as pd
from config.settings import DATE_FORMATS
from typing import Optional
def parse_date(date_str: str) -> Optional[datetime]:
    """
    Parse date string using multiple formats.
    Args:
        date_str (str): Date string to parse
    Returns:
        datetime: Parsed datetime object or None if parsing fails
    """
    if not date_str or pd.isna(date_str):
        return None
    date_str = str(date_str).strip()
    # Try pandas to_datetime first
    try:
        return pd.to_datetime(date_str)
    except:
        pass
    # Try all defined formats
    for fmt in DATE_FORMATS:
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    return None
def format_date(date: datetime, fmt: str = "%Y-%m-%d") -> str:
    """
    Format datetime object to string.
    Args:
        date (datetime): Date to format
        fmt (str): Output format
    Returns:
        str: Formatted date string
    """
    if not date:
        return None
    return date.strftime(fmt)
def get_last_day_of_month(date: datetime) -> datetime:
    """
    Get the last day of the month for a given date.
    Args:
        date (datetime): Input date
    Returns:
        datetime: Last day of the month
    """
    if date.month == 12:
        return date.replace(day=31)
    next_month = date.replace(day=1, month=date.month + 1)
    return next_month - timedelta(days=1)
def validate_date_range(date: datetime, min_date: Optional[datetime] = None, 
                       max_date: Optional[datetime] = None) -> bool:
    """
    Validate if date is within specified range.
    Args:
        date (datetime): Date to validate
        min_date (datetime, optional): Minimum allowed date
        max_date (datetime, optional): Maximum allowed date
    Returns:
        bool: True if date is within range
    """
    if not date:
        return False
    if min_date and date < min_date:
        return False
    if max_date and date > max_date:
        return False
    return True
def get_date_parts(date_obj):
    """
    Extract year, month, and day from a date object.
    Args:
        date_obj (datetime): Date to extract parts from
    Returns:
        tuple: (year, month, day) or None if extraction fails
    """
    if not date_obj:
        return None
    try:
        return date_obj.year, date_obj.month, date_obj.day
    except AttributeError:
        return None
</file>

<file path="src/utils/logging_utils.py">
"""
Logging utilities for the ETL job.
"""
import logging
import sys
from datetime import datetime
class ETLLogger:
    def __init__(self, job_name, log_level=logging.INFO):
        """
        Initialize the ETL logger.
        Args:
            job_name (str): Name of the ETL job
            log_level (int): Logging level (default: logging.INFO)
        """
        self.logger = logging.getLogger(job_name)
        self.logger.setLevel(log_level)
        # Create formatters and handlers
        self._setup_handlers()
        self.job_name = job_name
        self.start_time = None
        self.end_time = None
    def _setup_handlers(self):
        """Set up console and file handlers with formatters."""
        # Create formatters
        console_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(console_formatter)
        self.logger.addHandler(console_handler)
        # File handler
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_handler = logging.FileHandler(
            f'logs/etl_job_{timestamp}.log'
        )
        file_handler.setFormatter(file_formatter)
        self.logger.addHandler(file_handler)
    def start_job(self):
        """Log the start of the ETL job."""
        self.start_time = datetime.now()
        self.logger.info(f"Starting ETL job: {self.job_name}")
        self.logger.info(f"Start time: {self.start_time}")
    def end_job(self, status="completed"):
        """
        Log the end of the ETL job.
        Args:
            status (str): Job status (default: "completed")
        """
        self.end_time = datetime.now()
        duration = self.end_time - self.start_time if self.start_time else None
        self.logger.info(f"ETL job {status}: {self.job_name}")
        self.logger.info(f"End time: {self.end_time}")
        if duration:
            self.logger.info(f"Duration: {duration}")
    def log_step(self, step_name, status="started"):
        """
        Log an ETL step.
        Args:
            step_name (str): Name of the step
            status (str): Step status (default: "started")
        """
        self.logger.info(f"Step {status}: {step_name}")
    def log_error(self, error_msg, exc_info=None):
        """
        Log an error message.
        Args:
            error_msg (str): Error message
            exc_info (Exception, optional): Exception information
        """
        if exc_info:
            self.logger.error(error_msg, exc_info=exc_info)
        else:
            self.logger.error(error_msg)
    def log_warning(self, warning_msg):
        """
        Log a warning message.
        Args:
            warning_msg (str): Warning message
        """
        self.logger.warning(warning_msg)
    def log_info(self, info_msg):
        """
        Log an info message.
        Args:
            info_msg (str): Info message
        """
        self.logger.info(info_msg)
    def log_debug(self, debug_msg):
        """
        Log a debug message.
        Args:
            debug_msg (str): Debug message
        """
        self.logger.debug(debug_msg)
    def log_dataframe_info(self, df, df_name):
        """
        Log information about a DataFrame.
        Args:
            df: pandas or Spark DataFrame
            df_name (str): Name of the DataFrame
        """
        try:
            # Handle both pandas and Spark DataFrames
            if hasattr(df, 'shape'):  # pandas DataFrame
                rows, cols = df.shape
                self.logger.info(f"DataFrame {df_name}: {rows} rows, {cols} columns")
                self.logger.debug(f"Columns: {list(df.columns)}")
            else:  # Spark DataFrame
                rows = df.count()
                cols = len(df.columns)
                self.logger.info(f"DataFrame {df_name}: {rows} rows, {cols} columns")
                self.logger.debug(f"Columns: {df.columns}")
        except Exception as e:
            self.log_error(f"Error logging DataFrame info: {str(e)}")
    def log_column_mismatch(self, expected, actual):
        """Add detailed column mismatch logging"""
        self.log_error(
            f"Column mismatch found:\nExpected: {expected}\nActual: {actual}"
        )
</file>

<file path="src/utils/s3_utils.py">
"""
S3 utility functions for file operations.
"""
import pandas as pd
import s3fs
from typing import Union, Optional
class S3Manager:
    """Utility class for S3 operations."""
    def __init__(self, region_name: Optional[str] = None):
        """
        Initialize S3 manager.
        Args:
            region_name (str, optional): AWS region name
        """
        self.s3 = s3fs.S3FileSystem(anon=False)
        if region_name:
            self.s3.region_name = region_name
    def get_file(
        self, 
        s3_path: str,
        file_type: str = 'excel'
    ) -> pd.DataFrame:
        """
        Get file from S3.
        Args:
            s3_path (str): S3 path to the file
            file_type (str): Type of file ('excel' or 'parquet')
        Returns:
            pd.DataFrame: DataFrame containing file contents
        """
        with self.s3.open(s3_path, 'rb') as f:
            if file_type.lower() == 'excel':
                return pd.read_excel(f, engine='openpyxl')
            elif file_type.lower() == 'parquet':
                return pd.read_parquet(f)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
    def save_file(
        self, 
        data: pd.DataFrame,
        s3_path: str,
        file_type: str = 'parquet'
    ) -> None:
        """
        Save processed data back to S3.
        Args:
            data (pd.DataFrame): Data to save
            s3_path (str): S3 path to save to
            file_type (str): Type of file to save ('parquet' or 'excel')
        """
        with self.s3.open(s3_path, 'wb') as f:
            if file_type.lower() == 'parquet':
                data.to_parquet(f)
            elif file_type.lower() == 'excel':
                data.to_excel(f, engine='openpyxl', index=False)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
    def list_files(
        self, 
        s3_path: str,
        pattern: Optional[str] = None
    ) -> list:
        """
        List files in S3 path.
        Args:
            s3_path (str): S3 path to list
            pattern (str, optional): File pattern to match
        Returns:
            list: List of file paths
        """
        if pattern:
            return self.s3.glob(f"{s3_path}/{pattern}")
        return self.s3.ls(s3_path)
</file>

<file path="src/validation/__init__.py">
"""
Data validation module for schema and business rules.
"""
from .schema_validator import SchemaValidator
from .data_validator import StockDataValidator, SalesDataValidator, DataConsistencyValidator
__all__ = [
    'SchemaValidator',
    'StockDataValidator',
    'SalesDataValidator',
    'DataConsistencyValidator'
]
</file>

<file path="src/validation/data_validator.py">
"""
Business data validation module.
"""
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
class StockDataValidator:
    """Validator for stock data."""
    def __init__(self):
        """Initialize stock data validator."""
        self.errors = []
        self.warnings = []
        # Define required columns
        self.required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'AVAILABLE_QUANTITY'
        ]
        # Define numeric columns
        self.numeric_columns = [
            'AVAILABLE_QUANTITY',
            'BLOCKED_QUANTITY'
        ]
        # Define date columns
        self.date_columns = [
            'DATA_DATE',
            'EXPIRY_DATE'
        ]
        # Define categorical columns and their allowed values
        self.categorical_validations = {
            'INVENTORY_CATEGORY': ['GN', 'PR', 'PU']
        }
    def validate_data(self, df: pd.DataFrame) -> bool:
        """
        Validate stock data.
        Args:
            df (pd.DataFrame): DataFrame to validate
        Returns:
            bool: True if validation passes, False otherwise
        """
        is_valid = True
        # Check required columns
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        if missing_columns:
            self.errors.append(f"Missing required columns: {missing_columns}")
            is_valid = False
        # Check numeric columns
        for col in self.numeric_columns:
            if col in df.columns:
                non_numeric = df[~pd.to_numeric(df[col], errors='coerce').notnull()][col]
                if not non_numeric.empty:
                    self.errors.append(f"Non-numeric values found in {col}: {non_numeric.unique()}")
                    is_valid = False
        # Check date columns
        for col in self.date_columns:
            if col in df.columns:
                non_dates = df[~pd.to_datetime(df[col], errors='coerce').notnull()][col]
                if not non_dates.empty:
                    self.errors.append(f"Invalid dates found in {col}: {non_dates.unique()}")
                    is_valid = False
        # Check categorical values
        for col, allowed_values in self.categorical_validations.items():
            if col in df.columns:
                invalid_values = df[df[col].notna()][~df[col].isin(allowed_values)][col].unique()
                if len(invalid_values) > 0:
                    self.errors.append(f"Invalid values in {col}: {invalid_values}")
                    is_valid = False
        # Check for negative quantities
        for col in ['AVAILABLE_QUANTITY', 'BLOCKED_QUANTITY']:
            if col in df.columns:
                negative_values = df[df[col] < 0]
                if not negative_values.empty:
                    self.warnings.append(f"Negative values found in {col}")
        return is_valid
class SalesDataValidator:
    """Validator for sales data."""
    def __init__(self):
        """Initialize sales data validator."""
        self.errors = []
        self.warnings = []
        # Define required columns
        self.required_columns = [
            'DATA_DATE',
            'COUNTRY_NAME',
            'ORGANIZATION_NAME',
            'PRODUCT_ID',
            'SALES_QUANTITY',
            'SALES_VALUE'
        ]
        # Define numeric columns
        self.numeric_columns = [
            'SALES_QUANTITY',
            'RETURN_QUANTITY',
            'SALES_VALUE',
            'RETURN_VALUE',
            'TAX_IDENTIFICATION_NUMBER'
        ]
        # Define date columns
        self.date_columns = [
            'DATA_DATE',
            'INVOICE_DATE'
        ]
        # Define categorical columns and their allowed values
        self.categorical_validations = {
            'SALES_CATEGORY': ['GN', 'PR', 'PU']
        }
    def validate_data(self, df: pd.DataFrame) -> bool:
        """
        Validate sales data.
        Args:
            df (pd.DataFrame): DataFrame to validate
        Returns:
            bool: True if validation passes, False otherwise
        """
        is_valid = True
        # Check required columns
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        if missing_columns:
            self.errors.append(f"Missing required columns: {missing_columns}")
            is_valid = False
        # Check numeric columns
        for col in self.numeric_columns:
            if col in df.columns:
                non_numeric = df[~pd.to_numeric(df[col], errors='coerce').notnull()][col]
                if not non_numeric.empty:
                    self.errors.append(f"Non-numeric values found in {col}: {non_numeric.unique()}")
                    is_valid = False
        # Check date columns
        for col in self.date_columns:
            if col in df.columns:
                non_dates = df[~pd.to_datetime(df[col], errors='coerce').notnull()][col]
                if not non_dates.empty:
                    self.errors.append(f"Invalid dates found in {col}: {non_dates.unique()}")
                    is_valid = False
        # Check categorical values
        for col, allowed_values in self.categorical_validations.items():
            if col in df.columns:
                invalid_values = df[df[col].notna()][~df[col].isin(allowed_values)][col].unique()
                if len(invalid_values) > 0:
                    self.errors.append(f"Invalid values in {col}: {invalid_values}")
                    is_valid = False
        # Check for negative quantities and values
        if 'SALES_QUANTITY' in df.columns and 'RETURN_QUANTITY' in df.columns:
            net_quantity = df['SALES_QUANTITY'] - df['RETURN_QUANTITY']
            if (net_quantity < 0).any():
                self.warnings.append("Return quantity exceeds sales quantity")
        if 'SALES_VALUE' in df.columns and 'RETURN_VALUE' in df.columns:
            net_value = df['SALES_VALUE'] - df['RETURN_VALUE']
            if (net_value < 0).any():
                self.warnings.append("Return value exceeds sales value")
        return is_valid
class DataConsistencyValidator:
    """Validator for data consistency across related datasets."""
    def validate_stock_sales_consistency(
        self, 
        stock_df: pd.DataFrame, 
        sales_df: pd.DataFrame
    ) -> List[str]:
        """
        Validate consistency between stock and sales data.
        Args:
            stock_df (pd.DataFrame): Stock DataFrame
            sales_df (pd.DataFrame): Sales DataFrame
        Returns:
            List[str]: List of inconsistency warnings
        """
        warnings = []
        # Check product consistency
        stock_products = set(stock_df['PRODUCT_ID'].unique())
        sales_products = set(sales_df['PRODUCT_ID'].unique())
        products_in_sales_not_stock = sales_products - stock_products
        if products_in_sales_not_stock:
            warnings.append(
                f"Products in sales but not in stock: {products_in_sales_not_stock}"
            )
        # Check organization consistency
        stock_orgs = set(stock_df['ORGANIZATION_NAME'].unique())
        sales_orgs = set(sales_df['ORGANIZATION_NAME'].unique())
        orgs_mismatch = stock_orgs.symmetric_difference(sales_orgs)
        if orgs_mismatch:
            warnings.append(
                f"Organizations not matching between stock and sales: {orgs_mismatch}"
            )
        return warnings
</file>

<file path="src/validation/schema_validator.py">
"""
Schema validation for data frames.
"""
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
from src.utils.date_utils import validate_date_range
class SchemaValidator:
    """Validator for DataFrame schemas."""
    def __init__(self, schema: Dict[str, str]):
        """
        Initialize schema validator.
        Args:
            schema (Dict[str, str]): Schema dictionary mapping column names to data types
        """
        self.schema = schema
        self.errors = []
    def validate_columns(self, df: pd.DataFrame) -> bool:
        """
        Validate presence and types of columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
        Returns:
            bool: True if validation passes, False otherwise
        """
        # Check for missing columns
        missing_columns = [col for col in self.schema.keys() if col not in df.columns]
        if missing_columns:
            self.errors.append(f"Missing columns: {missing_columns}")
            return False
        # Check column types
        for col, dtype in self.schema.items():
            try:
                if dtype == 'datetime64[ns]':
                    pd.to_datetime(df[col], errors='raise')
                elif dtype == 'Int64':
                    pd.to_numeric(df[col], errors='raise')
                elif dtype == 'float64':
                    pd.to_numeric(df[col], errors='raise')
            except (ValueError, TypeError):
                self.errors.append(f"Invalid type for column {col}. Expected {dtype}")
                return False
        return True
    def validate_mandatory_values(self, df: pd.DataFrame, mandatory_columns: List[str]) -> bool:
        """
        Validate presence of values in mandatory columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            mandatory_columns (List[str]): List of mandatory column names
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col in mandatory_columns:
            if col not in df.columns:
                self.errors.append(f"Missing mandatory column: {col}")
                return False
            null_count = df[col].isnull().sum()
            if null_count > 0:
                self.errors.append(f"Found {null_count} null values in mandatory column {col}")
                return False
        return True
    def validate_numeric_ranges(
        self, 
        df: pd.DataFrame, 
        range_validations: Dict[str, Dict[str, float]]
    ) -> bool:
        """
        Validate numeric ranges for specified columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            range_validations (Dict[str, Dict[str, float]]): Dictionary mapping column names
                to their min/max range values
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col, ranges in range_validations.items():
            if col not in df.columns:
                continue
            min_val = ranges.get('min')
            max_val = ranges.get('max')
            if min_val is not None and (df[col] < min_val).any():
                self.errors.append(f"Values below minimum {min_val} found in column {col}")
                return False
            if max_val is not None and (df[col] > max_val).any():
                self.errors.append(f"Values above maximum {max_val} found in column {col}")
                return False
        return True
    def validate_date_ranges(
        self, 
        df: pd.DataFrame, 
        date_columns: List[str],
        min_date: Optional[datetime] = None,
        max_date: Optional[datetime] = None
    ) -> bool:
        """
        Validate date ranges for specified columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            date_columns (List[str]): List of date column names
            min_date (datetime, optional): Minimum allowed date
            max_date (datetime, optional): Maximum allowed date
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col in date_columns:
            if col not in df.columns:
                continue
            dates = pd.to_datetime(df[col], errors='coerce')
            if min_date is not None and (dates < min_date).any():
                self.errors.append(f"Dates before {min_date} found in column {col}")
                return False
            if max_date is not None and (dates > max_date).any():
                self.errors.append(f"Dates after {max_date} found in column {col}")
                return False
        return True
    def validate_categorical_values(
        self, 
        df: pd.DataFrame, 
        categorical_validations: Dict[str, List[str]]
    ) -> bool:
        """
        Validate categorical values for specified columns.
        Args:
            df (pd.DataFrame): DataFrame to validate
            categorical_validations (Dict[str, List[str]]): Dictionary mapping column names
                to their allowed values
        Returns:
            bool: True if validation passes, False otherwise
        """
        for col, allowed_values in categorical_validations.items():
            if col not in df.columns:
                continue
            invalid_values = df[df[col].notna()][~df[col].isin(allowed_values)][col].unique()
            if len(invalid_values) > 0:
                self.errors.append(
                    f"Invalid values found in column {col}: {invalid_values}"
                )
                return False
        return True
    def get_validation_errors(self) -> List[str]:
        """
        Get list of validation errors.
        Returns:
            List[str]: List of error messages
        """
        return self.errors 
    def validate_mandatory_columns(self, df, context):
        """Add column count validation"""
        mandatory_cols = self.get_mandatory_columns(context)
        matching_cols = [col for col in df.columns if col in mandatory_cols]
        return len(matching_cols) == len(mandatory_cols) 
    def validate_dates(self, df: pd.DataFrame, date_columns: Dict[str, Dict]) -> bool:
        """
        Validate date columns in DataFrame.
        Args:
            df (pd.DataFrame): DataFrame to validate
            date_columns (Dict[str, Dict]): Dictionary mapping column names to validation rules
        Returns:
            bool: True if validation passes
        """
        for col, rules in date_columns.items():
            if col not in df.columns:
                continue
            min_date = rules.get('min_date')
            max_date = rules.get('max_date')
            invalid_dates = df[~df[col].apply(
                lambda x: validate_date_range(x, min_date, max_date)
            )]
            if not invalid_dates.empty:
                self.errors.append(
                    f"Invalid dates found in column {col}"
                )
                return False
        return True
</file>

<file path="brainstorming.md">
# Migration and Validation Ideas

## Compatibility Validation Plan

### 1. Configuration Management
- Move all string constants to configuration
- Create centralized mapping definitions
- Document all special cases and their rules

### 2. Test Cases Required
```python
def validate_compatibility():
    test_cases = [
        # Case 1: Column renaming
        {
            'input': {'QUANTITY_AVAILABLE': 100},
            'expected': {'AVAILABLE_QUANTITY': 100}
        },
        # Case 2: Special case handling
        {
            'data_owner': 'SURGIPHARM',
            'structure': 'POSITION',
            'input': {'REGION': 'Test'},
            'expected': {'BRANCH_NAME': 'Test'}
        },
        # Case 3: Duplicate handling
        {
            'input': ['COL', 'COL', 'COL'],
            'expected': ['COL1', 'COL2', 'COL3']
        }
    ]
```

### 3. Pre-deployment Checklist
- [ ] All constants moved to configuration
- [ ] All special cases documented and implemented
- [ ] Database queries preserved
- [ ] S3 paths and access patterns unchanged
- [ ] Error handling covers all original cases
- [ ] Logging matches or exceeds original coverage

### 4. Validation Steps
1. Create parallel test environment
2. Run both old and new jobs with same input
3. Compare outputs:
   - Column names
   - Data types
   - Special case handling
   - Error handling
4. Verify database operations:
   - Table creation
   - Data insertion
   - Status updates

### 5. Risk Areas to Monitor
1. String constant references
2. Special case handling logic
3. S3 file path handling
4. Database connection patterns
5. Error handling coverage

## Migration Strategy

### Phase 1: Code Cleanup
1. Fix string quotations and references
2. Standardize column name references
3. Document all special cases

### Phase 2: Parallel Testing
1. Set up test environment
2. Run old and new code in parallel
3. Compare outputs
4. Document discrepancies

### Phase 3: Migration
1. Deploy new code structure
2. Monitor initial runs
3. Keep old code as backup
4. Gradual transition

### Phase 4: Validation
1. Verify all data transformations
2. Check error handling
3. Validate special cases
4. Performance comparison

## Future Improvements

### 1. Error Handling
- Enhanced logging
- Better error messages
- Automated error reporting

### 2. Performance Optimization
- Batch processing improvements
- Memory usage optimization
- Query optimization

### 3. Monitoring
- Real-time job status
- Performance metrics
- Error rate tracking

### 4. Documentation
- API documentation
- Configuration guide
- Troubleshooting guide
- Development guidelines
</file>

<file path="file_standardization_2025-02-03.py">
# Standard Python libraries
import datetime
from datetime import date, timedelta, datetime
import calendar
import json
import re
from decimal import Decimal
import sys
# Third-party libraries
import boto3
import s3fs
import pandas as pd
import numpy as np
# PySpark and AWS Glue libraries
from pyspark.context import SparkContext
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    TimestampType,
    DateType,
    LongType,
    DecimalType,
)
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
# ============================================================
# Job Initialization
# ============================================================
args = getResolvedOptions(sys.argv, [JOB_NAME, secret_name, env])
sparkContext = SparkContext()
glueContext = GlueContext(sparkContext)
sparkSession = glueContext.spark_session
glueJob = Job(glueContext)
glueJob.init(args[JOB_NAME], args)
# AWS Secrets Manager
def get_secret(secret_name):
    client = boto3.client('secretsmanager', region_name='eu-central-1')
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    secret = json.loads(get_secret_value_response[SecretString])
    return secret
secret_name = args[secret_name]
secret = get_secret(secret_name)
username = secret[username]
password = secret[password]
schemaname = imip
host = secret[host]
port = secret[port]
dbname = secret[dbname]
jdbc_url = f"jdbc:postgresql://{host}:{port}/{dbname}"
jdbc_properties = {
    "user": username,
    "password": password,
    "driver": "org.postgresql.Driver",
}
# ============================================================
# Define the path where the files are located as s3_main_path
# ============================================================
env_lower = args['env'].lower()
print(f"Debug 230: The environment is set to {env_lower}")
s3_main_path = f"s3://pt-s3-imip-{env_lower}-imip-all-data/mail/mntc/RocheIMIP-file/RECEIVED_FILE_PATH/"
# ============================================================
# Function to Insert Data into temp_load_info Table
# ============================================================
def insert_into_temp_load_info(v_table_name, v_context, v_frequency, v_is_invoice, v_is_process):
    v_table_name = v_table_name.upper()
    # Convert v_table_name to uppercase
    # Define the schema for the temp_load_info table
    print("Step 1: Defining schema for temp_load_info table.")
    schema = StructType(
        [
            StructField(table_name, StringType(), nullable=True),
            StructField(context, StringType(), nullable=True),
            StructField(frequency, StringType(), nullable=True),
            StructField(is_invoice, IntegerType(), nullable=True),
            StructField(is_process, IntegerType(), nullable=True),
            StructField(load_datetime, TimestampType(), nullable=True),
        ]
    )
    print("Step 2: Creating DataFrame with the provided values.")
    spark = SparkSession.builder.appName("PostgreSQL Writing").getOrCreate()
    print("Step 3: Creating Spark session for PostgreSQL write.")
    load_datetime = datetime.now()
    data = [
        Row(
            table_name=v_table_name,
            context=v_context,
            frequency=v_frequency,
            is_invoice=v_is_invoice,
            is_process=v_is_process,
            load_datetime=load_datetime,
        )
    ]
    df = spark.createDataFrame(data, schema=schema)
    print("Step 4: Writing DataFrame to PostgreSQL database.")
    df.write.format(jdbc).option(url, jdbc_url).option(dbtable, f"{schemaname}.temp_load_info").option(
        user, username
    ).option(password, password).mode(append).save()
    print("Step 5: Data successfully written to temp_load_info table.")
# ============================================================
# Function to update_daq_log_info
# ============================================================
def update_daq_log_info(id):
    """
    Updates the daq_log_info table in PostgreSQL to mark a record as processed.
    Parameters
    - id The ID of the record to be updated (can be numeric or non-numeric).
    - schemaname The schema name where the table is located.
    - jdbc_url JDBC URL for the PostgreSQL database.
    - username Username for the database connection.
    - password Password for the database connection.
    """
    # Create PostgreSQL JDBC connection
    spark = SparkSession.builder.appName("PostgreSQL Writing").getOrCreate()
    connSpark = spark.sparkContext._jvm.java.sql.DriverManager.getConnection(jdbc_url, username, password)
    stmt = connSpark.createStatement()
    # Check if id is numeric
    if isinstance(id, (int, float)):
        sql_execute_query = (
            f"UPDATE {schemaname}.daq_log_info SET is_processed = 1 WHERE id = {id}"
        )
    else:
        sql_execute_query = f"UPDATE {schemaname}.daq_log_info SET is_processed = 1 WHERE is_corrupt = 0 and is_processed = 0"
    print(f"Executing SQL update, {sql_execute_query}")
    stmt.execute(sql_execute_query)
    print("DAQ_LOG_INFO updated successfully.")
    if connSpark:
        connSpark.close()
# ============================================================
# Data is Selected from DAQ_LOG_INFO Table
# ============================================================
query_daq_log_info = f"""
    SELECT DISTINCT id, receiver_address, sender_address, file, sheet_name as daq_sheet_name, mail_date, 
    CASE 
        WHEN position('.' in reverse(file)) > 0 THEN 
        substring(file from (char_length(file) - position('.' in reverse(file)) + 2)) 
    ELSE 'EMPTY' 
    END as file_extension, 
    (date_trunc('month', mail_date) - INTERVAL '1 day') AS mail_date_prev_month_last_day 
    FROM {schemaname}.daq_log_info a 
    WHERE EXISTS ( 
        SELECT 1 
        FROM ( 
            SELECT 
                file, 
                sheet_name, 
                subject, 
                receiver_address, 
                sender_address, 
                to_char(mail_date, 'yyyymmdd') as str_mail_date, 
                max(id) as max_id 
            FROM {schemaname}.daq_log_info 
            WHERE is_corrupt = 0 AND is_processed = 0 
            GROUP BY file, sheet_name, subject, receiver_address, sender_address, to_char(mail_date, 'yyyymmdd') 
        ) b 
        WHERE a.id = b.max_id 
    )
"""
source_df = (
    sparkSession.read.format(jdbc)
    .option(url, jdbc_url)
    .option(query, query_daq_log_info)
    .option(driver, org.postgresql.Driver)
    .option(user, username)
    .option(password, password)
    .load()
)
dynamic_dframe = DynamicFrame.fromDF(source_df, glueContext, dynamic_df)
result_daq_log_info = (
    dynamic_dframe.select_fields(
        [
            id,
            receiver_address,
            sender_address,
            file,
            daq_sheet_name,
            mail_date,
            file_extension,
            mail_date_prev_month_last_day,
        ]
    )
    .toDF()
    .collect()
)
# ============================================================
# Data is Selected from DAQ_LOG_INFO Table and For loop is Started
# ============================================================
for row_daq_log_info in result_daq_log_info:
    (
        id,
        receiver_address,
        sender_address,
        file,
        daq_sheet_name,
        mail_date,
        file_extension,
        mail_date_prev_month_last_day,
    ) = row_daq_log_info
    table_name = f"temp_{id}"
    print(id , id)
    print(receiver_address , receiver_address)
    print(sender_address , sender_address)
    print(file , file)
    print(daq_sheet_name , daq_sheet_name)
    print(mail_date , mail_date)
    print(file_extension , file_extension)
    print(mail_date_prev_month_last_day , mail_date_prev_month_last_day)
    print("--------------------------------")
    # ============================================================
    # Read data from DD_ENTITY_DETAIL table
    # With Filter Applied to Sheet_name Column
    # Filter is applied to data_owner_mail and sender_address columns
    # ============================================================
    query_entity_detail = (f"""
        select distinct data_owner, country, context, period, frequency, data_owner_mail, structure, 
               file_table_name as entity_file_table_name, sheet_name as entity_sheet_name, 
               lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1, 
               lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) as sheet_name2, 
               case when lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
                         lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) 
                    then 1 
                    else 0 
               end as sheet_name_comparison_result 
          from  + schemaname + .dd_entity_detail 
        WHERE is_api = 0 
        AND data_owner_mail = ' + sender_address + ' 
        AND lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) 
    """)
    print(f"query_entity_detail 1 , {query_entity_detail}")
    source_df_entity_detail = (
        sparkSession.read.format(jdbc)
        .option(url, jdbc_url)
        .option(query, query_entity_detail)
        .option(driver, org.postgresql.Driver)
        .option(user, username)
        .option(password, password)
        .load()
    )
    dynamic_dframe_entity_detail = DynamicFrame.fromDF(source_df_entity_detail, glueContext, dynamic_df)
    result_entity_detail = (
        dynamic_dframe_entity_detail.select_fields(
            [
                data_owner,
                country,
                context,
                period,
                frequency,
                data_owner_mail,
                structure,
                entity_file_table_name,
                entity_sheet_name,
                sheet_name1,
                sheet_name2,
                sheet_name_comparison_result,
            ]
        )
        .toDF()
        .collect()
    )
    # ============================================================
    # Read data from DD_ENTITY_DETAIL table
    # With Filter Applied to Sheet_name Column
    # Filter is applied on File Name and COUNTRY Columns in DAQ_LOG_INFO Table
    # If there is no data from the first query, run the second query.
    # ============================================================
    if len(result_entity_detail) == 0:
        query_entity_detail = (f"""
            SELECT DISTINCT data_owner, country, context, period, frequency, data_owner_mail, structure, 
                   file_table_name as entity_file_table_name, sheet_name as entity_sheet_name, 
                   lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1, 
                   lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) as sheet_name2, 
                   case when lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
                   lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) then 1 else 0 end as sheet_name_comparison_result 
              FROM 
                   (SELECT ed., 
                           case  
                                when position('.' in reverse(file_table_name))  0 
                                then substring(file_table_name from (char_length(file_table_name) - position('.' in reverse(file_table_name)) + 2)) 
                                else 'EMPTY' 
                            end as file_extension 
                      FROM  + schemaname + .dd_entity_detail ed 
                   ) 
             WHERE is_api = 0 
               AND 
                   CASE 
                        WHEN upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) = 'XLS' 
                        THEN 'XLSX' 
                        ELSE upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) 
                    END = 
                   CASE 
                        WHEN replace(upper(coalesce(replace(' + file_extension + ', 'n', ''), 'file_ext')), 'İ', 'I') = 'XLS' 
                        THEN 'XLSX' 
                        ELSE replace(upper(coalesce(replace(' + file_extension + ', 'n', ''), 'file_ext')), 'İ', 'I') 
                    END 
            AND sheet_name = ' + daq_sheet_name + ' 
            AND lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) 
             AND ( 
                  ( 
                    (replace(replace(Upper(data_owner), 'İ', 'I'), ' ', '') = replace(replace(Upper(' + sender_address + '), 'İ', 'I'), ' ', '')) 
                    OR 
                    ((SELECT (Regexp_matches(data_owner_mail, '@([^.]+)'))[1]) = ' + sender_address + ') 
                  ) 
                  OR 
                  ( 
                    country = ( 
                              SELECT DISTINCT dc.country_name 
                                FROM  + schemaname + .email_connection_info eci, 
                                      + schemaname + .dim_countries dc 
                               WHERE eci.email_address = ' + receiver_address + ' 
                                 AND dc.country_id = eci.country_id 
                               ) 
                    AND (CASE 
                              WHEN POSITION('-' IN file_table_name)  0 
                              THEN SUBSTRING(file_table_name, 1, POSITION('-' IN file_table_name) - 1) 
                              WHEN POSITION('.' IN file_table_name)  0 
                              THEN SUBSTRING(file_table_name, 1, POSITION('.' IN file_table_name) - 1) 
                              ELSE file_table_name 
                          END) = 
                        (SELECT DISTINCT 
                                CASE 
                                     WHEN POSITION('-' IN file)  0 THEN SUBSTRING(file, 1, POSITION('-' IN file) - 1) 
                                     WHEN POSITION('.' IN file)  0 THEN SUBSTRING(file, 1, POSITION('.' IN file) - 1) 
                                     ELSE file 
                                 END AS file 
                            FROM  + schemaname + .daq_log_info 
                           WHERE id =  + str(id) + ) 
                        ) 
            ) 
        """)
        print(f"query_entity_detail 2 , {query_entity_detail}")
        source_df_entity_detail = (
            sparkSession.read.format(jdbc)
            .option(url, jdbc_url)
            .option(query, query_entity_detail)
            .option(driver, org.postgresql.Driver)
            .option(user, username)
            .option(password, password)
            .load()
        )
        dynamic_dframe_entity_detail = DynamicFrame.fromDF(source_df_entity_detail, glueContext, dynamic_df)
        result_entity_detail = (
            dynamic_dframe_entity_detail.select_fields(
                [
                    data_owner,
                    country,
                    context,
                    period,
                    frequency,
                    data_owner_mail,
                    structure,
                    entity_file_table_name,
                    entity_sheet_name,
                    sheet_name1,
                    sheet_name2,
                    sheet_name_comparison_result,
                ]
            )
            .toDF()
            .collect()
        )
    # ============================================================
    # Read data from DD_ENTITY_DETAIL table
    # Without Filter Applied to Sheet_name Column
    # Filter is applied to data_owner_mail and sender_address columns
    # If there is no data from the third query, run the second query.
    # ============================================================
    if len(result_entity_detail) == 0:
        query_entity_detail = (f"""
            select distinct data_owner, country, context, period, frequency, data_owner_mail, structure, 
                   file_table_name as entity_file_table_name, sheet_name as entity_sheet_name, 
                   lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1, 
                   lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) as sheet_name2, 
                   case when lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
                             lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) 
                        then 1 
                        else 0 
                   end as sheet_name_comparison_result 
              from  + schemaname + .dd_entity_detail 
             WHERE is_api = 0 
               AND data_owner_mail = ' + sender_address + ' 
        """)
        print(f"query_entity_detail 3 , {query_entity_detail}")
        source_df_entity_detail = (
            sparkSession.read.format(jdbc)
            .option(url, jdbc_url)
            .option(query, query_entity_detail)
            .option(driver, org.postgresql.Driver)
            .option(user, username)
            .option(password, password)
            .load()
        )
        dynamic_dframe_entity_detail = DynamicFrame.fromDF(source_df_entity_detail, glueContext, dynamic_df)
        result_entity_detail = (
            dynamic_dframe_entity_detail.select_fields(
                [
                    data_owner,
                    country,
                    context,
                    period,
                    frequency,
                    data_owner_mail,
                    structure,
                    entity_file_table_name,
                    entity_sheet_name,
                    sheet_name1,
                    sheet_name2,
                    sheet_name_comparison_result,
                ]
            )
            .toDF()
            .collect()
        )
    # ============================================================
    # Read data from DD_ENTITY_DETAIL table
    # Without Filter Applied to Sheet_name Column
    # Filter is applied on File Name and COUNTRY Columns in DAQ_LOG_INFO Table
    # If there is no data from the fourth query, run the second query.
    # ============================================================
    if len(result_entity_detail) == 0:
        query_entity_detail = (f"""
            SELECT DISTINCT data_owner, country, context, period, frequency, data_owner_mail, structure, 
                   file_table_name as entity_file_table_name, sheet_name as entity_sheet_name, 
                   lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) as sheet_name1, 
                   lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) as sheet_name2, 
                   case when lower(regexp_replace(sheet_name, '[0-9]', '', 'g')) = 
                   lower(regexp_replace(' + daq_sheet_name + ', '[0-9]', '', 'g')) then 1 else 0 end as sheet_name_comparison_result 
              FROM 
                   (SELECT ed., 
                           case  
                                when position('.' in reverse(file_table_name))  0 
                                then substring(file_table_name from (char_length(file_table_name) - position('.' in reverse(file_table_name)) + 2)) 
                                else 'EMPTY' 
                            end as file_extension 
                      FROM  + schemaname + .dd_entity_detail ed 
                   ) 
             WHERE is_api = 0 
               AND 
                   CASE 
                        WHEN upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) = 'XLS' 
                        THEN 'XLSX' 
                        ELSE upper(coalesce(replace(file_extension, 'n', ''), 'file_ext')) 
                    END = 
                   CASE 
                        WHEN replace(upper(coalesce(replace(' + file_extension + ', 'n', ''), 'file_ext')), 'İ', 'I') = 'XLS' 
                        THEN 'XLSX' 
                        ELSE replace(upper(coalesce(replace(' + file_extension + ', 'n', ''), 'file_ext')), 'İ', 'I') 
                    END 
             AND ( 
                  ( 
                    (replace(replace(Upper(data_owner), 'İ', 'I'), ' ', '') = replace(replace(Upper(' + sender_address + '), 'İ', 'I'), ' ', '')) 
                    OR 
                    ((SELECT (Regexp_matches(data_owner_mail, '@([^.]+)'))[1]) = ' + sender_address + ') 
                  ) 
                  OR 
                  ( 
                    country = ( 
                              SELECT DISTINCT dc.country_name 
                                FROM  + schemaname + .email_connection_info eci, 
                                      + schemaname + .dim_countries dc 
                               WHERE eci.email_address = ' + receiver_address + ' 
                                 AND dc.country_id = eci.country_id 
                               ) 
                    AND (CASE 
                              WHEN POSITION('-' IN file_table_name)  0 
                              THEN SUBSTRING(file_table_name, 1, POSITION('-' IN file_table_name) - 1) 
                              WHEN POSITION('.' IN file_table_name)  0 
                              THEN SUBSTRING(file_table_name, 1, POSITION('.' IN file_table_name) - 1) 
                              ELSE file_table_name 
                          END) = 
                        (SELECT DISTINCT 
                                CASE 
                                     WHEN POSITION('-' IN file)  0 THEN SUBSTRING(file, 1, POSITION('-' IN file) - 1) 
                                     WHEN POSITION('.' IN file)  0 THEN SUBSTRING(file, 1, POSITION('.' IN file) - 1) 
                                     ELSE file 
                                 END AS file 
                            FROM  + schemaname + .daq_log_info 
                           WHERE id =  + str(id) + ) 
                        ) 
            ) 
        )""")
        print(f"query_entity_detail 4 , {query_entity_detail}")
        source_df_entity_detail = (
            sparkSession.read.format(jdbc)
            .option(url, jdbc_url)
            .option(query, query_entity_detail)
            .option(driver, org.postgresql.Driver)
            .option(user, username)
            .option(password, password)
            .load()
        )
        dynamic_dframe_entity_detail = DynamicFrame.fromDF(source_df_entity_detail, glueContext, dynamic_df)
        result_entity_detail = (
            dynamic_dframe_entity_detail.select_fields(
                [
                    data_owner,
                    country,
                    context,
                    period,
                    frequency,
                    data_owner_mail,
                    structure,
                    entity_file_table_name,
                    entity_sheet_name,
                    sheet_name1,
                    sheet_name2,
                    sheet_name_comparison_result,
                ]
            )
            .toDF()
            .collect()
        )
    # ============================================================
    # Read data from table DD_ATTRIBUTE_DETAIL in the loop
    # ============================================================
    for row_entity_detail in result_entity_detail:
        (
            data_owner,
            country,
            context,
            period,
            frequency,
            data_owner_mail,
            structure,
            entity_file_table_name,
            entity_sheet_name,
            sheet_name1,
            sheet_name2,
            sheet_name_comparison_result,
        ) = row_entity_detail
        print(data_owner , data_owner)
        print(country , country)
        print(context , context)
        print(period , period)
        print(frequency , frequency)
        print(data_owner_mail , data_owner_mail)
        print(structure , structure)
        print(entity_file_table_name , entity_file_table_name)
        print(entity_sheet_name , entity_sheet_name)
        print(sheet_name1 , sheet_name1)
        print(sheet_name2 , sheet_name2)
        print(sheet_name_comparison_result , sheet_name_comparison_result)
        if data_owner == ALLIANCE and country == TURKIYE and daq_sheet_name.lower() != urundepobazinda:
            print(f"Condition met Data owner is '{data_owner}' and entity sheet name '{daq_sheet_name}' is different from 'urundepobazinda'.")
            print(f"Condition met Exiting the loop for '{daq_sheet_name}'.")
            break
        if data_owner == RAFED and country == UAE and daq_sheet_name.lower() not in ("mafraq-ssmc data", "tawam data"):
            print(f"Condition met Data owner is '{data_owner}' and entity sheet name '{daq_sheet_name}' is different from 'urundepobazinda'.")
            print(f"Condition met Exiting the loop for '{daq_sheet_name}'.")
            break
        query_attribute_detail = (f"""
            select original_column_name, second_column_name, etl_column_name, 
             column_position, starting_row, is_mandatory 
            from  + schemaname + .dd_attribute_detail 
            where data_owner = ' + data_owner + ' 
            and context = ' + context + ' 
            and file_table_name = ' + entity_file_table_name + ' 
            and sheet_name = ' + entity_sheet_name + ' 
            order by column_position, starting_row
        """)
        print(f"query_attribute_detail , {query_attribute_detail}")
        source_df_attribute_detail = (
            sparkSession.read.format(jdbc)
            .option(url, jdbc_url)
            .option(query, query_attribute_detail)
            .option(driver, org.postgresql.Driver)
            .option(user, username)
            .option(password, password)
            .load()
        )
        dynamic_dframe_attribute_detail = DynamicFrame.fromDF(source_df_attribute_detail, glueContext, dynamic_df)
        result_attribute_detail = (
            dynamic_dframe_attribute_detail.select_fields(
                [
                    original_column_name,
                    second_column_name,
                    etl_column_name,
                    column_position,
                    starting_row,
                    is_mandatory,
                ]
            )
            .toDF()
            .collect()
        )
        # ============================================================
        # Creating DataFrame for Inventory
        # ============================================================
        df_Result_Stock = pd.DataFrame(
            {
                "DATA_DATE": pd.Series(dtype=datetime64[ns]),  # DATE
                "COUNTRY_NAME": pd.Series(dtype=object),  # TEXT
                "ORGANIZATION_NAME": pd.Series(dtype=object),  # TEXT
                "BRANCH_NAME": pd.Series(dtype=object),  # TEXT
                "PRODUCT_ID": pd.Series(dtype=object),  # TEXT
                "PRODUCT_NAME": pd.Series(dtype=object),  # TEXT
                "AVAILABLE_QUANTITY": pd.Series(dtype=Int64),  # Nullable bigint
                "BLOCKED_QUANTITY": pd.Series(dtype=Int64),  # Nullable bigint
                "INVENTORY_CATEGORY": pd.Series(dtype=object),  # TEXT
                "BATCH_NUMBER": pd.Series(dtype=object),  # TEXT
                "EXPIRY_DATE": pd.Series(dtype=datetime64[ns]),  # DATE
            }
        )
        # ============================================================
        # Creating DataFrame for Sales
        # ============================================================
        df_Result_Sales = pd.DataFrame(
            {
                "DATA_DATE": pd.Series(dtype=datetime64[ns]),  # DATE
                "COUNTRY_NAME": pd.Series(dtype=object),  # TEXT
                "ORGANIZATION_NAME": pd.Series(dtype=object),  # TEXT
                "BRANCH_NAME": pd.Series(dtype=object),  # TEXT
                "CUSTOMER_ID": pd.Series(dtype=object),  # TEXT
                "CUSTOMER_NAME": pd.Series(dtype=object),  # TEXT
                "PRODUCT_ID": pd.Series(dtype=object),  # TEXT
                "PRODUCT_NAME": pd.Series(dtype=object),  # TEXT
                "INVOICE_DATE": pd.Series(dtype=datetime64[ns]),  # DATE
                "SALES_QUANTITY": pd.Series(dtype=Int64),  # BIGINT
                "RETURN_QUANTITY": pd.Series(dtype=Int64),  # BIGINT
                "SALES_CATEGORY": pd.Series(dtype=object),  # TEXT
                "SALES_VALUE": pd.Series(dtype=float64),  # DECIMAL(22,2) - float64 en uygun seçenektir
                "RETURN_VALUE": pd.Series(dtype=float64),  # DECIMAL(22,2) - float64 en uygun seçenektir
                "AUCTION_NUMBER": pd.Series(dtype=object),  # TEXT
                "TAX_IDENTIFICATION_NUMBER": pd.Series(dtype=Int64),  # BIGINT
            }
        )
        # ============================================================
        # Basic date formats
        # ============================================================
        # Date format patterns organized by year format (4-digit vs 2-digit) and separator type
        date_formats = [
            # 4-digit year formats (YYYY)
            # Hyphen-separated
            "%Y-%m-%d %H%M%S",  # 2024-12-31 235959
            "%Y-%m-%d %H%M",    # 2024-12-31 2359
            "%Y-%m-%d",         # 2024-12-31
            "%d-%m-%Y %H%M%S",  # 31-12-2024 235959
            "%d-%m-%Y %H%M",    # 31-12-2024 2359
            "%d-%m-%Y",         # 31-12-2024
            "%m-%d-%Y %H%M%S",  # 12-31-2024 235959
            "%m-%d-%Y %H%M",    # 12-31-2024 2359
            "%m-%d-%Y",         # 12-31-2024
            # Dot-separated
            "%Y.%m.%d %H%M%S",  # 2024.12.31 235959
            "%Y.%m.%d %H%M",    # 2024.12.31 2359
            "%Y.%m.%d",         # 2024.12.31
            "%d.%m.%Y %H%M%S",  # 31.12.2024 235959
            "%d.%m.%Y %H%M",    # 31.12.2024 2359
            "%d.%m.%Y",         # 31.12.2024
            "%m.%d.%Y %H%M%S",  # 12.31.2024 235959
            "%m.%d.%Y %H%M",    # 12.31.2024 2359
            "%m.%d.%Y",         # 12.31.2024
            # No separator
            "%Y%m%d%H%M%S",     # 20241231235959
            "%Y%m%d%H%M",       # 202412312359
            "%Y%m%d",           # 20241231
            "%d%m%Y%H%M%S",     # 31122024235959
            "%d%m%Y%H%M",       # 311220242359
            "%d%m%Y",           # 31122024
            "%m%d%Y%H%M%S",     # 12312024235959
            "%m%d%Y%H%M",       # 123120242359
            "%m%d%Y",           # 12312024
            # 2-digit year formats (YY)
            # Hyphen-separated
            "%d-%m-%y %H%M%S",  # 31-12-24 235959
            "%d-%m-%y %H%M",    # 31-12-24 2359
            "%d-%m-%y",         # 31-12-24
            "%y-%m-%d %H%M%S",  # 24-12-31 235959
            "%y-%m-%d %H%M",    # 24-12-31 2359
            "%y-%m-%d",         # 24-12-31
            "%m-%d-%y %H%M%S",  # 12-31-24 235959
            "%m-%d-%y %H%M",    # 12-31-24 2359
            "%m-%d-%y",         # 12-31-24
            # Dot-separated
            "%d.%m.%y %H%M%S",  # 31.12.24 235959
            "%d.%m.%y %H%M",    # 31.12.24 2359
            "%d.%m.%y",         # 31.12.24
            "%y.%m.%d %H%M%S",  # 24.12.31 235959
            "%y.%m.%d %H%M",    # 24.12.31 2359
            "%y.%m.%d",         # 24.12.31
            "%m.%d.%y %H%M%S",  # 12.31.24 235959
            "%m.%d.%y %H%M",    # 12.31.24 2359
            "%m.%d.%y",         # 12.31.24
            # No separator
            "%y%m%d%H%M%S",     # 241231235959
            "%y%m%d%H%M",       # 2412312359
            "%y%m%d",           # 241231
            "%d%m%y%H%M%S",     # 311224235959
            "%d%m%y%H%M",       # 3112242359
            "%d%m%y",           # 311224
            "%m%d%y%H%M%S",     # 123124235959
            "%m%d%y%H%M",       # 1231242359
            "%m%d%y",           # 123124
        ]
        # ============================================================
        # Read Excel file from S3 bucket
        # ============================================================
        s3_path = s3_main_path + str(id) + ".xlsx"
        print(f"Debug Reading file from S3 path {s3_path}")
        s3 = s3fs.S3FileSystem()
        with s3.open(s3_path, "rb") as file:    
            excel_data = pd.read_excel(
                file,
                sheet_name="Sheet1",
                dtype=str,
                keep_default_na=False,
                engine="openpyxl",
                header=None,
            )
            print("Debug Excel data read into Pandas DataFrame")
        # ============================================================
        # Convert Pandas DataFrame to Spark DataFrame
        # ============================================================
        veri_Excel_DF_base = sparkSession.createDataFrame(excel_data)
        print("Debug Converted Pandas DataFrame to Spark DataFrame")
        # Optional Convert back to Pandas DataFrame if needed
        veri_Excel_DF = veri_Excel_DF_base.toPandas()
        print("Debug Converted Spark DataFrame to Pandas DataFrame")
        print("File read from S3 bucket and processed successfully")
        # ============================================================
        # Create base1_dataframe1 and base2_dataframe2
        # ============================================================
        # Initialize empty DataFrames
        base1_dataframe1 = pd.DataFrame()
        base2_dataframe2 = pd.DataFrame()
        if structure == TABULAR:
            # ============================================================
            # Remove empty columns
            # ============================================================
            column_headers = veri_Excel_DF.columns.values.tolist()
            print(f"Debug Column headers before removal of empty columns {column_headers}")
            # Loop through columns to remove empty columns
            for column_name in veri_Excel_DF.columns:
                satirDolu = False
                for value in veri_Excel_DF[column_name]:
                    value = str(value).strip()
                    if (
                        value != "" and str(value).lower() != "nan" and str(value).lower() != "nat"
                    ):  # Check if value is not empty, 'nan', or 'nat'
                        satirDolu = True
                # If at least one row is full, add this column to the new DataFrame
                if satirDolu:
                    base1_dataframe1[column_name] = veri_Excel_DF[column_name]
            print("Debug base1_dataframe1 after removing empty columns")
            print(base1_dataframe1)
            # ============================================================
            # Remove empty rows
            # ============================================================
            base2_dataframe2 = pd.DataFrame(columns=base1_dataframe1.columns)
            print("Debug base2_dataframe2 initialized with columns from base1_dataframe1")
            for index, row in base1_dataframe1.iterrows():
                # Combine all values in the row
                concatenated_row = (
                    "".join(row.astype(str)).lower().replace("nan", "").replace("nat", "").replace(" ", "")
                )
                if concatenated_row != "":
                    base2_dataframe2 = base2_dataframe2.append(row)
            print("Debug base2_dataframe2 after removing empty rows")
            print(base2_dataframe2)
        elif structure in ("POSITION", "CUSTOM POSITION", "CUSTOMRAFED"):
            # If structure is POSITION or CUSTOM, just copy the DataFrame
            base2_dataframe2 = veri_Excel_DF.copy()
            print("Debug base2_dataframe2 copied from veri_Excel_DF")
        # ============================================================
        # Finished the first step, base2_dataframe2 was filled
            # ============================================================
        print("Debug Finished processing. base2_dataframe2 has been filled.")
        # ============================================================
        # REMOVE THE LINES CONTAINING SPECIFIC TEXTS LIKE PAGE OF, TOTAL, ETC.
        # ============================================================
        # Define unwanted characters to remove from text
        unwanted_chars = "0123456789 .,=+-()[]"
        translation_table = str.maketrans("", "", unwanted_chars)
        # Define the set of words to check for in rows
        words_to_check = {pageof, page2of2, toplam, total, totalqty}
        # Process each row in base2_dataframe2
        for index, row in base2_dataframe2.iterrows():
            # Concatenate all values in the row into a single string and convert to lowercase
            concatenated_row = "".join(row.astype(str)).lower()
            # Remove 'nan' and 'nat' substrings
            concatenated_row = concatenated_row.replace(nan, ).replace(nat, )
            # Remove unwanted characters
            concatenated_row = concatenated_row.translate(translation_table)
            # Check if the processed row contains any of the unwanted words
            if concatenated_row in words_to_check:
                if structure == TABULAR:
                    # Drop the row if it matches the unwanted criteria
                    base2_dataframe2 = base2_dataframe2.drop(index)
                elif structure in ("POSITION", "CUSTOM POSITION"):
                    # Replace the row with NaN values if it matches the unwanted criteria
                    base2_dataframe2.loc[index] = [np.nan] * len(base2_dataframe2.columns)
        # Reset the index of the DataFrame after modification
        base2_dataframe2.reset_index(drop=True, inplace=True)
        print("Debug Removed unwanted rows and reset index.")
        print("Total, Total, PageOf lines removed or cleared")
        # ============================================================
        # Process Data for KUWAIT Specific Structure
        # ============================================================
        if country == "KUWAIT" and structure == "POSITION":
            # Create a list to hold new rows after processing
            new_rows = []
            for index, row in base2_dataframe2.iterrows():
                # Identify columns that contain '~' to be split
                columns_to_split = [col_base2 for col_base2 in base2_dataframe2.columns if "~" in str(row[col_base2])]
                # If no columns need splitting, append the row as-is
                if not columns_to_split:
                    new_rows.append(row)
                    continue
                # Find the maximum number of splits required for any column
                max_splits = max(len(str(row[col_base2]).split("~")) for col_base2 in columns_to_split)
                list_num_index_column = []
                for i in range(max_splits):
                    new_row = row.copy()
                    for col_base2 in base2_dataframe2.columns:
                        if col_base2 in columns_to_split:
                            split_values = str(row[col_base2]).split("~")
                            if i < len(split_values):
                                new_row[col_base2] = split_values[i]
                            else:
                                new_row[col_base2] = None
                        else:
                            str_value = str(row[col_base2])
                            if str_value.isdigit() and str(col_base2) != 0:
                                exists = any(
                                    item[index] == index and item[col_base2] == col_base2
                                    for item in list_num_index_column
                                )
                                if not exists:
                                    new_row[col_base2] = row[col_base2]
                                    list_num_index_column.append({"index": index, "col_base2": col_base2})
                                else:
                                    new_row[col_base2] = None
                            else:
                                new_row[col_base2] = row[col_base2]
                    new_rows.append(new_row)
            # Update DataFrame with the processed rows
            base2_dataframe2 = pd.DataFrame(new_rows)
            # Optional check for rows still containing '~'
            # contains_tilde = base2_dataframe2.apply(lambda x x.str.contains('~', na=False)).any().any()
            # print(DataFrame'de '~' sembolü içeren satır var mı, contains_tilde)
            print("Converted grouped rows for the ~ symbol for Kuwait into a list")
        # ============================================================
        # Convert Column Headings to Integer and Adjust Index
        # ============================================================
        # Convert column headings to integer
        base2_dataframe2.columns = base2_dataframe2.columns.astype(int)
        # Start column names at 1 instead of 0
        base2_dataframe2.columns = [column + 1 for column in base2_dataframe2.columns]
        # Reset index after modifications
        base2_dataframe2.reset_index(drop=True, inplace=True)
        # Save the DataFrame to CSV
        base2_dataframe2.to_csv(base2_dataframe2.csv, index=False)
        # Optional CSV export with different delimiter and encoding
        # base2_dataframe2.to_csv(base2_dataframe2.csv, sep=';', encoding='utf-8')
        print("base2_dataframe2 column names made Integer")
        # ============================================================
        # FIND MATCHING COLUMNS
        # ============================================================
        print("list_column_match codes started")
        # Initialize the list to store matching columns
        list_column_match = []
        # Initialize counters for mandatory columns
        use_count_second_column_name = 0
        # ============================================================
        # Check if 'second_column_name' is used and count mandatory columns
        # ============================================================
        # Count non-empty second_column_name and etl_column_name
        for row_attribute_detail in result_attribute_detail:
            (
                original_column_name,
                second_column_name,
                etl_column_name,
                column_position,
                starting_row,
                is_mandatory,
            ) = row_attribute_detail
            if second_column_name not in [NA, NULL, None] and etl_column_name not in [NA, NULL, None]:
                use_count_second_column_name += 1
        # Initialize column counters based on context and mandatory status
        column_count_sales_is_mandatory_1 = 0
        column_count_stock_is_mandatory_1 = 0
        # Count mandatory columns based on context and use of second_column_name
        for row_attribute_detail in result_attribute_detail:
            (
                original_column_name,
                second_column_name,
                etl_column_name,
                column_position,
                starting_row,
                is_mandatory,
            ) = row_attribute_detail
            if use_count_second_column_name == 0:
                if original_column_name not in [NA, NULL, None] or etl_column_name not in [NA, NULL, None]:
                    if context == SALES and is_mandatory == 1:
                        column_count_sales_is_mandatory_1 += 1
                    elif context == STOCK and is_mandatory == 1:
                        column_count_stock_is_mandatory_1 += 1
                    elif context == SALESSTOCK and is_mandatory == 1:
                        column_count_sales_is_mandatory_1 += 1
                        column_count_stock_is_mandatory_1 += 1
            else:
                if second_column_name not in [NA, NULL, None] or etl_column_name not in [NA, NULL, None]:
                    if context == SALES and is_mandatory == 1:
                        column_count_sales_is_mandatory_1 += 1
                    elif context == STOCK and is_mandatory == 1:
                        column_count_stock_is_mandatory_1 += 1
                    elif context == SALESSTOCK and is_mandatory == 1:
                        column_count_sales_is_mandatory_1 += 1
                        column_count_stock_is_mandatory_1 += 1
        # ============================================================
        # Populate 'list_column_match' with matching columns
        # ============================================================
        for row_attribute_detail in result_attribute_detail:
            (
                original_column_name,
                second_column_name,
                etl_column_name,
                column_position,
                starting_row,
                is_mandatory,
            ) = row_attribute_detail
            # Determine which column name to use for matching
            searched_excel_column_name = ""
            if use_count_second_column_name == 0:
                if original_column_name not in [NA, NULL, None]:
                    searched_excel_column_name = original_column_name
            else:
                if second_column_name not in [NA, NULL, None]:
                    searched_excel_column_name = second_column_name
            # Clean up the searched column name
            searched_excel_column_name = (
                lambda x: (
                    "NULL" if str(x).strip() in ["", "nan", "NaN", "nat", "NaT", "null", "NULL", None] else x.strip()
                )
                )(searched_excel_column_name)
            if structure == "TABULAR":
                # Search for a match in the DataFrame
                found_match = False
                for indexForRow, row2_base2_dataframe2 in base2_dataframe2.iterrows():
                    for ColumnPositionNo, column_value in row2_base2_dataframe2.items():
                        if str(searched_excel_column_name).strip() == str(column_value).strip():
                            # Check if this combination has been processed before
                            prev_count = len(
                                list(
                                    filter(
                                        lambda x: (indexForRow not in x or x[indexForRow] == indexForRow)
                                        and (
                                            ColumnPositionNo not in x
                                            or x[ColumnPositionNo] == int(ColumnPositionNo)
                                        ),
                                        list_column_match,
                                    )
                                )
                            )
                            if prev_count == 1:
                                print("This row has been processed before")
                            else:
                                # Add match to list
                                list_column_match.append(
                                    {
                                        "structure": structure,
                                        "context": context,
                                        "searched_excel_column_name": str(column_value),
                                        "etl_column_name": etl_column_name,
                                        "indexForRow": int(indexForRow),
                                        "ColumnPositionNo": int(ColumnPositionNo),
                                        "is_mandatory": is_mandatory,
                                    }
                                )
                                found_match = True
                                break
                    if found_match:
                        break
            elif structure in ("POSITION", "CUSTOM POSITION", "CUSTOMRAFED"):
                list_column_match.append(
                    {
                        "structure": structure,
                        "context": context,
                        "searched_excel_column_name": str(searched_excel_column_name),
                        "etl_column_name": etl_column_name,
                        "indexForRow": int(starting_row - 1),
                        "ColumnPositionNo": int(column_position),
                        "is_mandatory": is_mandatory,
                    }
                )
        print("The first phase for list_column_match is finished")
        # ============================================================
        # ADDING THE SALES_QUANTITY COLUMN TO THE LIST for RAFED
        # ============================================================
        if data_owner == "RAFED" and structure == "CUSTOMRAFED" and country == "UAE":
            # Check if 'SALES_QUANTITY' is already in the list_column_match
            print("Checking if 'SALES_QUANTITY' exists in list_column_match.")
            sales_quantity_exists = any(item["etl_column_name"] == "SALES_QUANTITY" for item in list_column_match)
            # If 'SALES_QUANTITY' does not exist, add it to the list
            if not sales_quantity_exists:
                # Find the index for the row where 'etl_column_name' is 'QUANTITY_AVAILABLE'
                print("Finding index for 'QUANTITY_AVAILABLE'.")
                rafed_indexForRow = next(
                    (
                        item[indexForRow]
                        for item in list_column_match
                        if item[etl_column_name] == QUANTITY_AVAILABLE
                    ),
                    0,
                )
                # Determine the column position number
                # Determine the column position number
                print("Determining column position number.")
                rafed_ColumnPositionNo = 0
                if base2_dataframe2 is not None and not base2_dataframe2.empty:
                    rafed_ColumnPositionNo = len(base2_dataframe2.columns) - 10
                    if rafed_ColumnPositionNo < 0:
                        print("Column position number is negative. Exiting.")
                        break
                # Calculate the month and year
                print("Calculating month and year.")
                rafed_year = mail_date.year
                rafed_month = mail_date.month - 1
                if rafed_month == 0:
                    rafed_month = 12
                    rafed_year -= 1
                find_date = rafed_year * 100 + rafed_month
                # Retrieve the specific row from base2_dataframe2
                print(f"Retrieving row {rafed_indexForRow} from base2_dataframe2.")
                row_Rafed = base2_dataframe2.iloc[rafed_indexForRow]
                # Search for the column that matches the calculated date
                print("Searching for matching column based on calculated date.")
                for col_rafed in row_Rafed.index:
                    value_row_rafed = row_Rafed[col_rafed]
                    try:
                        # Attempt to convert value to date
                        date_value = pd.to_datetime(value_row_rafed, errors="raise")
                        # Convert to YYYYMM integer format
                        int_value = date_value.year * 100 + date_value.month
                        # Check if the integer value matches the calculated find_date
                        if int_value == find_date:
                            rafed_ColumnPositionNo = int(col_rafed)
                            print(f"Matching column found: {rafed_ColumnPositionNo}.")
                            break
                    except ValueError:
                        # If conversion fails, continue to the next value
                        continue
                # Print the determined column position number
                print("rafed_ColumnPositionNo: ", rafed_ColumnPositionNo)
                # Update the DataFrame and append the new item to list_column_match
                print("Updating base2_dataframe2 and appending new item to list_column_match.")
                base2_dataframe2[rafed_ColumnPositionNo].iloc[rafed_indexForRow] = "SALES_QUANTITY"
                list_column_match.append(
                    {
                        "structure": structure,
                        "context": context,
                        "searched_excel_column_name": "SALES_QUANTITY",
                        "etl_column_name": "SALES_QUANTITY",
                        "indexForRow": rafed_indexForRow,
                        "ColumnPositionNo": rafed_ColumnPositionNo,
                        "is_mandatory": 0,
                    }
                )
                print("Item 'SALES_QUANTITY' successfully added to list_column_match.")
        # ============================================================
        # STRIP WHITESPACE FROM 'searched_excel_column_name'
        # ============================================================
        list_column_match = [
            {
                "item": item,
                "searched_excel_column_name": item["searched_excel_column_name"].strip(),
            }
            for item in list_column_match
        ]
        # ============================================================
        # RENAMING COLUMN NAMES IN 'list_column_match'
        # ============================================================
        # Change 'QUANTITY_AVAILABLE' to 'AVAILABLE_QUANTITY'
        for item in list_column_match:
            if item["etl_column_name"] and "QUANTITY_AVAILABLE" in item["etl_column_name"]:
                item["etl_column_name"] = "AVAILABLE_QUANTITY"
        # Change 'BATCH_NO' to 'BATCH_NUMBER'
        for item in list_column_match:
            if item["etl_column_name"] and "BATCH_NO" in item["etl_column_name"]:
                item["etl_column_name"] = "BATCH_NUMBER"
        # Change 'REGION' to 'BRANCH_NAME' for specific data owner and structure
        if data_owner == "SURGIPHARM" and structure == "POSITION":
            for item in list_column_match:
                if item["etl_column_name"] and "REGION" in item["etl_column_name"]:
                    item["etl_column_name"] = "BRANCH_NAME"
        # ============================================================
        # APPEND NUMBERS TO DUPLICATE 'searched_excel_column_name' VALUES
        # ============================================================
        # Create a list of 'searched_excel_column_name' values
        searched_names_list_column_match = [item["searched_excel_column_name"] for item in list_column_match]
        # Count occurrences of each name and append a number if duplicates exist
        name_count = {}
        for item in list_column_match:
            name = item["searched_excel_column_name"]
            if searched_names_list_column_match.count(name) > 1:
                if name not in name_count:
                    name_count[name] = 1
                    item["searched_excel_column_name"] = f"{name}1"
                else:
                    name_count[name] += 1
                    item["searched_excel_column_name"] = f"{name}{name_count[name]}"
        # ============================================================
        # COUNT MANDATORY ITEMS
        # ============================================================
        # Count items where 'is_mandatory' is 1
        column_Count_is_mandatory_1 = len(list(filter(lambda x: x["is_mandatory"] == 1, list_column_match)))
        # Count items with 'is_mandatory' 1 based on context
        column_count_matching_sales_is_mandatory_1 = len(
            list(
                filter(
                    lambda x: (x["context"] == "SALES" or x["context"] == "SALESSTOCK") and x["is_mandatory"] == 1,
                    list_column_match
                )
            )
        )
        column_count_matching_stock_is_mandatory_1 = len(
            list(
                filter(
                    lambda x: (x["context"] == "STOCK" or x["context"] == "SALESSTOCK") and x["is_mandatory"] == 1,
                    list_column_match
                )
            )
        )
        # View Column Match List
        print("List column match:", list_column_match)
        print("List column match final phase finished")
        # ============================================================
        # HANDLING HEADER ROW FOR QUIMICA SUIZA
        # ============================================================
        if country == "PERU" and data_owner == "QUIMICA SUIZA" and structure in ("POSITION", "CUSTOM POSITION"):
            print("Adding header row for 'QUIMICA SUIZA' data without header")
            # Count columns in list_column_match and base2_dataframe2
            count_list_column_match = len(list_column_match)
            count_columns_base2_dataframe2 = len(base2_dataframe2.columns)
            if count_list_column_match == count_columns_base2_dataframe2:
                # Create an empty row with the same columns as base2_dataframe2
                empty_row = {}
                for col_base2 in base2_dataframe2.columns:
                    empty_row[col_base2] = None
                # Add the empty row as the first row in the DataFrame
                base2_dataframe2 = pd.concat([pd.DataFrame([empty_row]), base2_dataframe2], ignore_index=True)
                # Fill in the header row with column names from list_column_match
                for item in list_column_match:
                    row_indexMatch = item[indexForRow]
                    ColumnPositionNoMatch = item[ColumnPositionNo]
                    searched_column_nameMatch = item[searched_excel_column_name]
                    base2_dataframe2.at[row_indexMatch, ColumnPositionNoMatch] = searched_column_nameMatch
            print("Header row added successfully")
        # ============================================================
        # CHECKING COLUMN HEADINGS FROM DD_ATTRIBUTE_DETAIL
        # ============================================================
        all_match = True
        if structure in ("POSITION", "CUSTOM POSITION", "CUSTOMRAFED"):
            print("Starting column heading validation")
            for item in list_column_match:
                row_indexMatch = item[indexForRow]
                ColumnPositionNoMatch = item[ColumnPositionNo]
                searched_column_nameMatch = item[searched_excel_column_name]
                is_mandatoryMatch = item[is_mandatory]
                if is_mandatoryMatch == 1:
                    # Check if the specified column and row indices exist in the DataFrame
                    if (ColumnPositionNoMatch not in base2_dataframe2.columns or row_indexMatch not in base2_dataframe2.index):
                        all_match = False
                    else:
                        # If there is a period tag at the beginning of the column, remove the period tag.
                        if (
                            searched_column_nameMatch is not None
                            and searched_column_nameMatch != ""
                            and str(searched_column_nameMatch).lower() not in [nan, nat]
                            and not (isinstance(searched_column_nameMatch, float) and np.isnan(searched_column_nameMatch))
                            and not (isinstance(searched_column_nameMatch, pd.Timestamp) and pd.isna(searched_column_nameMatch))
                        ):
                            # Remove spaces
                            # searched_column_nameMatch = searched_column_nameMatch.replace( , )
                            searched_column_nameMatch = ''.join(searched_column_nameMatch.split())
                            # Remove numeric values
                            searched_column_nameMatch = ''.join(char for char in searched_column_nameMatch if not char.isdigit())
                            # Remove specific characters
                            searched_column_nameMatch = searched_column_nameMatch.replace("_", "").replace("-", "")
                            # Assign NULL if the result is an empty string
                            if searched_column_nameMatch.strip() == "":
                                searched_column_nameMatch = SPECIAL_VALUES['NULL']
                        # Get and clean the column name from base2_dataframe2
                        base2_column_name = str(base2_dataframe2.loc[row_indexMatch, ColumnPositionNoMatch]).strip()
                        if pd.isna(base2_column_name) or base2_column_name.lower() in NULL_VALUES:
                            base2_column_name = SPECIAL_VALUES['NULL']
                        # If there is a period tag at the beginning of the column, remove the period tag.
                        if (
                            base2_column_name is not None
                            and base2_column_name != ""
                            and str(base2_column_name).lower() not in NULL_VALUES
                            and not (isinstance(base2_column_name, float) and np.isnan(base2_column_name))
                            and not (isinstance(base2_column_name, pd.Timestamp) and pd.isna(base2_column_name))
                        ):
                            # Remove spaces
                            base2_column_name = "".join(base2_column_name.split())
                            # Remove numeric values
                            base2_column_name = "".join(char for char in base2_column_name if not char.isdigit())
                            # Remove specific characters
                            base2_column_name = base2_column_name.replace("_", "").replace("-", "")
                            # Assign NULL if the result is an empty string
                            if base2_column_name.strip() == "":
                                base2_column_name = SPECIAL_VALUES['NULL']
                        # Special handling for SURGIPHARM in KENYA context
                        if (data_owner == DATA_OWNERS['SURGIPHARM'] and 
                            country == COUNTRIES['KENYA'] and 
                            context == CONTEXT_TYPES['SALES']):
                            if base2_column_name.startswith("MAINSTORES"):
                                base2_column_name = SPECIAL_VALUES['NULL']
                            if searched_column_nameMatch == "SALESSTOCK":
                                searched_column_nameMatch = SPECIAL_VALUES['NULL']
                        if searched_column_nameMatch != base2_column_name:
                            all_match = False
                            print(f"Mismatch found searched_column_nameMatch {searched_column_nameMatch}")
                            print(f"base2_column_name {base2_column_name}")
                            print(f"id: {id}")
                            break
            # Clear list_column_match if any mismatch is found
            if not all_match:
                list_column_match.clear()
                column_Count_is_mandatory_1 = 0
                column_count_stock_is_mandatory_1 = 0
                column_count_sales_is_mandatory_1 = 0
                column_count_matching_stock_is_mandatory_1 = 0
                column_count_matching_sales_is_mandatory_1 = 0
            print("Column heading validation completed")
        # ============================================================
        # REMOVE HEADER LINES IN ALLIANCE FILE IF THEY REAPPEAR
        # ============================================================
        if (data_owner == DATA_OWNERS['ALLIANCE'] and 
            structure == STRUCTURE_TYPES['POSITION'] and 
            all_match):
            print("Processing data for ALLIANCE with POSITION structure.")
            join_base2_first_column_name = ""
            start_index = 0
            # Find headings
            print("Finding headings in list_column_match.")
            for item in list_column_match:
                row_index = item["indexForRow"]
                start_index = row_index
                ColumnPositionNoMatch = item["ColumnPositionNo"]
                join_base2_first_column_name += str(base2_dataframe2.loc[row_index, ColumnPositionNoMatch])
            print(f"Headings concatenated for comparison: {join_base2_first_column_name}")
            join_base2_other_column_name = ""
            start_index += 1
            # Loop through the rows starting from start_index
            print(f"Looping through rows starting from index: {start_index}")
            for row_index in range(start_index, len(base2_dataframe2)):
                if row_index == start_index:
                    join_base2_other_column_name = ""
                    if row_index == 24:
                        # Temporary debug variable (to be removed or used for debugging)
                        asdfasdfa = 1
                    # Concatenate column values to create the comparison string
                    for item in list_column_match:
                        ColumnPositionNoMatch = item["ColumnPositionNo"]
                        join_base2_other_column_name += str(base2_dataframe2.loc[row_index, ColumnPositionNoMatch])
                    # Compare the concatenated values
                    if join_base2_first_column_name == join_base2_other_column_name:
                        print(f"Row {row_index} matches with heading. Removing this row.")
                        # Remove headings
                        base2_dataframe2 = base2_dataframe2.drop(row_index)
            # Reset the index of the DataFrame
            print("Resetting index of base2_dataframe2.")
            base2_dataframe2.reset_index(drop=True, inplace=True)
            print("Processing completed. DataFrame updated.")
        # ============================================================
        # PROCESS COLUMN FILLING FOR SURGIPHARM
        # ============================================================
        if data_owner == SURGIPHARM and structure == POSITION and len(list_column_match) > 0:
            print("Processing data for SURGIPHARM with POSITION structure.")
            # ============================================================
            # FILL 'BRANCH_NAME' COLUMN
            # ============================================================
            print("Filling REGION-BRANCH_NAME Column.")
            ColumnPositionNo_BRANCH_NAME = next(
                (item["ColumnPositionNo"] for item in list_column_match 
                 if item["etl_column_name"] == COLUMN_NAMES['BRANCH_NAME']),
                None
            )
            value_BRANCH_NAME = ""
            if str(ColumnPositionNo_BRANCH_NAME).strip().lower() not in NULL_VALUES:
                for index in range(len(base2_dataframe2) - 1):
                    line_value = str(base2_dataframe2[ColumnPositionNo_BRANCH_NAME].iloc[index])
                    if line_value.startswith("MAIN STORES - "):
                        value_BRANCH_NAME = line_value.replace("MAIN STORES - ", "")
                        print(f"Found branch name {value_BRANCH_NAME}")
                    elif value_BRANCH_NAME:
                        base2_dataframe2[ColumnPositionNo_BRANCH_NAME].iloc[index] = f"SURGIPHARM {value_BRANCH_NAME}"
                        print(f"Updated branch name at row {index}: {base2_dataframe2[ColumnPositionNo_BRANCH_NAME].iloc[index]}")
            # ============================================================
            # FILL PRODUCT_NAME Column
            # ============================================================
            print("Filling PRODUCT_NAME Column.")
            ColumnPositionNo_PRODUCT_NAME = next(
                (item["ColumnPositionNo"] for item in list_column_match 
                 if item["etl_column_name"] == COLUMN_NAMES['PRODUCT_NAME']),
                None
            )
            value_PRODUCT_NAME = ""
            if str(ColumnPositionNo_PRODUCT_NAME).strip().lower() not in NULL_VALUES:
                for index in range(len(base2_dataframe2) - 1):
                    line_value = str(base2_dataframe2[ColumnPositionNo_PRODUCT_NAME].iloc[index])
                    if line_value.lower() not in NULL_VALUES:
                        value_PRODUCT_NAME = line_value
                        print(f"Found product name {value_PRODUCT_NAME}")
                    if value_PRODUCT_NAME:
                        base2_dataframe2[ColumnPositionNo_PRODUCT_NAME].iloc[index] = value_PRODUCT_NAME
                        print(f"Updated product name at row {index}: {base2_dataframe2[ColumnPositionNo_PRODUCT_NAME].iloc[index]}")
            # ============================================================
            # FILL DATA_DATE Column
            # ============================================================
            print("Filling DATA_DATE Column.")
            ColumnPositionNo_DATA_DATE = next(
                (item["ColumnPositionNo"] for item in list_column_match if item["etl_column_name"] == "DATA_DATE"),
                None,
            )
            value_DATA_DATE = ""
            if str(ColumnPositionNo_DATA_DATE).strip().lower() not in ("", "nan", "nat", "none"):
                for index in range(len(base2_dataframe2) - 1):
                    line_value = str(base2_dataframe2[ColumnPositionNo_DATA_DATE].iloc[index])
                    if line_value.lower() not in ("nan", "nat", ""):
                        value_DATA_DATE = line_value
                        print(f"Found data date: {value_DATA_DATE}")
                    if value_DATA_DATE:
                        base2_dataframe2[ColumnPositionNo_DATA_DATE].iloc[index] = value_DATA_DATE
                        print(f"Updated data date at row {index}: {base2_dataframe2[ColumnPositionNo_DATA_DATE].iloc[index]}")
        # ============================================================
        # PROCESS FOR "QUIMICA SUIZA"
        # ============================================================
        if country == "PERU" and data_owner == "QUIMICA SUIZA" and structure in ("POSITION", "CUSTOM POSITION"):
            # Filter rows with specific BRANCH_CODE values
            ColumnPositionNo_BRANCH_CODE = next(
                (item["searched_excel_column_name"] for item in list_column_match if item["etl_column_name"] == "BRANCH_CODE"),
                None
            )
            if ColumnPositionNo_BRANCH_CODE is not None and ColumnPositionNo_BRANCH_CODE in base3_dataframe3.columns:
                filtered_dataframe = base3_dataframe3[
                    base3_dataframe3[ColumnPositionNo_BRANCH_CODE].isin(['3000', '0001'])
                ]
                base3_dataframe3 = filtered_dataframe.copy()
            # Add 'BRANCH_NAME' column if it does not exist and set values based on BRANCH_CODE
            if "BRANCH_NAME" not in base3_dataframe3.columns:
                base3_dataframe3["BRANCH_NAME"] = None
            if ColumnPositionNo_BRANCH_CODE is not None and ColumnPositionNo_BRANCH_CODE in base3_dataframe3.columns:
                base3_dataframe3["BRANCH_NAME"] = base3_dataframe3[ColumnPositionNo_BRANCH_CODE].apply(
                    lambda x: "QUIMICA SUIZA PUBLIC" if x == "3000" else ("QUIMICA SUIZA PRIVATE" if x == "0001" else x)
                )
                # Add BRANCH_NAME to list_column_match if not already present
                branch_name_exists = any(item["etl_column_name"] == "BRANCH_NAME" for item in list_column_match)
                if not branch_name_exists:
                    list_column_match.append(
                        {
                            "structure": structure,
                            "context": context,
                            "searched_excel_column_name": "BRANCH_NAME",
                            "etl_column_name": "BRANCH_NAME",
                            "indexForRow": next(
                                (item["indexForRow"] for item in list_column_match if item["etl_column_name"] == "BRANCH_CODE"),
                                0
                            ),
                            "ColumnPositionNo": "BRANCH_NAME",
                            "is_mandatory": 0
                        }
                    )
            base3_dataframe3.reset_index(drop=True, inplace=True)
        # ================================
        # SECTION: QUIMICA SUIZA STOCK CONTEXT
        # ================================
        if (
            country == "PERU"
            and data_owner == "QUIMICA SUIZA"
            and structure in ("POSITION", "CUSTOM POSITION")
            and context == "STOCK"
        ):
            print("Processing QUIMICA SUIZA STOCK context...")
            # Fill NULL values in 'BATCH_NUMBER' column with 'N/A'
            ColumnPositionNo_BATCH_NUMBER = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "BATCH_NUMBER"
                ),
                None,
            )
            if ColumnPositionNo_BATCH_NUMBER is not None and ColumnPositionNo_BATCH_NUMBER in base3_dataframe3.columns:
                print(f"Filling NULL values in '{ColumnPositionNo_BATCH_NUMBER}' with 'N/A'.")
                base3_dataframe3[ColumnPositionNo_BATCH_NUMBER] = base3_dataframe3[
                    ColumnPositionNo_BATCH_NUMBER
                ].str.strip()
                base3_dataframe3[ColumnPositionNo_BATCH_NUMBER].replace(["", None, np.nan, "null"], "N/A", inplace=True)
                print("NULL values filled successfully.")
            # Check if 'INVENTORY_CATEGORY' column exists; if not, add it
            if "INVENTORY_CATEGORY" not in base3_dataframe3.columns:
                print("Adding 'INVENTORY_CATEGORY' column with default value 'GN'.")
                base3_dataframe3["INVENTORY_CATEGORY"] = "GN"
            if "INVENTORY_CATEGORY" in base3_dataframe3.columns and "BRANCH_NAME" in base3_dataframe3.columns:
                print("Updating 'INVENTORY_CATEGORY' based on 'BRANCH_NAME'.")
                base3_dataframe3["INVENTORY_CATEGORY"] = base3_dataframe3["BRANCH_NAME"].apply(
                    lambda x: (
                        "PR" if x == "QUIMICA SUIZA PRIVATE" else ("PU" if x == "QUIMICA SUIZA PUBLIC" else "GN")
                    )
                )
                print("'INVENTORY_CATEGORY' updated successfully.")
                # Check and append 'INVENTORY_CATEGORY' to the list if not present
                inventory_category_exists = any(
                    item["etl_column_name"] == "INVENTORY_CATEGORY" for item in list_column_match
                )
                if not inventory_category_exists:
                    print("Appending 'INVENTORY_CATEGORY' to list_column_match.")
                    list_column_match.append(
                        {
                            "structure": structure,
                            "context": context,
                            "searched_excel_column_name": "INVENTORY_CATEGORY",
                            "etl_column_name": "INVENTORY_CATEGORY",
                            "indexForRow": next(
                                (
                                    item["indexForRow"]
                                    for item in list_column_match
                                    if item["etl_column_name"] == "BRANCH_NAME"
                                ),
                                0,
                            ),
                            "ColumnPositionNo": "INVENTORY_CATEGORY",
                            "is_mandatory": 0,
                        }
                    )
                print("'INVENTORY_CATEGORY' successfully appended to list_column_match.")
            # Process and calculate 'BLOCKED_QUANTITY'
            ColumnPositionNo_QUANTITY_IN_TRANSIT1 = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "QUANTITY_IN_TRANSIT1"
                ),
                None,
            )
            ColumnPositionNo_QUANTITY_IN_TRANSIT2 = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "QUANTITY_IN_TRANSIT2"
                ),
                None,
            )
            if "BLOCKED_QUANTITY" not in base3_dataframe3.columns:
                print("Adding 'BLOCKED_QUANTITY' column with default value 0.")
                base3_dataframe3["BLOCKED_QUANTITY"] = 0
            # Ensure 'QUANTITY_IN_TRANSIT1' and 'QUANTITY_IN_TRANSIT2' are numeric
            if (
                ColumnPositionNo_QUANTITY_IN_TRANSIT1
                and ColumnPositionNo_QUANTITY_IN_TRANSIT1 in base3_dataframe3.columns
            ):
                print("Converting 'QUANTITY_IN_TRANSIT1' to numeric.")
                base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT1] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT1],
                    errors="coerce",
                )
            else:
                base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT1] = 0
            if (
                ColumnPositionNo_QUANTITY_IN_TRANSIT2
                and ColumnPositionNo_QUANTITY_IN_TRANSIT2 in base3_dataframe3.columns
            ):
                print("Converting 'QUANTITY_IN_TRANSIT2' to numeric.")
                base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT2] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT2],
                    errors="coerce",
                )
            else:
                base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT2] = 0
            print("Calculating 'BLOCKED_QUANTITY'.")
            base3_dataframe3["BLOCKED_QUANTITY"] = base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT1].fillna(
                0
            ) + base3_dataframe3[ColumnPositionNo_QUANTITY_IN_TRANSIT2].fillna(0)
            base3_dataframe3["BLOCKED_QUANTITY"] = base3_dataframe3["BLOCKED_QUANTITY"] / 1000
            print("'BLOCKED_QUANTITY' calculation complete.")
            # Check and append 'BLOCKED_QUANTITY' to the list if not present
            blocked_quantity_exists = any(item["etl_column_name"] == "BLOCKED_QUANTITY" for item in list_column_match)
            if not blocked_quantity_exists:
                print("Appending 'BLOCKED_QUANTITY' to list_column_match.")
                list_column_match.append(
                    {
                        "structure": structure,
                        "context": context,
                        "searched_excel_column_name": "BLOCKED_QUANTITY",
                        "etl_column_name": "BLOCKED_QUANTITY",
                        "indexForRow": 0,
                        "ColumnPositionNo": "BLOCKED_QUANTITY",
                        "is_mandatory": 0,
                    }
                )
            print("'BLOCKED_QUANTITY' successfully appended to list_column_match.")
            # Validate 'AVAILABLE_QUANTITY' column and process data
            ColumnPositionNo_AVAILABLE_QUANTITY = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "AVAILABLE_QUANTITY"
                ),
                None,
            )
            if (
                ColumnPositionNo_AVAILABLE_QUANTITY is not None
                and ColumnPositionNo_AVAILABLE_QUANTITY in base3_dataframe3.columns
            ):
                print("Converting 'AVAILABLE_QUANTITY' to numeric and scaling.")
                base3_dataframe3[ColumnPositionNo_AVAILABLE_QUANTITY] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_AVAILABLE_QUANTITY],
                    errors="coerce",
                )
                base3_dataframe3 = base3_dataframe3.dropna(subset=[ColumnPositionNo_AVAILABLE_QUANTITY])
                base3_dataframe3 = base3_dataframe3[base3_dataframe3[ColumnPositionNo_AVAILABLE_QUANTITY] >= 0]
                print("'AVAILABLE_QUANTITY' processed and cleaned.")
        # ================================
        # SECTION: QUIMICA SUIZA SALES CONTEXT
        # ================================
        if (
            country == "PERU"
            and data_owner == "QUIMICA SUIZA"
            and structure in ("POSITION", "CUSTOM POSITION")
            and context == "SALES"
        ):
            print("Processing QUIMICA SUIZA SALES context...")
            # Check if 'SALES_CATEGORY' column exists; if not, add it
            if "SALES_CATEGORY" not in base3_dataframe3.columns:
                print("Adding 'SALES_CATEGORY' column with default value 'GN'.")
                base3_dataframe3["SALES_CATEGORY"] = "GN"
            if "SALES_CATEGORY" in base3_dataframe3.columns and "BRANCH_NAME" in base3_dataframe3.columns:
                print("Updating 'SALES_CATEGORY' based on 'BRANCH_NAME'.")
                base3_dataframe3["SALES_CATEGORY"] = base3_dataframe3["BRANCH_NAME"].apply(
                    lambda x: (
                        "PR" if x == "QUIMICA SUIZA PRIVATE" else ("PU" if x == "QUIMICA SUIZA PUBLIC" else "GN")
                    )
                )
                print("'SALES_CATEGORY' updated successfully.")
                # Check and append 'SALES_CATEGORY' to the list if not present
                inventory_category_exists = any(
                    item["etl_column_name"] == "SALES_CATEGORY" for item in list_column_match
                )
                if not inventory_category_exists:
                    print("Appending 'SALES_CATEGORY' to list_column_match.")
                    list_column_match.append(
                        {
                            "structure": structure,
                            "context": context,
                            "searched_excel_column_name": "SALES_CATEGORY",
                            "etl_column_name": "SALES_CATEGORY",
                            "indexForRow": next(
                                (
                                    item["indexForRow"]
                                    for item in list_column_match
                                    if item["etl_column_name"] == "BRANCH_NAME"
                                ),
                                0,
                            ),
                            "ColumnPositionNo": "SALES_CATEGORY",
                            "is_mandatory": 0,
                        }
                    )
                print("'SALES_CATEGORY' successfully appended to list_column_match.")
            # Process and scale 'SALES_QUANTITY'
            ColumnPositionNo_SALES_QUANTITY = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "SALES_QUANTITY"
                ),
                None,
            )
            if (
                ColumnPositionNo_SALES_QUANTITY is not None
                and ColumnPositionNo_SALES_QUANTITY in base3_dataframe3.columns
            ):
                print("Converting 'SALES_QUANTITY' to numeric and scaling.")
                base3_dataframe3[ColumnPositionNo_SALES_QUANTITY] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_SALES_QUANTITY], errors="coerce"
                )
                print("SALES_QUANTITY after conversion and scaling:")
                print(base3_dataframe3)
                base3_dataframe3.loc[
                    base3_dataframe3[ColumnPositionNo_SALES_QUANTITY].notna(),
                    ColumnPositionNo_SALES_QUANTITY,
                ] = (
                    base3_dataframe3[ColumnPositionNo_SALES_QUANTITY] / 1000
                )
                print("SALES_QUANTITY after scaling:")
                print(base3_dataframe3)
            # Process and scale 'RETURN_QUANTITY'
            ColumnPositionNo_RETURN_QUANTITY = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "RETURN_QUANTITY"
                ),
                None,
            )
            if (
                ColumnPositionNo_RETURN_QUANTITY is not None
                and ColumnPositionNo_RETURN_QUANTITY in base3_dataframe3.columns
            ):
                print("Converting 'RETURN_QUANTITY' to numeric and scaling.")
                base3_dataframe3[ColumnPositionNo_RETURN_QUANTITY] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_RETURN_QUANTITY], errors="coerce"
                )
                print("RETURN_QUANTITY after conversion and scaling:")
                print(base3_dataframe3)
                base3_dataframe3.loc[
                    base3_dataframe3[ColumnPositionNo_RETURN_QUANTITY].notna(),
                    ColumnPositionNo_RETURN_QUANTITY,
                ] = (
                    base3_dataframe3[ColumnPositionNo_RETURN_QUANTITY] / 1000
                )
                print("RETURN_QUANTITY after scaling:")
                print(base3_dataframe3)
            # Process and scale 'SALES_VALUE'
            ColumnPositionNo_SALES_VALUE = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "SALES_VALUE"
                ),
                None,
            )
            if ColumnPositionNo_SALES_VALUE is not None and ColumnPositionNo_SALES_VALUE in base3_dataframe3.columns:
                print("Converting 'SALES_VALUE' to numeric and scaling.")
                base3_dataframe3[ColumnPositionNo_SALES_VALUE] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_SALES_VALUE], errors="coerce"
                )
                print("SALES_VALUE after conversion and scaling:")
                print(base3_dataframe3)
                base3_dataframe3.loc[
                    base3_dataframe3[ColumnPositionNo_SALES_VALUE].notna(),
                    ColumnPositionNo_SALES_VALUE,
                ] = (
                    base3_dataframe3[ColumnPositionNo_SALES_VALUE] / 1000
                )
                print("SALES_VALUE after scaling:")
                print(base3_dataframe3)
            # Process and scale 'RETURN_VALUE'
            ColumnPositionNo_RETURN_VALUE = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "RETURN_VALUE"
                ),
                None,
            )
            if ColumnPositionNo_RETURN_VALUE is not None and ColumnPositionNo_RETURN_VALUE in base3_dataframe3.columns:
                print("Converting 'RETURN_VALUE' to numeric and scaling.")
                base3_dataframe3[ColumnPositionNo_RETURN_VALUE] = pd.to_numeric(
                    base3_dataframe3[ColumnPositionNo_RETURN_VALUE], errors="coerce"
                )
                print("RETURN_VALUE after conversion and scaling:")
                print(base3_dataframe3)
                base3_dataframe3.loc[
                    base3_dataframe3[ColumnPositionNo_RETURN_VALUE].notna(),
                    ColumnPositionNo_RETURN_VALUE,
                ] = (
                    base3_dataframe3[ColumnPositionNo_RETURN_VALUE] / 1000
                )
                print("RETURN_VALUE after scaling:")
                print(base3_dataframe3)
        base3_dataframe3.reset_index(drop=True, inplace=True)
        if (
            country == "PERU"
            and data_owner == "QUIMICA SUIZA"
            and structure in ("POSITION", "CUSTOM POSITION")
            and context == "SALES"
            and len(base3_dataframe3) > 0
        ):
            print("Starting product join process for 'QUIMICA SUIZA' sales...")
            # Define SQL query for product lookup
            query_LKP_PERU_PRODUCT = (
                "SELECT CAST(CAST(product_id AS INTEGER) AS TEXT) AS lkp_product_id, "
                "product_name AS lkp_product_name FROM " + schemaname + ".lkp_peru_product"
            )
            print("SQL Query for product lookup:", query_LKP_PERU_PRODUCT)
            # Read data from JDBC source
            source_df = (
                sparkSession.read.format("jdbc")
                .option("url", jdbc_url)
                .option("query", query_LKP_PERU_PRODUCT)
                .option("driver", "org.postgresql.Driver")
                .option("user", username)
                .option("password", password)
                .load()
            )
            # Convert Spark DataFrame to DynamicFrame
            dynamic_frame_lkp_product = DynamicFrame.fromDF(source_df, glueContext, "dynamic_df")
            print("DynamicFrame created for product lookup.")
            # Select required columns
            dynamic_frame_lkp_product = dynamic_frame_lkp_product.select_fields(["lkp_product_id", "lkp_product_name"])
            # Convert DynamicFrame to Pandas DataFrame
            pandas_df_lkp_product = dynamic_frame_lkp_product.toDF().toPandas()
            print("Converted DynamicFrame to Pandas DataFrame for product lookup.")
            # Strip leading zeros from PRODUCT_ID in base3_dataframe3
            searched_excel_column_name_PRODUCT_ID = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "PRODUCT_ID"
                ),
                None,
            )
            base3_dataframe3[searched_excel_column_name_PRODUCT_ID] = base3_dataframe3[
                searched_excel_column_name_PRODUCT_ID
            ].str.lstrip("0")
            print("Stripped leading zeros from 'PRODUCT_ID' in base3_dataframe3.")
            # Merge DataFrames
            merged_dataframe3 = pd.merge(
                base3_dataframe3,
                pandas_df_lkp_product,
                left_on=searched_excel_column_name_PRODUCT_ID,
                right_on="lkp_product_id",
                how="inner",
            )
            print("DataFrames merged on 'PRODUCT_ID'.")
            # Clean up merged DataFrame
            if "lkp_product_id" in merged_dataframe3.columns:
                merged_dataframe3 = merged_dataframe3.drop(columns=["lkp_product_id"])
                print("Dropped 'lkp_product_id' column from merged DataFrame.")
            if "PRODUCT_NAME" in merged_dataframe3.columns:
                merged_dataframe3 = merged_dataframe3.drop(columns=["PRODUCT_NAME"])
                print("Dropped 'PRODUCT_NAME' column from merged DataFrame.")
            if "lkp_product_name" in merged_dataframe3.columns:
                merged_dataframe3.rename(columns={"lkp_product_name": "PRODUCT_NAME"}, inplace=True)
                print("Renamed 'lkp_product_name' to 'PRODUCT_NAME' in merged DataFrame.")
            # Update base3_dataframe3
            base3_dataframe3 = merged_dataframe3
            # Check if 'PRODUCT_NAME' column exists in list_column_match
            product_name_exists = any(item["etl_column_name"] == "PRODUCT_NAME" for item in list_column_match)
            if not product_name_exists:
                print("Appending 'PRODUCT_NAME' to list_column_match.")
                list_column_match.append(
                    {
                        "structure": structure,
                        "context": context,
                        "searched_excel_column_name": "PRODUCT_NAME",
                        "etl_column_name": "PRODUCT_NAME",
                        "indexForRow": next(
                            (
                                item["indexForRow"]
                                for item in list_column_match
                                if item["etl_column_name"] == "PRODUCT_ID"
                            ),
                            0,
                        ),
                        "ColumnPositionNo": "PRODUCT_NAME",
                        "is_mandatory": 0,
                    }
                )
            print("'PRODUCT_NAME' successfully appended to list_column_match.")
        base3_dataframe3.reset_index(drop=True, inplace=True)
        print("Reset index for base3_dataframe3 after product join.")
        # ================================
        # SECTION: CUSTOMER JOIN PROCESS OF SALES OF "QUIMICA SUIZA"
        # ================================
        if (
            country == "PERU"
            and data_owner == "QUIMICA SUIZA"
            and structure in ("POSITION", "CUSTOM POSITION")
            and context == "SALES"
            and len(base3_dataframe3) > 0
        ):
            print("Starting customer join process for 'QUIMICA SUIZA' sales...")
            # Define SQL query for maximum client ID
            query_max_id_for_client = (
                "SELECT MAX(id) AS max_id FROM " + schemaname + ".daq_log_info WHERE LOWER(file) LIKE '%client%'"
            )
            print("SQL Query for maximum client ID:", query_max_id_for_client)
            # Read data from JDBC source
            source_df = (
                sparkSession.read.format("jdbc")
                .option("url", jdbc_url)
                .option("query", query_max_id_for_client)
                .option("driver", "org.postgresql.Driver")
                .option("user", username)
                .option("password", password)
                .load()
            )
            # Convert Spark DataFrame to DynamicFrame
            dynamic_dframe = DynamicFrame.fromDF(source_df, glueContext, "dynamic_df")
            print("DynamicFrame created for client ID query.")
            # Extract max ID from DynamicFrame
            result_daq_log_info = dynamic_dframe.select_fields(["max_id"]).toDF().collect()
            max_id = result_daq_log_info[0]["max_id"] if result_daq_log_info else None
            print("Max client ID obtained:", max_id)
            # s3_path_max = "s3://roche-ereteam/IMIP_Excel_file_to_DB/" + str(max_id) + ".xlsx"
            s3_path_max = s3_main_path + str(max_id) + ".xlsx"
            print(f"Debug: Reading file from S3 path max: {s3_path_max}")
            # Read Excel data from S3
            s3 = s3fs.S3FileSystem()
            with s3.open(
                s3_path_max,
                "rb",
            ) as file:
                excel_data_customer = pd.read_excel(
                    file,
                    sheet_name="Sheet1",
                    dtype=str,
                    keep_default_na=False,
                    engine="openpyxl",
                    header=None,
                )
            print("Excel data for customer read from S3.")
            # Convert Pandas DataFrame to Spark DataFrame
            veri_Excel_Customer_base = sparkSession.createDataFrame(excel_data_customer)
            veri_Excel_Customer_DF = veri_Excel_Customer_base.toPandas()
            print("Converted Excel data to Pandas DataFrame for customer.")
            # Define column specifications for parsing
            colspecs = [
                (0, 19),
                (19, 27),
                (27, 33),
                (33, 73),
                (73, 93),
                (93, 105),
                (105, 165),
                (165, 175),
                (175, 185),
                (185, 225),
                (225, 265),
                (265, 305),
                (305, 325),
                (325, 333),
                (333, 341),
                (341, 345),
                (345, 347),
                (347, 349),
                (349, 357),
                (357, 359),
                (359, 363),
            ]
            # Create regex pattern for column splitting
            regex_pattern = "".join([f"(.{{{end - start}}})" for start, end in colspecs])
            print("Regex pattern for column splitting:", regex_pattern)
            # Define function to split columns using regex
            def split_columns_using_regex(row):
                match = re.match(regex_pattern, row[0])
                if match:
                    return match.groups()
                return [None] * len(colspecs)
            # Apply regex function to split columns
            split_data = veri_Excel_Customer_DF.apply(split_columns_using_regex, axis=1)
            veri_Excel_Customer_DF = pd.DataFrame(split_data.tolist())
            print("Split columns in customer DataFrame.")
            # Rename columns
            veri_Excel_Customer_DF.columns = veri_Excel_Customer_DF.columns.astype(str)
            if "1" in veri_Excel_Customer_DF.columns:
                veri_Excel_Customer_DF.rename(columns={"1": "lkp_customer_id"}, inplace=True)
            if "3" in veri_Excel_Customer_DF.columns:
                veri_Excel_Customer_DF.rename(columns={"3": "lkp_customer_name"}, inplace=True)
            # Get searched column name for CUSTOMER_ID
            searched_excel_column_name_CUSTOMER_ID = next(
                (
                    item["searched_excel_column_name"]
                    for item in list_column_match
                    if item["etl_column_name"] == "CUSTOMER_ID"
                ),
                None,
            )
            print("Searched column name for CUSTOMER_ID:", searched_excel_column_name_CUSTOMER_ID)
            # Strip leading zeros from CUSTOMER_ID in customer DataFrame
            veri_Excel_Customer_DF["lkp_customer_id"] = veri_Excel_Customer_DF["lkp_customer_id"].str.lstrip("0")
            print("Stripped leading zeros from 'lkp_customer_id' in customer DataFrame.")
            # Update CUSTOMER_ID in base3_dataframe3
            base3_dataframe3[searched_excel_column_name_CUSTOMER_ID] = base3_dataframe3[
                searched_excel_column_name_CUSTOMER_ID
            ].str.lstrip("0")
            # Merge DataFrames
            merged_dataframe3 = pd.merge(
                base3_dataframe3,
                veri_Excel_Customer_DF,
                left_on=searched_excel_column_name_CUSTOMER_ID,
                right_on="lkp_customer_id",
                how="inner",
            )
            print("DataFrames merged on 'CUSTOMER_ID'.")
            # Clean up merged DataFrame
            if "lkp_customer_id" in merged_dataframe3.columns:
                merged_dataframe3 = merged_dataframe3.drop(columns=["lkp_customer_id"])
                print("Dropped 'lkp_customer_id' column from merged DataFrame.")
            if "CUSTOMER_NAME" in merged_dataframe3.columns:
                merged_dataframe3 = merged_dataframe3.drop(columns=["CUSTOMER_NAME"])
                print("Dropped 'CUSTOMER_NAME' column from merged DataFrame.")
            if "lkp_customer_name" in merged_dataframe3.columns:
                merged_dataframe3.rename(columns={"lkp_customer_name": "CUSTOMER_NAME"}, inplace=True)
                print("Renamed 'lkp_customer_name' to 'CUSTOMER_NAME' in merged DataFrame.")
            # Update base3_dataframe3
            base3_dataframe3 = merged_dataframe3
            # Check if 'CUSTOMER_NAME' column exists in list_column_match
            customer_name_exists = any(item["etl_column_name"] == "CUSTOMER_NAME" for item in list_column_match)
            if not customer_name_exists:
                print("Appending 'CUSTOMER_NAME' to list_column_match.")
                list_column_match.append(
                    {
                        "structure": structure,
                        "context": context,
                        "searched_excel_column_name": "CUSTOMER_NAME",
                        "etl_column_name": "CUSTOMER_NAME",
                        "indexForRow": next(
                            (
                                item["indexForRow"]
                                for item in list_column_match
                                if item["etl_column_name"] == "CUSTOMER_ID"
                            ),
                            0,
                        ),
                        "ColumnPositionNo": "CUSTOMER_NAME",
                        "is_mandatory": 0,
                    }
                )
            print("'CUSTOMER_NAME' successfully appended to list_column_match.")
            base3_dataframe3.reset_index(drop=True, inplace=True)
            print("Reset index for base3_dataframe3 after customer join.")
            print("Final DataFrame after customer join:")
            print(base3_dataframe3)
        # ================================
        # SECTION STOCK AND SALESSTOCK CONTEXT PROCESSING
        # ================================
        if (
            context in [STOCK, SALESSTOCK]
            and len(base3_dataframe3)  0
            and (column_count_stock_is_mandatory_1 == column_count_matching_stock_is_mandatory_1)
            and (column_count_matching_stock_is_mandatory_1 != 0)
        )
            stock_list_column_match = list_column_match
            print(Starting stock processing...)
            # ================================
            # INTERMEDIATE SECTION CITYPHARMACY STOCK CONTEXT
            # ================================
            if data_owner == CITYPHARMACY and context == STOCK
                print(Processing CITYPHARMACY stock data...)
                # Find 'searched_excel_column_name' for 'STOCK_CATEGORY'
                searched_excel_column_name_STOCK_CATEGORY = next(
                    (
                        item[searched_excel_column_name]
                        for item in stock_list_column_match
                        if item[etl_column_name] == STOCK_CATEGORY
                    ),
                    None,
                )
                print(Searched column name for STOCK_CATEGORY, searched_excel_column_name_STOCK_CATEGORY)
                # Filter out rows where STOCK_CATEGORY is 'I'
                base3_dataframe3 = base3_dataframe3[base3_dataframe3[searched_excel_column_name_STOCK_CATEGORY] != I]
                print(Filtered out rows where STOCK_CATEGORY is 'I'.)
                # Reset index to start from zero
                base3_dataframe3 = base3_dataframe3.reset_index(drop=True)
                print(Index reset for base3_dataframe3.)
            # ================================
            # INTERMEDIATE SECTION REMOVE EMPTY ROWS IN AVAILABLE_QUANTITY
            # ================================
            # Find 'searched_excel_column_name' for 'AVAILABLE_QUANTITY'
            searched_excel_column_name_AVAILABLE_QUANTITY = next(
                (
                    item[searched_excel_column_name]
                    for item in stock_list_column_match
                    if item[etl_column_name] == AVAILABLE_QUANTITY
                ),
                None,
            )
            print(Searched column name for AVAILABLE_QUANTITY, searched_excel_column_name_AVAILABLE_QUANTITY)
            if not searched_excel_column_name_AVAILABLE_QUANTITY
                raise ValueError(Column 'AVAILABLE_QUANTITY' not found.)
            # Remove rows with NaN, NaT, None, null, or empty string values
            valid_entries = base3_dataframe3[
                base3_dataframe3[searched_excel_column_name_AVAILABLE_QUANTITY].apply(
                    lambda x not (pd.isna(x) or str(x).strip().lower() in [nan, nat, none, null, ])
                )
            ]
            print(Removed empty rows in AVAILABLE_QUANTITY column.)
            # Reset index
            valid_entries.reset_index(drop=True, inplace=True)
            stock_base3_dataframe3 = valid_entries.copy()
            print(Index reset and data copied to stock_base3_dataframe3.)
            # ================================
            # INTERMEDIATE SECTION TRANSFER ETL COLUMN NAMES TO 'matched_columns'
            # ================================
            matched_columns = [
                item[etl_column_name] for item in stock_list_column_match if item[etl_column_name] is not None
            ]
            print(Matched columns, matched_columns)
            # ================================
            # INTERMEDIATE SECTION CHECK AND ASSIGN VALUES FOR RESULT DATAFRAME
            # ================================
            for column_name in df_Result_Stock.columns
                if column_name in matched_columns
                    find_searched_excel_column_name = None
                    find_etl_column_name = None
                    for item in stock_list_column_match
                        if item[etl_column_name] == column_name
                            find_searched_excel_column_name = item[searched_excel_column_name]
                            if column_name == AVAILABLE_QUANTITY or column_name == BLOCKED_QUANTITY
                                stock_base3_dataframe3[find_searched_excel_column_name].fillna(
                                    0, inplace=True
                                )
                                numeric_values = pd.to_numeric(
                                    stock_base3_dataframe3[find_searched_excel_column_name],
                                    errors=coerce,
                                )
                                df_Result_Stock[column_name] = numeric_values.fillna(0).astype(int)
                            elif column_name == DATA_DATE or column_name == EXPIRY_DATE
                                df_Result_Stock[column_name] = pd.to_datetime(
                                    stock_base3_dataframe3[find_searched_excel_column_name]
                                )
                            else
                                df_Result_Stock[column_name] = stock_base3_dataframe3[find_searched_excel_column_name]
                            print(fProcessed column {column_name})
                            break
            # ================================
            # INTERMEDIATE SECTION HANDLE UNMATCHED COLUMNS
            # ================================
            for column_name in df_Result_Stock.columns
                if column_name not in matched_columns
                    if column_name == DATA_DATE
                        df_Result_Stock[column_name] = data_date
                    elif column_name == COUNTRY_NAME
                        df_Result_Stock[column_name] = country
                    elif column_name == ORGANIZATION_NAME
                        df_Result_Stock[column_name] = organization_name
                    elif column_name == BRANCH_NAME
                        if data_owner in [BEK, ISKOOP, SELÇUK, ALLIANCE]
                            df_Result_Stock[column_name] = stock_base3_dataframe3[apparo_branch_name]
                        else
                            df_Result_Stock[column_name] = branch_name
                    elif column_name == PRODUCT_ID
                        df_Result_Stock[column_name] = NULL
                    elif column_name == PRODUCT_NAME
                        df_Result_Stock[column_name] = NULL
                    elif column_name == AVAILABLE_QUANTITY
                        df_Result_Stock[column_name] = 0
                    elif column_name == BLOCKED_QUANTITY
                        df_Result_Stock[column_name] = 0
                    elif column_name == INVENTORY_CATEGORY
                        df_Result_Stock[column_name] = GN
                    elif column_name == BATCH_NUMBER
                        df_Result_Stock[column_name] = NA
                    elif column_name == EXPIRY_DATE
                        df_Result_Stock[column_name] = pd.to_datetime(2199-12-31 000000, format=%Y-%m-%d %H%M%S)
                    print(fHandled unmatched column {column_name})
            # ================================
            # INTERMEDIATE SECTION SPECIAL CASE FOR RAFED
            # ================================
            if data_owner == RAFED and structure == CUSTOMRAFED and country == UAE
                if daq_sheet_name == Tawam Data
                    df_Result_Stock[ORGANIZATION_NAME] = TAWAM
                elif daq_sheet_name == Mafraq-SSMC Data
                    df_Result_Stock[ORGANIZATION_NAME] = SSMC
                print(Updated ORGANIZATION_NAME for RAFED special case.)
            # ================================
            # INTERMEDIATE SECTION CITY PHARMACY SPECIAL CASE
            # ================================
            if data_owner == CITYPHARMACY and context == STOCK
                print(Processing CITYPHARMACY special case...)
                # Update 'REGION_NAME' column
                searched_excel_column_name_REGION = next(
                    (
                        item[searched_excel_column_name]
                        for item in stock_list_column_match
                        if item[etl_column_name] == REGION
                    ),
                    None,
                )
                df_Result_Stock[REGION_NAME] = stock_base3_dataframe3[searched_excel_column_name_REGION]
                df_Result_Stock[REGION_NAME] = df_Result_Stock[REGION_NAME].str.upper().str.replace(İ, I).str.split( ).str[0]
                # Update 'BRANCH_NAME' column
                df_Result_Stock[BRANCH_NAME] = df_Result_Stock.apply(
                    lambda row (
                        row[ORGANIZATION_NAME] +  SHARJAH
                        if pd.notna(row[REGION_NAME]) and row[COUNTRY_NAME] == UAE
                        and row[ORGANIZATION_NAME] == CITYPHARMACY
                        and row[REGION_NAME].upper() == SHARJAH
                        else (
                            row[ORGANIZATION_NAME] +  KIZAD
                            if pd.notna(row[REGION_NAME]) and row[COUNTRY_NAME] == UAE
                            and row[ORGANIZATION_NAME] == CITYPHARMACY
                            and row[REGION_NAME].upper() != SHARJAH
                            else (
                                row[ORGANIZATION_NAME] +  KIZAD
                                if pd.isna(row[REGION_NAME]) and row[COUNTRY_NAME] == UAE
                                and row[ORGANIZATION_NAME] == CITYPHARMACY
                                else row[BRANCH_NAME]
                            )
                        )
                    ),
                    axis=1,
                )
                df_Result_Stock = df_Result_Stock.drop(columns=[REGION_NAME])
                # Update 'INVENTORY_CATEGORY' based on 'STOCK_CATEGORY'
                searched_excel_column_name_STOCK_CATEGORY = next(
                    (
                        item[searched_excel_column_name]
                        for item in stock_list_column_match
                        if item[etl_column_name] == STOCK_CATEGORY
                    ),
                    None,
                )
                df_Result_Stock[STOCK_CATEGORY] = stock_base3_dataframe3[searched_excel_column_name_STOCK_CATEGORY]
                df_Result_Stock[INVENTORY_CATEGORY] = df_Result_Stock.apply(
                    lambda row (
                        PU
                        if row[ORGANIZATION_NAME] == CITYPHARMACY and row[STOCK_CATEGORY] == I
                        else (
                            PR
                            if row[ORGANIZATION_NAME] == CITYPHARMACY and row[STOCK_CATEGORY] == N
                            else row[INVENTORY_CATEGORY]
                        )
                    ),
                    axis=1,
                )
                df_Result_Stock = df_Result_Stock.drop(columns=[STOCK_CATEGORY])
                print(CITYPHARMACY special case processing complete.)
            # ================================
            # INTERMEDIATE SECTION CLEAN UP PRODUCT_NAME
            # ================================
            df_Result_Stock[PRODUCT_NAME] = df_Result_Stock[PRODUCT_NAME].str.replace(rs+,  )
            print(Cleaned up PRODUCT_NAME column.)
            # ================================
            # INTERMEDIATE SECTION GROUPING AND FINAL PROCESSING
            # ================================
            # Fill empty values with defaults
            df_Result_Stock[DATA_DATE].fillna(2199-12-31, inplace=True)
            df_Result_Stock[COUNTRY_NAME].fillna(NULL, inplace=True)
            df_Result_Stock[ORGANIZATION_NAME].fillna(NULL, inplace=True)
            df_Result_Stock[BRANCH_NAME].fillna(NULL, inplace=True)
            df_Result_Stock[PRODUCT_ID].fillna(NULL, inplace=True)
            df_Result_Stock[PRODUCT_NAME].fillna(NULL, inplace=True)
            df_Result_Stock[AVAILABLE_QUANTITY].fillna(0, inplace=True)
            df_Result_Stock[BLOCKED_QUANTITY].fillna(0, inplace=True)
            df_Result_Stock[INVENTORY_CATEGORY].fillna(GN, inplace=True)
            df_Result_Stock[BATCH_NUMBER].fillna(NULL, inplace=True)
            df_Result_Stock[EXPIRY_DATE].fillna(2199-12-31, inplace=True)
            # Group by relevant columns and aggregate
            df_grouped = (
                df_Result_Stock.groupby(
                    [
                        DATA_DATE,
                        COUNTRY_NAME,
                        ORGANIZATION_NAME,
                        BRANCH_NAME,
                        PRODUCT_ID,
                        PRODUCT_NAME,
                        INVENTORY_CATEGORY,
                        BATCH_NUMBER,
                        EXPIRY_DATE,
                    ]
                )
                .agg({AVAILABLE_QUANTITY sum, BLOCKED_QUANTITY sum})
                .reset_index()
            )
            print(Grouped and aggregated data.)
            # Rearrange columns in the desired order
            df_grouped = df_grouped[
                [
                    DATA_DATE,
                    COUNTRY_NAME,
                    ORGANIZATION_NAME,
                    BRANCH_NAME,
                    PRODUCT_ID,
                    PRODUCT_NAME,
                    AVAILABLE_QUANTITY,
                    BLOCKED_QUANTITY,
                    INVENTORY_CATEGORY,
                    BATCH_NUMBER,
                    EXPIRY_DATE,
                ]
            ]
            df_Result_Stock = df_grouped
            print(Rearranged columns and completed df_Result_Stock.)
            # ================================
            # SECTION END df_Result_Stock Creation Completed
            # ================================
            print(df_Result_Stock creation complete)
            print(df_Result_Stock)
        # ================================
        # SECTION SALES AND SALESSTOCK CONTEXT PROCESSING
        # ================================
        data_Date_Match_Statu = 0
        invoice_Date_Match_Statu = 0
        if (
            context in [SALES, SALESSTOCK]
            and len(base3_dataframe3)  0
            and (column_count_sales_is_mandatory_1 == column_count_matching_sales_is_mandatory_1)
            and (column_count_matching_sales_is_mandatory_1 != 0)
        )
            sales_list_column_match = list_column_match
            print(Initial Data Check)
            print(base3_dataframe3 , base3_dataframe3)
            # ================================
            # INTERMEDIATE SECTION FILTER VALID ENTRIES
            # ================================
            # Find the column name for SALES_QUANTITY in sales_list_column_match
            searched_excel_column_name_SALES_QUANTITY = next(
                (
                    item[searched_excel_column_name]
                    for item in sales_list_column_match
                    if item[etl_column_name] == SALES_QUANTITY
                ),
                None,
            )
            if not searched_excel_column_name_SALES_QUANTITY
                raise ValueError(Column 'SALES_QUANTITY' not found.)
            # Filtering exclude NaN, NaT, None, null and empty string values
            valid_entries = base3_dataframe3[
                base3_dataframe3[searched_excel_column_name_SALES_QUANTITY].apply(
                    lambda x not (pd.isna(x) or str(x).strip().lower() in [nan, nat, none, null, ])
                )
            ]
            valid_entries.reset_index(drop=True, inplace=True)
            sales_base3_dataframe3 = valid_entries.copy()
            print(Valid Entries Filtering Complete)
            print(sales_list_column_match , sales_list_column_match)
            # ================================
            # INTERMEDIATE SECTION MATCH COLUMNS AND ASSIGN VALUES
            # ================================
            matched_columns = [
                item[etl_column_name] for item in sales_list_column_match if item[etl_column_name] is not None
            ]
            print(matched_columns , matched_columns)
            print(df_Result_Sales , df_Result_Sales)
            print(sales_list_column_match , sales_list_column_match)
            print(sales_base3_dataframe3 , sales_base3_dataframe3)
            # Check for matching columns and assign values
            for column_name in df_Result_Sales.columns
                if column_name in matched_columns
                    find_searched_excel_column_name = None
                    find_etl_column_name = None
                    for item in sales_list_column_match
                        if item[etl_column_name] == column_name
                            find_searched_excel_column_name = item[searched_excel_column_name]
                            if column_name == DATA_DATE or column_name == INVOICE_DATE
                                df_Result_Sales[column_name] = pd.to_datetime(
                                    sales_base3_dataframe3[find_searched_excel_column_name]
                                )
                            elif (
                                column_name == SALES_QUANTITY
                                or column_name == RETURN_QUANTITY
                                or column_name == TAX_IDENTIFICATION_NUMBER
                            )
                                sales_base3_dataframe3[find_searched_excel_column_name].fillna(
                                    0, inplace=True
                                )
                                numeric_values = pd.to_numeric(
                                    sales_base3_dataframe3[find_searched_excel_column_name],
                                    errors=coerce,
                                )
                                df_Result_Sales[column_name] = numeric_values.fillna(0).astype(int)
                            elif column_name == SALES_VALUE or column_name == RETURN_VALUE
                                sales_base3_dataframe3[find_searched_excel_column_name].fillna(0, inplace=True)
                                numeric_values = pd.to_numeric(
                                    sales_base3_dataframe3[find_searched_excel_column_name],
                                    errors=coerce,
                                )
                                df_Result_Sales[column_name] = numeric_values.fillna(0).astype(float)
                                df_Result_Sales[column_name] = df_Result_Sales[column_name].apply(lambda x Decimal(x))
                            else
                                df_Result_Sales[column_name] = sales_base3_dataframe3[find_searched_excel_column_name]
                                break
            data_Date_Match_Statu = 0
            invoice_Date_Match_Statu = 0
            # ================================
            # INTERMEDIATE SECTION DATA DATE AND INVOICE DATE PROCESSING
            # ================================
            for column_name in df_Result_Sales.columns
                if column_name in matched_columns
                    if column_name == DATA_DATE
                        data_Date_Match_Statu = 1
                    elif column_name == INVOICE_DATE
                        invoice_Date_Match_Statu = 1
            print(1855)
            if data_Date_Match_Statu == 0 and invoice_Date_Match_Statu == 1
                df_Result_Sales[DATA_DATE] = (
                    pd.to_datetime(df_Result_Sales[INVOICE_DATE], format=%d.%m.%Y).dt.to_period(M).dt.end_time
                )
            elif data_Date_Match_Statu == 0 and invoice_Date_Match_Statu == 0
                df_Result_Sales[DATA_DATE] = data_date
                df_Result_Sales[INVOICE_DATE] = df_Result_Sales[DATA_DATE]
            elif data_Date_Match_Statu == 1 and invoice_Date_Match_Statu == 0
                df_Result_Sales[INVOICE_DATE] = df_Result_Sales[DATA_DATE]
            print(1863)
            searched_excel_column_name_DATA_DATE = next(
                (
                    item[searched_excel_column_name]
                    for item in sales_list_column_match
                    if item[etl_column_name] == DATA_DATE
                ),
                None,
            )
            # ================================
            # INTERMEDIATE SECTION DEFAULT VALUES FOR UNMATCHED COLUMNS
            # ================================
            for column_name in df_Result_Sales.columns
                if column_name not in matched_columns
                    if column_name == DATA_DATE and frequency != MONTHLY
                        df_Result_Sales[column_name] = data_date
                    elif column_name == INVOICE_DATE and frequency != MONTHLY
                        df_Result_Sales[column_name] = data_date
                    elif column_name == COUNTRY_NAME
                        df_Result_Sales[column_name] = country
                    elif column_name == ORGANIZATION_NAME
                        df_Result_Sales[column_name] = organization_name
                    elif column_name == BRANCH_NAME
                        if data_owner in [BEK, ISKOOP, SELÇUK, ALLIANCE]
                            df_Result_Sales[column_name] = sales_base3_dataframe3[apparo_branch_name]
                        else
                            df_Result_Sales[column_name] = branch_name
                    elif column_name == CUSTOMER_ID
                        df_Result_Sales[column_name] = NULL
                    elif column_name == CUSTOMER_NAME
                        df_Result_Sales[column_name] = NA
                    elif column_name == PRODUCT_ID
                        df_Result_Sales[column_name] = NULL
                    elif column_name == PRODUCT_NAME
                        df_Result_Sales[column_name] = NULL
                    elif column_name == SALES_QUANTITY
                        df_Result_Sales[column_name] = 0
                    elif column_name == RETURN_QUANTITY
                        df_Result_Sales[column_name] = 0
                    elif column_name == SALES_CATEGORY
                        df_Result_Sales[column_name] = GN
                    elif column_name == SALES_VALUE
                        df_Result_Sales[column_name] = 0
                    elif column_name == RETURN_VALUE
                        df_Result_Sales[column_name] = 0
                    elif column_name == AUCTION_NUMBER
                        df_Result_Sales[column_name] = -1
                    elif column_name == TAX_IDENTIFICATION_NUMBER
                        df_Result_Sales[column_name] = -1
            print(1902 - Default Values Assignment Complete)
            # ================================
            # INTERMEDIATE SECTION DATA ADJUSTMENTS BASED ON CONDITIONS
            # ================================
            if data_owner == RAFED and structure == CUSTOMRAFED and country == UAE
                if daq_sheet_name == Tawam Data
                    df_Result_Sales[SALES_QUANTITY] = df_Result_Sales[SALES_QUANTITY]  -1
                    df_Result_Sales[ORGANIZATION_NAME] = TAWAM
                elif daq_sheet_name == Mafraq-SSMC Data
                    df_Result_Sales[ORGANIZATION_NAME] = SSMC
            print(Data Adjustments Complete)
            if data_owner == CITYPHARMACY and context == SALES and country == UAE
                df_Result_Sales[SALES_CATEGORY] = df_Result_Sales.apply(
                    lambda row (
                        PU
                        if data_owner == CITYPHARMACY and row[SALES_CATEGORY] == I
                        else (PR if data_owner == CITYPHARMACY and row[SALES_CATEGORY] == N else GN)
                    ),
                    axis=1,
                )
                searched_excel_column_name_REGION = next(
                    (
                        item[searched_excel_column_name]
                        for item in sales_list_column_match
                        if item[etl_column_name] == REGION
                    ),
                    None,
                )
                df_Result_Sales[REGION_NAME] = sales_base3_dataframe3[searched_excel_column_name_REGION]
                df_Result_Sales[REGION_NAME] = df_Result_Sales[REGION_NAME].str.upper()
                df_Result_Sales[REGION_NAME] = df_Result_Sales[REGION_NAME].str.replace(
                    İ, I
                )
                df_Result_Sales[BRANCH_NAME] = df_Result_Sales.apply(
                    lambda row (
                        row[ORGANIZATION_NAME] +  KIZAD
                        if pd.notna(row[REGION_NAME])
                        and row[COUNTRY_NAME] == UAE
                        and row[ORGANIZATION_NAME] == CITYPHARMACY
                        and row[REGION_NAME].upper() in [ABU DHABI, AL AIN]
                        else (
                            row[ORGANIZATION_NAME] +  SHARJAH
                            if pd.notna(row[REGION_NAME])
                            and row[COUNTRY_NAME] == UAE
                            and row[ORGANIZATION_NAME] == CITYPHARMACY
                            and row[REGION_NAME].upper() not in [ABU DHABI, AL AIN]
                            else (
                                row[ORGANIZATION_NAME] +  SHARJAH
                                if pd.isna(row[REGION_NAME])
                                and row[COUNTRY_NAME] == UAE
                                and row[ORGANIZATION_NAME] == CITYPHARMACY
                                else row[BRANCH_NAME]
                            )
                        )
                    ),
                    axis=1,
                )
                df_Result_Sales = df_Result_Sales.drop(columns=[REGION_NAME])
            print(Citypharmacy Adjustments Complete)
            # ================================
            # INTERMEDIATE SECTION ADJUST SALES AND RETURN VALUES
            # ================================
            df_Result_Sales[RETURN_QUANTITY] = df_Result_Sales[SALES_QUANTITY].apply(
                lambda x abs(x) if x  0 else 0
            )
            df_Result_Sales[RETURN_VALUE] = np.where(
                df_Result_Sales[SALES_QUANTITY]  0,
                abs(df_Result_Sales[SALES_VALUE]),
                0,
            )
            df_Result_Sales.loc[df_Result_Sales[SALES_QUANTITY]  0, SALES_VALUE] = 0
            df_Result_Sales[SALES_QUANTITY] = df_Result_Sales[SALES_QUANTITY].apply(lambda x max(0, x))
            print(1942 - Sales and Return Values Adjustment Complete)
            # ================================
            # INTERMEDIATE SECTION CLEANUP AND GROUPING
            # ================================
            df_Result_Sales[PRODUCT_NAME] = df_Result_Sales[PRODUCT_NAME].str.replace(rs+,  )
            df_Result_Sales[DATA_DATE].fillna(2199-12-31, inplace=True)
            df_Result_Sales[INVOICE_DATE].fillna(2199-12-31, inplace=True)
            df_Result_Sales[COUNTRY_NAME].fillna(NULL, inplace=True)
            df_Result_Sales[ORGANIZATION_NAME].fillna(NULL, inplace=True)
            df_Result_Sales[BRANCH_NAME].fillna(NULL, inplace=True)
            df_Result_Sales[CUSTOMER_ID].fillna(NULL, inplace=True)
            df_Result_Sales[CUSTOMER_NAME].fillna(NA, inplace=True)
            df_Result_Sales[PRODUCT_ID].fillna(NULL, inplace=True)
            df_Result_Sales[PRODUCT_NAME].fillna(NULL, inplace=True)
            df_Result_Sales[SALES_QUANTITY].fillna(0, inplace=True)
            df_Result_Sales[RETURN_QUANTITY].fillna(0, inplace=True)
            df_Result_Sales[SALES_CATEGORY].fillna(GN, inplace=True)
            df_Result_Sales[SALES_VALUE].fillna(0, inplace=True)
            df_Result_Sales[RETURN_VALUE].fillna(0, inplace=True)
            df_Result_Sales[AUCTION_NUMBER].fillna(-1, inplace=True)
            df_Result_Sales[TAX_IDENTIFICATION_NUMBER].fillna(-1, inplace=True)
            df_grouped = (
                df_Result_Sales.groupby(
                    [
                        DATA_DATE,
                        COUNTRY_NAME,
                        ORGANIZATION_NAME,
                        BRANCH_NAME,
                        CUSTOMER_ID,
                        CUSTOMER_NAME,
                        PRODUCT_ID,
                        PRODUCT_NAME,
                        INVOICE_DATE,
                        SALES_CATEGORY,
                        AUCTION_NUMBER,
                        TAX_IDENTIFICATION_NUMBER,
                    ]
                )
                .agg(
                    {
                        SALES_QUANTITY sum,
                        RETURN_QUANTITY sum,
                        SALES_VALUE sum,
                        RETURN_VALUE sum,
                    }
                )
                .reset_index()
            )
            # Rearrange column order
            df_grouped = df_grouped[
                [
                    DATA_DATE,
                    COUNTRY_NAME,
                    ORGANIZATION_NAME,
                    BRANCH_NAME,
                    CUSTOMER_ID,
                    CUSTOMER_NAME,
                    PRODUCT_ID,
                    PRODUCT_NAME,
                    INVOICE_DATE,
                    SALES_QUANTITY,
                    RETURN_QUANTITY,
                    SALES_CATEGORY,
                    SALES_VALUE,
                    RETURN_VALUE,
                    AUCTION_NUMBER,
                    TAX_IDENTIFICATION_NUMBER,
                ]
            ]
            df_Result_Sales = df_grouped
            print(df_Result_Sales - Final Output)
            print(df_Result_Sales , df_Result_Sales)
        # ================================
        # START OF INSERT AND UPDATE OPERATIONS FOR POSTGRESQL DATABASE TABLES
        # ================================
        if df_Result_Stock.shape[0]  0
            # ================================
            # INTERMEDIATE SECTION STOCK DATA FRAME TO POSTGRESQL
            # ================================
            # Create Spark session
            spark = SparkSession.builder.appName(PostgreSQL Writing).getOrCreate()
            # Iterate through column names and rename them to lowercase
            df_Result_Stock.columns = [column_name.lower() for column_name in df_Result_Stock.columns]
            # Print the original and new column names
            for column_name in df_Result_Stock.columns
                print(fNew column name {column_name})
            # Define schema for Spark DataFrame with lowercase column names
            schema = StructType(
                [
                    StructField(data_date, DateType(), nullable=True),
                    StructField(country_name, StringType(), nullable=True),
                    StructField(organization_name, StringType(), nullable=True),
                    StructField(branch_name, StringType(), nullable=True),
                    StructField(product_id, StringType(), nullable=True),
                    StructField(product_name, StringType(), nullable=True),
                    StructField(available_quantity, IntegerType(), nullable=True),
                    StructField(blocked_quantity, IntegerType(), nullable=True),
                    StructField(inventory_category, StringType(), nullable=True),
                    StructField(batch_number, StringType(), nullable=True),
                    StructField(expiry_date, DateType(), nullable=True),
                ]
            )
            # Create Spark DataFrame from Pandas DataFrame with the defined schema
            spark_df = spark.createDataFrame(df_Result_Stock, schema=schema)
            # Write Spark DataFrame to PostgreSQL using JDBC
            spark_df.write.format(jdbc).option(url, jdbc_url).option(
                dbtable, f{schemaname}.{table_name}_i
            ).option(user, username).option(password, password).mode(overwrite).save()
            print(Stock DataFrame written to PostgreSQL.)
            # ================================
            # INTERMEDIATE SECTION CALL POSTGRESQL PROCEDURE
            # ================================
            try
                # Create PostgreSQL JDBC connection
                connSpark = spark.sparkContext._jvm.java.sql.DriverManager.getConnection(jdbc_url, username, password)
                stmt = connSpark.createStatement()
                sql_execute_query = (
                    fCALL {schemaname}.proc_update_temp_table('{schemaname}', '{table_name}_i')
                )
                print(Executing SQL procedure, sql_execute_query)
                stmt.execute(sql_execute_query)
                print(Procedure successfully called.)
            except Exception as e
                print(Error, e)
            finally
                if connSpark
                    connSpark.close()
            # Update temporary load info
            insert_into_temp_load_info(table_name + _i, 'INVENTORY', frequency, 0, 0)
        if df_Result_Sales.shape[0]  0
            # ================================
            # INTERMEDIATE SECTION SALES DATA FRAME TO POSTGRESQL
            # ================================
            # Create Spark session
            spark = SparkSession.builder.appName(Write to PostgreSQL).getOrCreate()
            # Iterate through column names and rename them to lowercase
            df_Result_Sales.columns = [column_name.lower() for column_name in df_Result_Sales.columns]
            # Print the original and new column names
            for column_name in df_Result_Sales.columns
                print(fNew column name {column_name})
            # Define schema for Spark DataFrame with lowercase column names
            schema = StructType(
                [
                    StructField(data_date, DateType(), nullable=True),
                    StructField(country_name, StringType(), nullable=True),
                    StructField(organization_name, StringType(), nullable=True),
                    StructField(branch_name, StringType(), nullable=True),
                    StructField(customer_id, StringType(), nullable=True),
                    StructField(customer_name, StringType(), nullable=True),
                    StructField(product_id, StringType(), nullable=True),
                    StructField(product_name, StringType(), nullable=True),
                    StructField(invoice_date, DateType(), nullable=True),
                    StructField(sales_quantity, LongType(), nullable=True),
                    StructField(return_quantity, LongType(), nullable=True),
                    StructField(sales_category, StringType(), nullable=True),
                    StructField(sales_value, DecimalType(22, 2), nullable=True),
                    StructField(return_value, DecimalType(22, 2), nullable=True),
                    StructField(auction_number, StringType(), nullable=True),
                    StructField(tax_identification_number, LongType(), nullable=True),
                ]
            )
            # Convert SALES_VALUE and RETURN_VALUE to Decimal type
            df_Result_Sales[sales_value] = df_Result_Sales[sales_value].apply(lambda x Decimal(x))
            df_Result_Sales[return_value] = df_Result_Sales[return_value].apply(lambda x Decimal(x))
            # Create Spark DataFrame from Pandas DataFrame with the defined schema
            spark_df = spark.createDataFrame(df_Result_Sales, schema=schema)
            print(Sales DataFrame schema, schema)
            print(Writing Sales DataFrame to PostgreSQL.)
            # Write Spark DataFrame to PostgreSQL using JDBC
            spark_df.write.format(jdbc).option(url, jdbc_url).option(
                dbtable, f{schemaname}.{table_name}_s
            ).option(user, username).option(password, password).mode(overwrite).save()
            print(Sales DataFrame written to PostgreSQL.)
            # ================================
            # INTERMEDIATE SECTION CALL POSTGRESQL PROCEDURE
            # ================================
            try
                # Create PostgreSQL JDBC connection
                connSpark = spark.sparkContext._jvm.java.sql.DriverManager.getConnection(jdbc_url, username, password)
                stmt = connSpark.createStatement()
                sql_execute_query = (
                    fCALL {schemaname}.proc_update_temp_table('{schemaname}', '{table_name}_s')
                )
                print(Executing SQL procedure, sql_execute_query)
                stmt.execute(sql_execute_query)
                print(Procedure successfully called.)
            except Exception as e
                print(Error, e)
            finally
                if connSpark
                    connSpark.close()
            # Update temporary load info
            insert_into_temp_load_info(table_name + _s, 'SALES', frequency, invoice_Date_Match_Statu, 0)
        # ================================
        # INTERMEDIATE SECTION FINAL UPDATE TO DAQ_LOG_INFO
        # ================================
        update_daq_log_info(id)
# =======================================================
# FINAL STEP Mark All Unprocessed, Non-Corrupt Records as Processed in DAQ_LOG_INFO
# =======================================================
update_daq_log_info('ALL_RECORDS')
# ============================================================
# Job Commit
# ============================================================
print(Step Final Committing the Glue job.)
glueJob.commit()
</file>

<file path="file_standardization_job.py">
"""
Main ETL job script for file standardization.
"""
import sys
import os
# Add deployment package to Python path
if 'GLUE_DEPLOYMENT_PATH' in os.environ:
    deployment_path = os.environ['GLUE_DEPLOYMENT_PATH']
else:
    # Default to the script's directory
    deployment_path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(deployment_path)
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from src.database.connection import DatabaseConnection
from src.database.operations import DatabaseOperations
from src.etl.extractors import DataExtractor
from src.etl.transformers import DataTransformer
from src.etl.loaders import DataLoader
from src.utils.logging_utils import ETLLogger
def run_etl_job():
    """Main ETL job execution function."""
    # Initialize logging
    logger = ETLLogger("FileStandardizationJob")
    logger.start_job()
    try:
        # Get job parameters
        args = getResolvedOptions(sys.argv, ['JOB_NAME', 'secret_name', 'env'])
        # Initialize Spark context
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        logger.log_info(f"Environment: {args['env']}")
        # Initialize components
        db_connection = DatabaseConnection(args['secret_name'])
        db_operations = DatabaseOperations(db_connection)
        extractor = DataExtractor(args['env'], logger)
        transformer = DataTransformer(logger)
        loader = DataLoader(db_connection, logger)
        # Get unprocessed files
        logger.log_step("Fetching unprocessed files")
        unprocessed_files = db_operations.get_unprocessed_files()
        if not unprocessed_files.count():
            logger.log_info("No unprocessed files found")
            return
        # Process each file
        for file_row in unprocessed_files.collect():
            try:
                logger.log_step(f"Processing file: {file_row.file}")
                # Get entity details using sequential matching
                entity_details = db_operations.get_entity_details(
                    daq_sheet_name=file_row.daq_sheet_name,
                    sender_address=file_row.sender_address,
                    file_extension=file_row.file_extension,
                    receiver_address=file_row.receiver_address,
                    file_id=file_row.id
                )
                if not entity_details.count():
                    logger.log_warning(f"No entity details found for file: {file_row.file}")
                    continue
                # Process each entity
                for entity_row in entity_details.collect():
                    try:
                        logger.log_step(f"Processing entity: {entity_row.data_owner}")
                        # Get attribute details
                        attribute_details = db_operations.get_attribute_details(
                            entity_row.data_owner,
                            entity_row.context,
                            entity_row.entity_file_table_name,
                            entity_row.entity_sheet_name
                        ).collect()
                        # Extract data
                        file_info = {
                            'file_path': file_row.file,
                            'file_type': file_row.file_extension,
                            'sheet_name': file_row.daq_sheet_name,
                            'data_owner': entity_row.data_owner,
                            'country': entity_row.country
                        }
                        raw_data = extractor.extract_data(file_info)
                        if raw_data is None:
                            continue
                        # Transform data based on context
                        if entity_row.context.upper() == 'STOCK':
                            processed_data = transformer.transform_stock_data(
                                raw_data,
                                attribute_details
                            )
                            if processed_data:
                                success = loader.load_stock_data(
                                    processed_data,
                                    f"temp_{file_row.id}"
                                )
                        else:  # SALES
                            processed_data = transformer.transform_sales_data(
                                raw_data,
                                attribute_details
                            )
                            if processed_data:
                                success = loader.load_sales_data(
                                    processed_data,
                                    f"temp_{file_row.id}"
                                )
                        if success:
                            # Insert load info
                            db_operations.insert_temp_load_info(
                                f"temp_{file_row.id}",
                                entity_row.context,
                                entity_row.frequency,
                                1 if entity_row.context.upper() == 'SALES' else 0,
                                1
                            )
                    except Exception as entity_error:
                        logger.log_error(
                            f"Error processing entity {entity_row.data_owner}: {str(entity_error)}",
                            exc_info=entity_error
                        )
                        continue
                # Mark file as processed
                db_operations.mark_file_as_processed(file_row.id)
            except Exception as file_error:
                logger.log_error(
                    f"Error processing file {file_row.file}: {str(file_error)}",
                    exc_info=file_error
                )
                continue
        # Cleanup
        loader.cleanup_temp_tables()
        logger.end_job()
        job.commit()
    except Exception as e:
        logger.log_error("Job failed", exc_info=e)
        logger.end_job("failed")
        raise
if __name__ == "__main__":
    run_etl_job()
</file>

<file path="README.md">
# File Standardization ETL Job

This AWS Glue ETL job standardizes file formats from various sources into a consistent format for data processing.

## Overview

The ETL job processes files from different sources, applying standardization rules based on entity and attribute configurations stored in a database. It supports both stock and sales data processing with different validation and transformation rules.

## Architecture

- **Main Components**:
  - File Processing
  - Entity Matching
  - Data Transformation
  - Data Loading

- **Key Features**:
  - Sequential entity matching strategy
  - Configurable attribute mapping
  - Support for multiple file formats
  - Comprehensive error handling and logging
  - Special case handling for different data owners

## Project Structure

```
.
├── config/
│   ├── queries.py        # SQL query templates
│   ├── settings.py       # Configuration settings
│   └── db_config.py      # Database configuration
├── src/
│   ├── database/
│   │   ├── connection.py    # Database connection handling
│   │   └── operations.py    # Database operations
│   ├── etl/
│   │   ├── extractors.py    # Data extraction
│   │   ├── transformers.py  # Data transformation
│   │   └── loaders.py       # Data loading
│   ├── business/
│   │   ├── rules.py         # Business rules
│   │   └── special_cases.py # Special case handling
│   ├── validation/
│   │   ├── schema_validator.py  # Schema validation
│   │   └── data_validator.py    # Data validation
│   └── utils/
│       ├── logging_utils.py     # Logging utilities
│       ├── s3_utils.py          # S3 operations
│       └── date_utils.py        # Date handling utilities
└── file_standardization_job.py  # Main job script
```

## Setup

1. **AWS Environment Requirements**:
   - AWS Glue service role with appropriate permissions
   - S3 bucket for job scripts and data
   - Secrets Manager for database credentials

2. **Database Requirements**:
   - PostgreSQL database
   - Required tables:
     - `daq_log_info`
     - `dd_entity_detail`
     - `dd_attribute_detail`
     - `temp_load_info`

3. **Configuration**:
   - Update `config/settings.py` with environment-specific settings
   - Configure database connection in AWS Secrets Manager
   - Set up appropriate IAM roles and permissions

## Deployment

1. Package the code:
   ```bash
   # Create deployment package
   mkdir deployment
   cp -r src config file_standardization_job.py deployment/
   ```

2. Upload to S3:
   ```bash
   aws s3 cp deployment s3://your-bucket/glue-jobs/file-standardization/ --recursive
   ```

3. Create AWS Glue job:
   - Script path: `s3://your-bucket/glue-jobs/file-standardization/file_standardization_job.py`
   - Worker type: G.1X or G.2X
   - Python version: 3.9
   - Job parameters:
     - `--JOB_NAME`
     - `--secret_name`
     - `--env`

## Monitoring

- CloudWatch Logs for detailed job execution logs
- Glue Job metrics for performance monitoring
- Database logs for data processing validation

## Error Handling

The job includes comprehensive error handling:
- File-level error handling
- Entity-level error handling
- Data validation errors
- Database operation errors
- AWS service errors

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="requirements.txt">
boto3>=1.26.0
s3fs>=2023.1.0
pandas>=1.5.0
numpy>=1.23.0
pyspark>=3.3.0
psycopg2-binary>=2.9.5
openpyxl>=3.1.0
xlrd>=2.0.1
</file>

</files>
